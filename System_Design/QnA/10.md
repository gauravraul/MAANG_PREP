# System Design Interview Q&A – Set 2 (Medium Level)

---

## Q1: Design Instagram (Photo Sharing System)

### Answer:

### Functional Requirements:
- Upload photos
- View feed (photos from followed users)
- Like/comment on photos
- Follow/unfollow users

### Non-Functional Requirements:
- Highly available
- Low latency feed
- Scalable storage

### High-Level Architecture:
Client → API Gateway → Services → Databases & Object Storage  

### Core Services:
- User Service
- Photo Service
- Feed Service
- Like/Comment Service
- Follow Service

### Data Storage:
- **Relational DB** for users, follows, metadata  
- **Object Storage (e.g., S3)** for photos  
- **Cache (Redis)** for hot feeds & user profiles  

### Feed Generation:
- **Push model**: When user posts, push to followers’ feeds  
- **Pull model**: Generate feed on demand (for users with many follows or celebrities)  
- **Hybrid**: Normal users → push, celebrities → pull  

### Scaling:
- Shard photos by `photo_id` or `user_id`
- Use CDN for photo delivery
- Use cache for timelines & user info

### Failure Handling:
- Fallback to slightly stale cached feed
- Retry feed fanout in background
- Graceful degradation (show partial feed)

---

## Q2: Design Twitter (Timeline + Short Text Posts)

### Answer:

### Functional Requirements:
- Post tweet (up to N characters)
- Follow/unfollow
- View home timeline
- Like/retweet

### Architecture:
Client → API Gateway →  
- Tweet Service  
- User/Follow Service  
- Timeline/Feed Service  
- Engagement Service (likes, retweets)  

### Data Model (simplified):
- `User(id, name, …)`
- `Tweet(id, user_id, text, created_at, …)`
- `Follow(follower_id, followee_id)`

### Timeline Strategies:
- **Fan-out on write**: Precompute timeline when tweet is written  
- **Fan-out on read**: Compute timeline when user opens app  
- **Hybrid**: Similar to Instagram

### Performance:
- Cache user timelines in Redis
- Use pagination with `max_id` / `since_id`
- Use search/indexing service (Elastic) for hashtag/search

### Scaling:
- Shard tweets by `tweet_id`
- Separate read/write DBs
- Use message queue (Kafka) for async fanout

---

## Q3: Design WhatsApp (or Similar) at Scale

### Answer:

### Requirements:
- 1:1 chat and group chat
- Online presence
- Message delivery guarantees (at least once)
- End-to-end encryption (high-level mention)

### Architecture:
Client → Gateway → Chat Service → Message Queue → Storage  

### Core Components:
- **Connection Server** (WebSockets)
- **Chat Service** (business logic)
- **Presence Service** (online/offline)
- **Group Service**
- **Push Notification Service**
- **Storage Service** (message history)

### Storage:
- NoSQL DB (Cassandra/Dynamo-style) keyed by `(chat_id, message_id)`
- Separate index for per-user unread message counts

### Delivery Semantics:
- Client sends → server ACK → deliver to receiver
- Store delivery status (sent, delivered, read)
- Persistent queue per user/device

### Scaling:
- Shard by `user_id`
- Keep WebSocket servers stateless, use Redis for session mapping
- Partition groups by `group_id`

---

## Q4: Design YouTube (Video Streaming Platform)

### Answer:

### Functional Requirements:
- Upload video
- Stream video
- Search videos
- Like/comment/subscribe
- Recommendations (basic mention)

### High-Level Flow:
Upload:  
Client → Upload Service → Transcoding Service → Object Storage → CDN  

Playback:  
Client → CDN → Video Chunks  

### Core Services:
- Upload Service
- Transcoding Service
- Metadata Service (title, tags, owner, etc.)
- Streaming Service
- Search Service
- Recommendation Service (high-level only)

### Storage:
- Metadata in SQL/NoSQL
- Raw + transcoded video in Object Storage
- Thumbnails in Object Storage

### Transcoding:
- Different resolutions (144p–4K)
- Asynchronous video processing via queue (Kafka, etc.)
- Status polling or callback to client

### Scaling:
- CDN for video delivery
- Cache metadata for popular videos
- Pre-generate and cache frequently accessed home/trending pages

---

## Q5: Design Uber / Ola (Ride Hailing System)

### Answer:

### Functional Requirements:
- Register drivers and riders
- Find nearby drivers
- Matching rider ↔ driver
- Pricing & trip tracking
- Basic ETA

### Core Components:
- Rider Service
- Driver Service
- Matching Service
- Geo-Location Service
- Trip Service
- Pricing Service

### Location Handling:
- Drivers send GPS updates frequently
- Use a **spatial index** (e.g., geohash) to query nearby drivers
- Maintain in-memory/Redis data structure for online drivers

### Matching:
- For a rider request:
  - Get nearby drivers from Geo-Location Service
  - Filter by status (available)
  - Rank by distance/ETA
  - Assign trip and notify driver

### Storage:
- Trips: SQL/NoSQL with time-series optimizations
- Geo: in-memory store + periodic persistence

### Scaling:
- Partition drivers by region/city
- Use separate clusters per geography
- Rate limit location update frequency

---

## Q6: Design a Distributed Cache (like Redis Cluster)

### Answer:

### Functional Requirements:
- Key-value store
- Low latency reads/writes
- Support for scaling horizontally
- Basic TTL

### Non-Functional:
- High availability
- Partition tolerance

### Architecture:
Client → Cache Client Library → Multiple Cache Nodes  

### Key Distribution:
- **Consistent hashing** to map keys to nodes
- Avoid heavy reshuffling when nodes are added/removed

### Replication:
- Master–replica model
- Reads from replica, writes to master (or configurable)
- Async replication for performance

### Failover:
- Health checks for nodes
- Automatic leader election (sentinel/ZooKeeper/consensus)
- Client retries with new topology

### Eviction:
- LRU / LFU / Random
- Per-node or global policy

---

## Q7: Design a Distributed Lock Service

### Answer:

### Requirements:
- Provide mutual exclusion for shared resources
- Prevent multiple clients from doing the same critical work
- Locks must expire (avoid deadlock from crashed clients)

### Architecture:
- Centralized lock server (e.g., Redis, ZooKeeper, etcd)
- Or consensus-based lock among nodes (e.g., Raft-based)

### Using Redis (example):
- Use `SET resource lock_id NX PX ttl`
- Only the client with `lock_id` can release the lock
- Locks auto-expire after `ttl`

### Key Points:
- Use unique identifiers per lock owner
- Support retries with backoff
- Handle clock skew (avoid relying purely on local time)

---

## Q8: Design a Feature Flag Service (Toggle Features Per User)

### Answer:

### Functional Requirements:
- Enable/disable feature globally
- Roll out feature to a percentage of users
- Enable per user/segment

### Architecture:
Client / Service → Feature Flag SDK → Feature Service / Cache  

### Data Model:
- `FeatureFlag(name, enabled, rollout_percentage, rules, …)`
- Rules by `user_id`, region, app version, etc.

### Flow:
1. App calls SDK: `isEnabled("new_feature", user_context)`
2. SDK fetches flags from service/cache
3. Rules + hashing (for % rollout) decide final boolean

### Scaling:
- Flags relatively small → push config to edge/cache
- Use pub/sub for cache invalidation (on flag update)
- Admin UI for config changes

---

## Q9: Design a Multi-Tenant SaaS System (e.g., CRM for Many Companies)

### Answer:

### Requirements:
- Support multiple companies (tenants)
- Data isolation per tenant
- Role-based access per tenant

### Tenant Isolation Strategies:
1. **Shared DB, shared schema** with `tenant_id` column  
2. **Shared DB, separate schema per tenant**  
3. **Separate DB per tenant**  

### Trade-offs:
- Shared schema: cheaper, simpler, but noisy neighbors risk
- Separate DB: best isolation but expensive & more ops overhead

### Architecture:
Client → Auth Service → App Service → Tenant-Aware Data Layer  

### Data Layer:
- Always filter by `tenant_id`
- Use row-level security or app-level filtering
- Connection routing if separate DBs per tenant

### Scaling:
- Add tenants by creating new schema/DB
- Config service mapping `tenant -> DB`
- Background jobs scoped by tenant

---

## Q10: Design a Real-Time Leaderboard System (Like Gaming Leaderboard)

### Answer:

### Requirements:
- Maintain ranking of players by score
- Real-time updates
- Query top N players
- Query rank of a specific player

### Data Structures:
- Sorted set (e.g., Redis Sorted Set)
- Key: `leaderboard_id`, Member: `player_id`, Score: `score`

### Operations:
- Update score: `ZADD leaderboard score player`
- Get top N: `ZREVRANGE leaderboard 0 N-1`
- Get rank of player: `ZREVRANK leaderboard player`

### Scaling:
- Partition leaderboards by game/region
- Use time-based leaderboards (daily/weekly) to limit size
- Periodically archive old leaderboards to persistent DB

### Durability:
- Periodic snapshotting to disk
- Write-ahead log / append-only file for recovery

---

# Answering Framework (for Medium-Level Questions)

For each system, try to explicitly cover:

1. Requirements (functional + non-functional)  
2. High-level architecture (main services)  
3. Data model and storage choices  
4. Core flows (write path, read path)  
5. Scaling and partitioning  
6. Caching strategies  
7. Fault-tolerance and recovery  
8. Consistency vs availability trade-offs  
9. Security / multi-tenancy (when relevant)  
10. Monitoring, logging, and metrics
