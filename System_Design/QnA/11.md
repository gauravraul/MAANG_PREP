# System Design Interview Q&A – Set 3 (Medium / Hard Mix)

---

## Q1: Design a Content Delivery Network (CDN)

### Answer:

### Functional Requirements:
- Cache static content (images, videos, HTML, CSS, JS)
- Serve users from nearest location (low latency)
- Handle large traffic spikes

### Non-Functional:
- High availability
- Low latency
- Geo-distribution

### High-Level Architecture:
Client → DNS → Nearest Edge Server (PoP) → Origin Server (fallback)

### Core Components:
- **Edge Servers (PoPs)**: cache content close to users
- **Origin Server**: main source of content
- **DNS-based Routing**: map user to nearest PoP
- **Cache Invalidation Service**: purge/refresh content

### Key Concepts:
- **TTL**: how long content stays cached
- **Cache Invalidation**:
  - Time-based (expire after TTL)
  - Manual purge (API)
  - Versioned URLs (e.g., `/app.v3.js`)
- **Geo-DNS / Anycast** routing

### Scaling:
- Add more PoPs in high demand regions
- Anycast IPs for global routing
- Hierarchical caching (regional → global)

---

## Q2: Design a Web Crawler (Like Googlebot)

### Answer:

### Functional Requirements:
- Crawl web pages
- Respect `robots.txt`
- Avoid crawling too frequently
- Detect & avoid duplicate content

### High-Level Architecture:
URL Frontier → Fetcher → Parser → URL Extractor → Indexer → URL Frontier

### Core Components:
- **URL Frontier / Scheduler**: decides which URL to crawl next
- **Fetcher**: downloads HTML
- **Parser**: extracts links and content
- **Duplicate Detector**: avoids re-crawling or indexing same content
- **Politeness Module**: rate limits per domain

### Data Structures:
- Priority queue for URLs (by importance/freshness)
- Hash set / Bloom filter to avoid duplicates
- Per-domain queue to respect `robots.txt` and rate limits

### Scaling:
- Partition URLs by domain hash
- Multiple crawler workers
- Sharded storage for crawled data

---

## Q3: Design Google Docs (Real-Time Collaborative Editor)

### Answer:

### Functional Requirements:
- Multiple users edit same doc in real time
- Low-latency updates
- Conflict resolution
- Version history

### High-Level Architecture:
Clients ↔ Collaboration Server ↔ Storage

### Core Concepts:
- **Operational Transformation (OT)** or **CRDTs** for conflict resolution
- **Sessions** per document
- **Change stream** broadcast to all connected clients

### Flow:
1. User edits locally → generates operation
2. Operation sent to server
3. Server transforms/merges operation vs current doc state
4. Server updates doc state and broadcasts transformed operation to all clients
5. Each client applies operations in same order

### Storage:
- Document snapshots
- Operation log (for replay/history)

### Scaling:
- Partition by `document_id` → route all doc operations to same collaboration node
- Use WebSockets for bidirectional low-latency updates

---

## Q4: Design an Online Booking System (e.g., Hotel / Movie Tickets)

### Answer:

### Functional Requirements:
- View availability
- Reserve/hold a seat or room
- Confirm booking (payment)
- Prevent double-booking

### Challenges:
- **Concurrency**: multiple users booking same seat
- **Consistency**: ensure no overbooking

### High-Level Architecture:
Client → API Gateway → Booking Service → Inventory Service → Payment Service → DB

### Approaches to Avoid Overbooking:
1. **Pessimistic Locking**:
   - Lock seat/room row while processing
   - Pros: simple, strong consistency
   - Cons: lower throughput

2. **Optimistic Locking (Versioning)**:
   - Read seat with version
   - Update `WHERE id = ? AND version = ?`
   - If no rows updated → retry
   - Better concurrency

3. **Reservation Holds**:
   - Mark seat as "held" for a short TTL
   - After successful payment → confirm
   - If timeout → release hold

### Scaling:
- Partition by theatre_id / hotel_id
- Cache read-only availability snapshots
- Use message queues for asynchronous tasks (emails, invoices)

---

## Q5: Design Dropbox / Google Drive (File Sync & Share)

### Answer:

### Functional Requirements:
- Upload, download, delete files
- Sync across devices
- Share links with others
- Version history

### High-Level Architecture:
Client → Sync Client → Metadata Service → DB  
Client → File Storage Service → Object Storage

### Metadata:
- Folder structure
- File versions
- Permissions
- Shared links

### Sync Logic:
- Client computes file hash (chunk-level)
- Upload only changed chunks (delta sync)
- Use change logs per user
- Clients poll or use push notifications to get updates

### Storage:
- **Object Storage** for file chunks
- **SQL/NoSQL** for metadata

### Scaling:
- Content-addressable storage (deduplication via hash)
- Shard metadata by user_id
- CDN for download acceleration

---

## Q6: Design an Auto-Complete / Typeahead System (Search Suggestions)

### Answer:

### Functional Requirements:
- Suggest queries as user types
- Suggestions sorted by popularity and relevance
- Low latency (sub 100ms)

### Data Structures:
- **Trie / Prefix Tree** for matching prefixes
- **Top-K list** at each node (most popular completions)
- Or **inverted index + prefix search**

### High-Level Architecture:
Client → Suggestion Service → In-Memory Index / Cache → Persistent Storage (for training data)

### Suggestion Source:
- Past user queries
- Trending terms
- Domain-specific dictionary

### Ranking:
- Frequency (query count)
- Recency (time decay)
- Context (user location, language, previous queries)

### Scaling:
- Shard trie/index by first character(s)
- Keep index in memory (Redis, in-process)
- Offline batch jobs to recompute rankings

---

## Q7: Design an Ads Serving System (Simplified)

### Answer:

### Functional Requirements:
- Select ads for a given page/view
- Target by user / context
- Track impressions and clicks
- Budget / frequency capping

### High-Level Architecture:
Request → Ad Server → Candidate Selection → Ranking → Return Top N Ads

### Core Components:
- **Ad Inventory Service** (ad campaigns, budgets, targeting rules)
- **Targeting Engine** (filters ads by context/user)
- **Ranking Engine** (sorts by bid * CTR, relevance, etc.)
- **Tracking Service** (logs impressions, clicks, conversions)
- **Reporting/Billing Service**

### Flow:
1. Page sends ad request with context (user, page, geo, device)
2. Targeting engine filters eligible ads
3. Ranking engine scores ads (e.g., eCPM: bid * predicted CTR)
4. Top N ads returned
5. Impressions/clicks logged asynchronously (via Kafka)

### Scaling:
- Cache frequently used campaign data
- Shard by advertiser / campaign
- Use approximate counters for real-time budget checks

---

## Q8: Design a Metrics & Monitoring System (Like Prometheus + Grafana)

### Answer:

### Functional Requirements:
- Collect metrics from services
- Store time-series metrics
- Query & visualize
- Alert on thresholds

### High-Level Architecture:
App → Metrics Client → Metrics Gateway → Time-Series DB → Dashboard & Alerting

### Data Model:
- **Time-Series**: (metric_name, labels, timestamp, value)
- Examples: `http_requests_total{service="user",status="500"}`

### Collection Models:
- **Push**: apps push metrics to gateway
- **Pull**: gateway scrapes metrics endpoints

### Storage:
- Time-series database (Prometheus-like, InfluxDB, etc.)
- Retention + downsampling (raw → 1m → 5m → 1h rollups)

### Alerts:
- Rules engine (e.g. “error_rate > 5% for 5m”)
- Integrations with email, Slack, PagerDuty

### Scaling:
- Shard metrics by metric_name / tenant
- Remote storage for long-term retention

---

## Q9: Design a Configuration Management Service (Centralized Config)

### Answer:

### Functional Requirements:
- Central store for app config
- Fetch config at startup and/or runtime
- Versioning and rollback
- Env-specific config (dev/stage/prod)

### High-Level Architecture:
Admin UI → Config Service → Config Store  
Apps → Config SDK → Config Service / Cache

### Features:
- Namespaces (by app, environment)
- Key-value or structured config (JSON/YAML)
- Audit logs for changes
- Rollouts:
  - Global
  - Per percentage of instances

### Consistency:
- Strong consistency for config fetch
- Or eventual consistency with push-based updates + local caching

### Scaling:
- Cache configs in memory / Redis
- Use watch/long-poll mechanism or pub/sub to notify apps of changes

---

## Q10: Design a CI/CD Pipeline System (Like GitHub Actions / Jenkins)

### Answer:

### Functional Requirements:
- Trigger builds on code changes
- Run tests
- Build artifacts
- Deploy to environments
- Show logs and status

### High-Level Architecture:
VCS (Git) → Webhook → CI Orchestrator → Worker Pool → Artifact Store / Deploy

### Core Components:
- **Trigger Service**: listens to webhooks (push/PR)
- **Pipeline Orchestrator**: stores pipeline config (YAML), manages stages/jobs
- **Workers / Runners**: execute jobs in containers/VMs
- **Artifact Store**: store build outputs (binaries, Docker images)
- **Deployment Service**: deploys to environments (Kubernetes, VMs)
- **UI / API**: monitor status, logs, re-runs

### Execution Model:
- Queue jobs in message queue
- Workers pull jobs
- Isolated execution (container per job)
- Stream logs back to server/UI

### Scaling:
- Auto-scale worker pool
- Shard pipelines by repo/organization
- Cache dependencies (e.g., language package caches) on workers

---

# Reminder: Answering Structure for These Questions

For each design, try to cover:

1. Functional & non-functional requirements  
2. High-level architecture diagram in words  
3. Core components/services  
4. Data model & storage  
5. Key algorithms / data structures (if relevant)  
6. Scale-out strategy (sharding, caching, queues)  
7. Consistency / concurrency considerations  
8. Failure handling & resilience  
9. Security & multi-tenancy (if relevant)  
10. Monitoring, logging, metrics
