# System Design Interview Q&A – Set 15 (Fundamental / Bare-Metal Adversarial Scenarios)

> These are **pure reasoning puzzles** used for top-tier L6–L8 interviews.  
> No fancy tools. Only: **CPU + Memory + Disk + Network**.  
> All guarantees must be built from scratch.

---

## Q1: Design a Reliable Message Delivery System  
**Constraint:**  
> You have only CPU, RAM, Disk, and Network. No databases, no queues, no cloud services.

### Answer:

### Invariants:
- Messages must not be lost
- Messages must not be duplicated
- Delivery must be retry-safe

### Minimal Design:

1. **Write-Ahead Log (WAL):**
   - On message arrival → append to disk log  
   - fsync or batched fsync for durability  

2. **In-Memory Index:**
   - Map: `message_id → offset_in_log`

3. **Delivery State File:**
   - Track each message:
     - PENDING  
     - DELIVERED  

4. **Exactly-Once Semantics:**
   - Before processing:
     - Check delivery state  
   - After processing:
     - Mark DELIVERED → flush to disk  
   - Idempotent consumer required  

5. **Crash Recovery:**
   - On restart:
     - Reconstruct index from WAL  
     - Replay until last known offset  

---

## Q2: Implement a Distributed Lock  
**Constraint:**  
> No database, no Redis, no ZooKeeper.  
> Only: UDP/TCP + disk.

### Answer:

### Strategy: **File-Based Lock + Heartbeats**  
(Distributed approximation, not perfect but workable in constraints)

1. **Lock File:**
   - Node writes its `node_id + expiry_time`  
   - fsync  

2. **Acquire:**
   - If lock file empty or expired → write your ID with new lease  

3. **Renewal:**
   - Node sends renewal every X seconds  

4. **Failure Handling:**
   - If node dies → lease expires → next contender overwrites  

5. **Network Split Handling:**
   - Lease uses monotonic timestamps  
   - If connectivity lost → node cannot renew → lock released automatically  

---

## Q3: Build a Durable Counter  
**Constraint:**  
> Must survive crashes and corrupt writes. Only disk allowed.

### Answer:

### Strategy:  
1. **Append-only log:**  
   - Each increment = append `+1` record  

2. **Checkpointing:**  
   - Every N increments, compact to a snapshot:  
     - Write “counter = X”  
     - fsync  

3. **Recovery:**  
   - Read snapshot  
   - Replay remaining log entries  

4. **Corruption Handling:**
   - Validate each log entry with CRC  
   - Stop at first corrupted entry  

---

## Q4: Implement a Distributed Counter  
**Constraint:**  
> Nodes cannot coordinate except via unreliable network.

### Answer:

### Strategy: CRDT G-Counter

1. **Each node holds its own integer.**  
2. **Increment only local integer.**  
3. **Merge rule:**  
   - For each node index:
     - Take `max(local[i], remote[i])`  

4. **Global value = SUM(all nodes)**  

5. **Survives:**
   - Partitions  
   - Duplicates  
   - Reordering  
   - Node restarts (with durable local copy)  

---

## Q5: Build a Reliable Replicated Log  
**Constraint:**  
> No consensus algorithm allowed (no Paxos/Raft).  
> Only network messaging + disk.

### Answer:

### Strategy: Leaderless replication with version vectors

1. **Each write = (data, version_vector)**  
2. **All replicas append entry**  
3. **On conflict:**
   - Use deterministic merge  
   - Or preserve all branches (multi-version)  

4. **Readers choose:**
   - Highest version  
   - Or resolve via application rules  

This is **eventually consistent**, not strictly linearizable.

---

## Q6: Design a Job Scheduler  
**Constraint:**  
> No queues, no DB. Jobs must not be lost.

### Answer:

### Strategy:

1. **Jobs stored in WAL:**  
   - Append-only job records  
   - State transitions written to WAL  

2. **Workers scan WAL for PENDING jobs:**  
   - Mark job “IN_PROGRESS”  
   - fsync  

3. **Leases:**
   - IN_PROGRESS contains `worker_id + expiry`  
   - If worker crashes → lease expires → another worker picks job  

4. **Termination:**
   - When job finished → mark COMPLETED in WAL  

---

## Q7: Build a Gossip System  
**Constraint:**  
> Only UDP allowed. Messages may be dropped.

### Answer:

### Strategy:

1. **Periodic Gossip:**
   - Every T ms → send state to random peers  

2. **State Versioning:**
   - Each node increments version on change  

3. **Merge Rule:**
   - Take highest version per node  

4. **Eventual Propagation:**  
   - With high probability, new state spreads to all nodes  

---

## Q8: Implement Heartbeats  
**Constraint:**  
> Must detect dead nodes in unreliable network.

### Answer:

### Strategy:

1. **Send heartbeat every X seconds**  
2. **Failure detection:**
   - If no heartbeat for K * X seconds → suspect dead  

3. **False Positives:**
   - Track suspicion score  
   - Require multiple misses before marking dead  

4. **Rejoin:**
   - Node broadcasts rejoin message  

---

## Q9: Build a Simple Distributed Key-Value Store  
**Constraint:**  
> No consistent hashing library, no DB.

### Answer:

### Strategy:

1. **Manual Consistent Hashing Ring:**
   - Divide hash space into N segments  
   - Assign each segment to node  

2. **Put:**
   - hash(key) → find segment → owner node  

3. **Replication:**
   - Replicate segment data to next R nodes in ring  

4. **Failure:**
   - If node dies → next replicas serve traffic  

5. **Recovery:**
   - New node takes over segments and loads data from replicas  

---

## Q10: Implement a WAL-Based Database From Scratch  
**Constraint:**  
> Must survive crashes and ensure writes won’t get reordered.

### Answer:

### Strategy:

1. **Append-only write log**  
2. **Each record includes:**
   - key  
   - value  
   - CRC  
   - sequence number  

3. **Memory Table (Memtable):**
   - In-memory hash map  
   - Flush to SSTable when full  

4. **Compaction:**
   - Merge SSTables periodically  

5. **Crash Recovery:**
   - Rebuild memtable from WAL  
   - Replay entries in order  

This is essentially implementing an LSM-tree database (like RocksDB).

---

# Bare-Metal Design Thinking (CTO-Level Insight)

When constrained to CPU + Memory + Disk + Network:

1. **Durability = fsync + WAL**  
2. **Ordering = sequence numbers**  
3. **Idempotency = deduplication logs**  
4. **Replication = version vectors or gossip**  
5. **Fault detection = heartbeats**  
6. **Coordination = leases**  
7. **State machine = append-only log**  
8. **Recovery = replay logs**  
9. **Consistency = app-layer conflict resolution**  
10. **Scalability = partitioning + compaction**

These are the primitive building blocks of ALL distributed systems.
