# System Design Interview Q&A – Set 4 (Harder)

---

## Q1: Design a Global-Scale Messaging System (Like Kafka)

### Answer:

### Functional Requirements:
- Append-only log of events
- Publish-subscribe model
- Durability and ordering within a partition
- High throughput (millions of messages/sec)

### Non-Functional:
- Partition tolerance
- Horizontal scalability
- At-least-once delivery (baseline)

### Concepts:
- **Topic**: category of messages
- **Partition**: ordered, append-only log
- **Broker**: server that hosts partitions
- **Consumer Group**: group of consumers sharing work

### High-Level Architecture:
Producers → Brokers (cluster) → Consumers  

### Data Distribution:
- Partition key (e.g., user_id) → hash → partition
- Within partition, messages are strictly ordered

### Replication:
- Each partition has **leader** + **followers**
- Writes go to leader, replicated to followers
- Use quorum (e.g., ISR – in-sync replicas) for durability

### Consumer Offsets:
- Each consumer group tracks offset per partition
- Offsets stored in internal topic or external store
- Consumer can replay by resetting offsets

### Scaling:
- Add more partitions for higher parallelism
- Add brokers and rebalance partitions
- Use SSD and sequential disk writes

---

## Q2: Design a Global Distributed Database (Like Google Spanner – Simplified)

### Answer:

### Functional Requirements:
- Globally distributed
- Strong consistency (read-after-write globally)
- SQL queries & transactions

### Core Ideas:
- **Sharding**: split data into ranges (by primary key)
- **Replication**: multiple replicas per region
- **Consensus**: Paxos/Raft for each shard
- **TrueTime / synchronized clocks** (conceptual) for external consistency

### High-Level Architecture:
Client → Query Router → Tablet Servers → Storage  
Control Plane manages:
- Shard placement
- Rebalancing
- Membership & metadata

### Transactions:
- Two-phase commit across shards
- Each shard uses consensus to agree on writes
- Timestamps used to define serialization order

### Scaling:
- Split hot ranges
- Move shards closer to read-heavy regions
- Use read-only replicas for analytics

### Trade-offs:
- Higher write latency due to global consensus
- Excellent consistency guarantees

---

## Q3: Design a Multi-Region Active-Active System (e.g., Global E-Commerce)

### Answer:

### Requirements:
- Serve traffic from multiple regions
- Low latency for users worldwide
- Survive complete region failure
- Data must eventually converge

### Architecture:
Client → Geo-DNS / Anycast → Regional Frontend → Regional Backend → Regional DB/Cache  
Regions connected via replication (async or semi-sync)

### Data Strategies:
- **Read-local, write-local, replicate async**
- Conflict resolution strategies:
  - Last write wins (with timestamps)
  - CRDTs for some data types (counters, sets)
  - Application-specific rules

### Use-Cases:
- **Strong-consistency-needed data**: e.g., payments → single-region writes or global DB (Spanner-like)
- **Eventual-consistency data**: carts, recommendations → region-local, async replicate

### Failover:
- Health-check regions
- DNS or load balancer reroutes traffic if region down
- Handle data drift after failover

---

## Q4: Design a Large-Scale Search Engine (Google-like, High-Level)

### Answer:

### Requirements:
- Crawl the web
- Build index
- Answer queries in milliseconds
- Rank results by relevance

### Pipeline:

#### 1. Crawling:
URL Frontier → Fetcher → Parser → Content Store

#### 2. Indexing:
Content Store → Indexer → Inverted Index

#### 3. Querying:
User Query → Query Parser → Index Lookup → Ranking → Results

### Inverted Index:
- Mapping: term → list of (doc_id, positions, weights)
- Sharded by term range (e.g., `a-c`, `d-f`, etc.)

### Ranking:
- TF-IDF / BM25-like scoring
- PageRank (link-based)
- Modern search: ML-based ranking models

### Scaling:
- Multiple index shards
- Replicas per shard
- Query routing to relevant shards
- Cache top queries and results

---

## Q5: Design a Real-Time Bidding (RTB) System for Ads

### Answer:

### Requirements:
- Handle bid requests in ~100ms end-to-end
- Real-time auctions for each impression
- Multiple demand sources

### Flow:
1. User opens page → Ad Exchange sends Bid Request  
2. RTB system queries DSPs / bidders  
3. Bidders respond with bids + creatives  
4. RTB picks highest bid (meeting rules)  
5. Ad served and impression tracked  

### Architecture:
Publisher → Ad Exchange → Bidder(s) → RTB Engine → Creative Delivery

### Key Challenges:
- Very low latency
- High throughput
- Budget and pacing control

### Design:
- In-memory representations of campaigns
- Pre-filter campaigns by targeting (geo, device, time)
- Real-time counters for spend and impressions
- Use efficient binary protocol and network IO

---

## Q6: Design a Large-Scale Rate Limiting System (Central + Distributed)

### Answer:

### Requirements:
- Global rate limits (per API key, IP, tenant)
- Local, low-latency enforcement
- Accurate enough at scale

### Architecture:
- **Local Enforcer**: runs near app instances, uses token bucket in memory
- **Central Coordinator**: maintains global quotas and sync with local enforcers

### Approach:
- Use **leaky bucket / token bucket** at local nodes
- Periodically sync counters with central service
- For strict global limits:
  - Counters stored in Redis Cluster
  - Use Lua scripts for atomic ops

### Scaling:
- Shard counters by key
- Use approximate algorithms (like probabilistic counters) for non-critical stats
- Backpressure / 429 responses on limit breach

---

## Q7: Design an Event-Driven Microservices Platform

### Answer:

### Requirements:
- Services communicate via events
- Loose coupling
- Reliable delivery
- Handle retries and dead letters

### High-Level Architecture:
Services → Event Bus (Kafka/RabbitMQ/Pulsar) → Services  

### Concepts:
- **Producers** publish events
- **Consumers** subscribe to topics
- **Event schema** (e.g., Avro, Protobuf)
- **DLQ (Dead Letter Queue)** for failures

### Patterns:
- Event sourcing
- CQRS (Command Query Responsibility Segregation)
- Saga pattern for distributed transactions

### Reliability:
- At-least-once delivery
- Idempotent consumers
- Retry policies and DLQ for poison messages

---

## Q8: Design a Feature Store for ML Systems

### Answer:

### Requirements:
- Central store of ML features
- Online low-latency access (serving)
- Offline batch access (training)
- Point-in-time correctness (no data leakage)

### Architecture:
- **Offline Store**: Data lake / warehouse (S3, BigQuery, etc.)
- **Online Store**: Low-latency KV store (Redis, Cassandra, etc.)
- **Registry**: Metadata for feature definitions, owners, versions

### Pipeline:
Raw Data → ETL / Stream Processing → Feature Computation →  
- Write to Offline Store (batch)  
- Write to Online Store (aggregated)  

### Serving:
Model Service → Feature Store SDK → Online Store → Return features

### Key Concerns:
- Consistency between offline and online features
- Backfills with correct event timestamps
- Automated TTL and materialization jobs

---

## Q9: Design a Privacy-Compliant Analytics System (GDPR-ish)

### Answer:

### Requirements:
- Collect user events
- Anonymize or pseudonymize data
- Allow user data deletion on request
- Aggregate reports

### Architecture:
Client → Event Collector → Anonymization Service → Data Warehouse → Reporting

### Techniques:
- Use pseudonymous IDs (random user_id, no direct PII)
- Separate PII (email, name) in isolated store with strict access
- Map PII ↔ anonymous user_id via secure mapping service

### “Right to be forgotten”:
- Locate user via PII mapping
- Delete or scramble corresponding user_id’s events or mark as deleted
- Use soft delete + periodic hard deletion jobs

### Aggregations:
- Always aggregate to cohorts/summaries in reports
- Use differential privacy techniques for high-privacy requirements

---

## Q10: Design a Large-Scale Object Storage System (Like S3)

### Answer:

### Requirements:
- Store billions of objects
- High durability and availability
- Versioning, lifecycle rules

### Data Model:
- Bucket → Objects (key, value, metadata)
- Key is opaque string (can encode path-like structure)

### Architecture:
Client → Frontend (REST API) → Metadata Service → Storage Nodes  

### Metadata Service:
- Stores mapping: (bucket, key) → object location(s)
- Stores versioning, ACLs, tags
- Strong consistency for metadata

### Data Storage:
- Objects stored as chunks across multiple storage nodes
- Replication across multiple availability zones
- Use erasure coding for better storage efficiency

### Durability:
- Multiple replicas or erasure-coded fragments
- Background scrubbing to detect & repair corruption
- Write path ensures data written to quorum of nodes

### Scaling:
- Partition metadata by bucket/key hash
- Add more storage nodes and rebalance
- Use consistent hashing or range partitioning

---

# Recap: Hard-Level System Design Answer Checklist

For harder questions, explicitly discuss:

1. **Consistency model** (strong, eventual, causal)  
2. **Replication & consensus** (Paxos/Raft, leader/follower, quorum)  
3. **Sharding & placement** (by key, region, tenant)  
4. **Failure modes** (network partition, region down, split-brain)  
5. **Backpressure & overload control**  
6. **Data lifecycle** (retention, compaction, archiving)  
7. **Security & compliance** (PII, encryption, audit)  
8. **Cost trade-offs** (storage vs compute vs latency)
