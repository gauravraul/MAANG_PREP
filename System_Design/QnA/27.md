# System Design Interview Q&A â€“ Set 19 (Reverse System Design / Diagnose from Symptoms)

> ğŸ§  These questions test **production intuition**.  
> Interviewer gives symptoms â†’ you infer architecture flaws â†’ propose fixes.

---

## Q1: System works fine normally, but during traffic spikes:
- Latency jumps suddenly
- DB CPU goes to 100%
- Cache hit rate drops sharply

### Answer:

### Likely Causes:
- Cache stampede
- Hot key expiration
- Thundering herd on DB

### Diagnosis Steps:
- Check cache TTL alignment
- Identify hot keys
- Check request fan-out patterns

### Fixes:
- Soft TTL + serve stale
- Request coalescing (single flight)
- Pre-warm hot keys
- Rate-limit DB fallbacks

---

## Q2: Messages are occasionally processed twice, but never lost

### Answer:

### Likely Architecture:
- At-least-once delivery queue
- No deduplication

### Root Cause:
- Consumer crashes after processing but before ACK

### Fix:
- Idempotent consumers
- Dedup store keyed by message_id
- Commit offset/state atomically with side effects

---

## Q3: Writes are fast, but users complain reads show stale data for minutes

### Answer:

### Likely Architecture:
- Async replication
- Eventual consistency
- Read replicas used heavily

### Fix Options:
- Read-your-writes via session affinity
- Allow quorum reads for critical flows
- Reduce replication lag
- Cache invalidation on write

---

## Q4: Leader election keeps flapping every few minutes

### Answer:

### Likely Causes:
- Aggressive timeouts
- GC pauses
- Network jitter

### Fixes:
- Increase election timeout
- Add randomized backoff
- Tune GC
- Isolate leader workload

---

## Q5: System slows down gradually over days, then crashes

### Answer:

### Likely Causes:
- Memory leak
- Unbounded cache
- Log accumulation

### Diagnosis:
- Heap growth trends
- Open file descriptors
- Disk usage

### Fixes:
- Add TTL / eviction
- Log rotation
- Backpressure
- Periodic restarts (last resort)

---

## Q6: Some users see incorrect leaderboard ranks after outages

### Answer:

### Likely Architecture:
- Eventual consistency
- Local caches during outage
- Non-idempotent merges

### Fix:
- CRDT-based counters
- Deterministic merge rules
- Reconciliation jobs
- Mark UI as approximate during recovery

---

## Q7: Deployments cause random 500 errors for a few minutes

### Answer:

### Likely Causes:
- Non-backward-compatible schema
- Cold starts
- Cache invalidation

### Fix:
- Backward-compatible schema changes
- Blue/green or canary deploys
- Warm up caches
- Gradual traffic shift

---

## Q8: One tenant slows down everyone else

### Answer:

### Likely Issue:
- Noisy neighbor problem

### Fixes:
- Per-tenant quotas
- Rate limiting
- Resource isolation
- Priority scheduling

---

## Q9: System recovers from crashes, but data is corrupted

### Answer:

### Likely Cause:
- Partial writes
- No WAL or checksum

### Fix:
- Write-ahead log
- CRC checks
- Atomic writes
- Fencing on restart

---

## Q10: Metrics show perfect health, but users complain

### Answer:

### Likely Issue:
- Monitoring wrong signals
- Missing user-centric metrics

### Fix:
- Measure SLOs (latency, error rate)
- Add real user monitoring
- Track tail latency (p95/p99)
- Instrument business KPIs

---

## Q11: API latency increases linearly with traffic

### Answer:

### Likely Cause:
- Synchronous fan-out calls
- N+1 request pattern

### Fix:
- Batch requests
- Async processing
- Cache intermediate results
- Reduce call graph depth

---

## Q12: System fails completely when a downstream service is slow

### Answer:

### Likely Issue:
- No backpressure
- No circuit breakers

### Fix:
- Circuit breakers
- Timeouts
- Bulkheads
- Graceful degradation

---

## Q13: After region failover, some data disappears

### Answer:

### Likely Cause:
- Async replication lag
- Writes not durable globally

### Fix:
- Local quorum durability
- Sync critical writes
- Replication lag monitoring
- Write fencing during failover

---

## Q14: Search results change order between pages

### Answer:

### Likely Cause:
- OFFSET-based pagination
- No stable sort key

### Fix:
- Cursor-based pagination
- Snapshot tokens
- Deterministic ordering

---

## Q15: Queue backlog grows even though consumers are healthy

### Answer:

### Likely Cause:
- Poison messages
- Retry storms

### Fix:
- Dead Letter Queue (DLQ)
- Retry limits
- Alert on poison rate

---

## Q16: System works in staging but fails in production

### Answer:

### Likely Causes:
- Data scale differences
- Traffic patterns
- Missing quotas

### Fix:
- Load testing
- Chaos testing
- Production-like staging
- Gradual rollout

---

## Q17: One API endpoint causes cascading failures

### Answer:

### Likely Cause:
- Shared resource exhaustion

### Fix:
- Per-endpoint limits
- Bulkhead isolation
- Separate thread pools

---

## Q18: Cache hit rate is high, but latency still bad

### Answer:

### Likely Causes:
- Cache deserialization cost
- Network latency to cache
- Oversized values

### Fix:
- Compress values
- Move cache closer
- Store precomputed responses

---

## Q19: System shows split-brain behavior

### Answer:

### Likely Cause:
- No quorum enforcement
- Clock-based leader election

### Fix:
- Majority-based consensus
- Leader fencing
- Avoid wall-clock decisions

---

## Q20: Everything looks correct on paper, but system still fails under load

### Answer:

### Likely Root Cause:
- Missing backpressure
- Assumed â€œhappy pathâ€

### Fix:
- Load shedding
- Queue limits
- Explicit overload modes
- Chaos experiments

---

# Reverse System Design Skill

In interviews, do this:

1. Identify symptom  
2. Infer missing primitive  
3. State violated invariant  
4. Propose minimal fix  
5. Add observability  

Example:
> â€œThis smells like at-least-once delivery without idempotency. Iâ€™d add a dedup store and make consumers idempotent.â€

---

If you want next:

ğŸ”¥ **Set 20 â€” Hiring Manager Round: â€œWhy will this fail in production?â€**  
ğŸ”¥ **FINAL master cheat sheet (all sets condensed)**  
ğŸ”¥ **Mock interview with live grading**

Just say **Next** ğŸš€
