# System Design Interview Q&A – Set 11 (Adversarial "What-If" Scenarios)

> ⚠️ These are *true interview killer questions* — no fixed answers.
> Each requires you to handle **unexpected constraints mid-design**.

---

## Q1: Design a Global Messaging System — THEN interviewer says:
> Now assume 30% of regions go offline every day for 2 hours. How do you guarantee message ordering globally?

### Answer:

### Invariants:
- Ordering guaranteed **per partition**, not globally
- Offline regions **buffer writes locally**
- Global ordering = (partition_id, sequence_number, timestamp)

### New Strategy:
1. **Local append-only log per region**
2. **Async cross-region replication**
3. **Monotonic reconciliation** rules:
   - If two versions conflict, pick higher sequence number → last-write-wins per partition
   - For cross-partition ordering → build **causal ordering graph**, not strict global FIFO

### Degradation Plan:
- During regional outage:
  - Local messaging continues
  - DR lag tolerated
  - New partition snapshots queued
- After recovery:
  - Anti-entropy merge
  - Deduplication and causal stitching

---

## Q2: Design a Distributed Rate Limiter — interviewer says:
> But Redis is down for 10 minutes. How do you enforce limits WITHOUT centralized counters?

### Answer:

### New Guarantees:
- **Local enforcement = approximate**
- **Global strict enforcement = eventual**

### Strategy:
- Local token buckets in each node
- Every node periodically:
  - Broadcasts delta usage
  - Consensus group aggregates global usage asynchronously
- If global usage uncertain → **fallback policy**:
  - Default deny if overload probability high
  - Allow if low-risk tenant

### Observability:
- Metrics for “risk score”
- Shadow tracking to measure drift

---

## Q3: Design a Real-Time Recommendation Engine — interviewer says:
> But half of the feature store is delayed by 10 seconds. How do you serve recommendations safely?

### Answer:

### New SLA:
- Model must not break due to partial features
- Features are **versioned**

### Strategy:
- Feature fetch returns:
  - value + freshness + version
- Model Serving:
  - Uses **fallback imputation** when stale
  - Business logic:
    - Stale critical feature → downgrade performance or skip
- Scoring metric:
  - Infer confidence
  - Down-rank results with low feature freshness

### Observability:
- % of stale feature usage
- Confidence-weighted CTR

---

## Q4: Design a Payment Ledger — interviewer says:
> But compliance requires that ledger mutations must be auditable even if machines are hacked. How?

### Answer:

### Immutable Guarantees:
- Append-only log
- Cryptographic integrity
- External verification

### Solution:
- Ledger segments signed using Merkle trees
- Periodic checkpoints notarized externally
- HSM-based write authorizations
- Tamper-proof:
  - If any historical mutation occurs → Merkle proof breaks immediately

### Blast Radius:
- If machine compromised:
  - Writes revoked at HSM layer
  - Ledger validation fails → quarantine node

---

## Q5: Design an E-Commerce Checkout — interviewer says:
> Assume DB is eventually consistent and you CAN’T use transactions. Inventory must never go negative.

### Answer:

### Strategy:
- Reservation-based flow:
  1. Reserve inventory (decrement)
  2. If payment fails → release inventory
- Reservation is:
  - Lease with TTL
  - Auto-expiry

### Guarantees:
- Inventory consistency global via:
  - Versioned compare & set write
  - Quorum acknowledgment
- No double-spend:
  - Use **idempotent reservation IDs**

---

## Q6: Design a Metric System — interviewer says:
> You must guarantee **exactly-once ingestion**, but agents may resend metrics multiple times.

### Answer:

### Invariants:
- Deduplication window per metric key
- Monotonic ingestion version

### Strategy:
- For each metric key:
  - Maintain `(metric_id, sequence)`
  - Duplicate detection via:
    - Dedup cache (TTL = 10–30 mins)
- Storage applies only if:
  - incoming.sequence > stored.sequence

---

## Q7: Design a Distributed Search Engine — interviewer says:
> But hardware cost must be cut by 70%. What can you remove without destroying UX?

### Answer:

### Trade-offs:
- Keep:
  - inverted index
  - relevance engine
  - front caching
- Remove:
  - hot replicas for every shard
  - real-time re-indexing

### Optimization:
- Cold shards on object storage
- Hot shards cached dynamically
- Prefetch based on trending queries

---

## Q8: Design a Video Streaming Platform — interviewer says:
> Now internet is unstable for 30–40% users. How to guarantee smooth playback?

### Answer:

### Strategy:
- Multi-bitrate streaming
- Chunk prefetch
- Edge caching
- QoS fallback:
  - degrade resolution, never pause
- Player-side:
  - ML prediction for next needed chunks
  - jitter buffer + adaptive windowing

---

## Q9: Design an IoT Update System — interviewer says:
> Half the devices wake up only once a week and have 2G bandwidth.

### Answer:

### Strategy:
- Delta updates only (binary diff)
- Signed config bundles
- Device-local rollback points
- Progressive rollout rules:
  - geographic
  - risk score based

### Sync:
- Best-effort pull with exponential retry
- Security:
  - local validation before apply

---

## Q10: Design a Self-Healing Microservices Platform — interviewer says:
> But you must not trigger cascading remediation loops.

### Answer:

### Strategy:
- ML anomaly detection → remediation candidate list
- Before taking action:
  - shadow evaluate improvement plan
  - require quorum confidence
- Cooldown timers:
  - per-service remediation throttling
- Blast radius control:
  - only take local fix per iteration, never global

---

# Interviewer Trick — How to Answer Adversarial Scenarios

When they change constraints mid-interview:

1. **Restate new invariants**
2. **Define new consistency or correctness goals**
3. **Shift from strict guarantees → probabilistic trade-offs**
4. **Introduce self-healing, TTLs, backpressure, idempotency**
5. **Explain graceful degradation path**
6. **Explain observability at each new constraint**
7. **Contain blast radius before full failure**
8. **Quantify drift, stale data, or delivery edge cases**
9. **Show rollback, canaries, recovery semantics**
10. **Treat outages as normal, not exceptional**
