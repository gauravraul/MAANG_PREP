# System Design Interview Q&A – Set 13 (Nightmare Scenarios / Real MAANG “Bar-Raiser” Questions)

> These **don’t have closed-form answers** — they test depth, composability, invariants, degradation, and ability to adapt designs under failure.

---

## Q1: Design a Distributed OLTP Store — THEN interviewer says:
> Every write must be durable **even if region crashes mid-write**, AND you must confirm success to the client in < 50ms.

### Answer:

### New Invariants:
- A write is “durable” only after:
  - Persisted to quorum (replicated)
  - **Without waiting for full cross-region replication**
- Client acknowledgment must be < 50ms

### Strategy:

1. **Regional Quorum Commit:**
   - On write:
     - Local region replicas commit synchronously (Raft/Paxos).
     - Client gets ACK once **local quorum** completes.

2. **Async Cross-Region Replication:**
   - After ACK:
     - Replicate async to remote regions.
     - If remote region fails, backlog is queued.

3. **Crash Recovery:**
   - If region crashes before async completes:
     - Local durable quorum ensures no data loss.
     - On restart:
       - Replica catches up from peer logs.

4. **Observability:**
   - Track:
     - replication lag
     - backlog growth
     - durability SLA

5. **Trade-Off:**
   - **Latency vs Global Safety**:
     - we choose local strong durability + eventual global durability

---

## Q2: Design a Distributed Workflow Engine — THEN interviewer says:
> A workflow step may run for 3 hours, and **nodes may restart mid-step**. Guarantee workflow correctness.

### Answer:

### Invariants:
- Long-running step must be **resumable & idempotent**.
- Workflow state must be durable & replayable.

### Strategy:

1. **Durable Step State:**
   - Step writes progress markers:
     - checkpoints, offsets, partial outputs
   - On failure:
     - Restart from last checkpoint

2. **Idempotent Execution:**
   - Actions:
     - must be idempotent
     - OR protected by transaction tokens

3. **Task Ownership:**
   - Distributed lease:
     - Node executes step only while holding lease
     - If node fails → lease expires → other node picks up

4. **Persistence:**
   - Workflow metadata persists to durable log or KV store
   - Every event recorded:
     - `started`, `heartbeat`, `checkpoint`, `completed`

5. **Observability:**
   - Track:
     - step runtime
     - failure retries
     - checkpoint gaps

---

## Q3: Design a ML Online Training System — THEN interviewer says:
> Training data is streaming and **may arrive late or out of order**. Model updates must be accurate.

### Answer:

### Invariants:
- Late data must not break correctness
- Updates must be exactly-once per event

### Strategy:

1. **Stream Processor with Event Time Semantics:**
   - Training engine groups events by **event-time windows**, not arrival-time.
   - Late events allowed up to N minutes.

2. **State Store:**
   - Each training update keyed by:
     - `(window_id, event_id)`
   - Dedup enabled to avoid double update.

3. **Revisions:**
   - If late event arrives after window is finalized:
     - Trigger **model delta update**
     - Or adjust weights incrementally

4. **Degradation:**
   - For super late events:
     - Decide policy:
       - ignore
       - record separately for offline retraining

---

## Q4: Design a Distributed Config System — THEN interviewer says:
> Config changes should propagate globally in < 5 seconds, BUT must be safe. How do you guarantee atomicity AND safety?

### Answer:

### Requirements:
- Safe rollout (no mass outage)
- Global propagation (< 5 sec)
- Atomic: version monotonicity per service

### Strategy:

1. **Versioned Config Bundles:**
   - Each bundle: signed & immutable
   - Version = monotonically increasing

2. **Push + Pull Hybrid:**
   - Push notifications for change
   - Clients confirm version fetch

3. **Safety Checks Before Activation:**
   - Small **canary rollout**
   - Real-time KPIs:
     - latency
     - error rate
   - If unhealthy → rollback globally

4. **Activation:**
   - Atomic switch per client:
     - Replace config only if:
       - version >= current
       - validation passes

---

## Q5: Design a Search System — THEN interviewer says:
> You must guarantee **correct pagination**, even while index is changing rapidly.

### Answer:

### Invariants:
- Pagination must be stable for duration of the user session
- Index can mutate concurrently

### Strategy:

1. **Snapshot Query:**
   - Query returns:
     - `snapshot_id`
     - first page docs

2. **Pagination:**
   - Next pages fetched under same `snapshot_id`
   - Stored snapshot view ensures stable ordering

3. **Snapshot Storage:**
   - Snapshot is:
     - metadata-only
     - immutable version of index pointers
   - TTL cleanup afterward

---

## Q6: Design a Metrics Platform — THEN interviewer says:
> You must guarantee **exact percentile estimates** for p95 / p99 across multi-region data.

### Answer:

### New Complexity:
- Percentile must be exact (not approximate)
- Across regions with partial failure

### Strategy:

1. **Two-Level Aggregation:**
   - Region-level:
     - Stores full histogram or sorted bucket data
   - Global:
     - Merges histograms using exact bin boundaries

2. **Failure Mode:**
   - If region unavailable:
     - freeze metrics at last known
     - show federation health indicator
   - Later merge backlog

---

## Q7: Design a Distributed Pub/Sub — THEN interviewer says:
> Consumers MUST see messages **exactly once**, but consumer code may crash mid-processing.

### Answer:

### Strategy:

1. **Message Leasing:**
   - Consumer gets message with lease:
     - must ACK before TTL
   - If no ACK:
     - message re-delivered

2. **Idempotent Consumer:**
   - Processing must be idempotent
   - Use consumer state store:
     - `(message_id, status)`

3. **Exactly-Once Semantics:**
   - “At-least-once + idempotency” = exactly once visible effect

---

## Q8: Design a Distributed Ledger — THEN interviewer says:
> Ledger must **survive partial writes, torn pages, and corrupted sectors**.

### Answer:

### Strategy:

1. **Append-Only Log:**
   - Block writes atomic at segment level
   - CRC checksum per block

2. **Fencing:**
   - On restart:
     - detect corrupted block
     - truncate back to last valid block

3. **Multiple Copies:**
   - quorum replication
   - each replica validates checksum before applying

---

## Q9: Design a Global ML Feature Store — THEN interviewer says:
> Some features must be served fresh from stream; others come from batch jobs. You must **never mix versions**.

### Answer:

### Strategy:

1. **Feature Version Bundles:**
   - Group features under same `version_set_id`
   - All features served with same version ID

2. **Late Features:**
   - Batch store and streaming layer reconciled
   - Late features applied via incremental deltas

3. **Serving:**
   - Client query requests:
     - `version_set_id`
     - engine returns only features matching that version

---

## Q10: Design a Distributed Training Job Queue — THEN interviewer says:
> GPUs can randomly fail mid-training. You must guarantee **progress preservation** and job completion.

### Answer:

### Strategy:

1. **Checkpoint Streaming:**
   - Periodic model state snapshots to shared storage
   - If GPU crash → resume from latest checkpoint

2. **Sharded Optimizer:**
   - State distributed across nodes
   - Resume orchestrated by scheduler

3. **Failure Scope:**
   - Granular leases per task
   - Node failure doesn’t abort whole training

---

# Final Adversarial Interview Strategy

When interviewer keeps changing constraints:

1. Re-state invariants  
2. Redefine guarantees (strong / eventual / session)  
3. Clarify **what correctness MUST mean** now  
4. Add:
   - idempotency  
   - leases  
   - async replication  
   - snapshots  
   - backpressure  
   - fallback modes  
5. Add **metrics + audit + observability**  
6. Always include **graceful degradation path**  
7. NEVER assume perfect clocks, networks, or machines  
8. Handle **replay, late data, partial writes, partitions, retries**  
9. End with:
   - how the system heals  
   - how you detect drift  
   - how you recover
