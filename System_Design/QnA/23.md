# System Design Interview Q&A – Set 16 (Deep Theory + Practical Hybrid)

---

## Q1: Explain Lamport Clocks vs Vector Clocks. When to use each?

### Answer:
- **Lamport Clocks**
  - Scalar logical clock `L`.
  - Rules: increment on local event; on receive set `L = max(L, L_recv) + 1`.
  - Provides **partial ordering** (happened-before ⇒ clock order), but **not causal**: `L(a) < L(b)` doesn't imply `a` caused `b`.
  - Cheap, small state (one integer).
  - Use when you need a simple monotonic ordering (e.g., to break ties) but don't need to detect concurrent events.

- **Vector Clocks**
  - Vector `V[node_count]` tracking per-node counters.
  - Merge: element-wise max. Compare vectors to determine `a → b`, `b → a`, or **concurrent**.
  - Detects **causality** and concurrency explicitly.
  - Cost: vector size = number of nodes (or use sparse representations).
  - Use when you must detect concurrent updates (e.g., CRDT merges, optimistic replication reconciliation).

---

## Q2: State CAP theorem precisely and give practical design patterns for each trade-off.

### Answer:
- **CAP (Brewer)**: In a distributed system under a **network partition (P)** you must choose between **Consistency (C)** and **Availability (A)**.
  - **C**: All clients see the same (single) copy (linearizability or strong consistency).
  - **A**: Every request receives a response (may be stale) despite partitions.

- **Practical patterns**
  - **CP systems** (choose consistency over availability): use consensus (Raft/Paxos). Good for metadata, leader election, config, payment ledgers.
  - **AP systems** (choose availability over consistency): use eventual consistency, CRDTs, conflict resolution, multi-master async replication. Good for user profiles, presence, likes.
  - **CA** (no partitions): virtually impossible at scale; CA design assumes network reliability—useful in single datacenter or when partition handling is out of scope.

- **Hybrid approaches**
  - **Session guarantees** (read-your-writes) — give per-client C while allowing global A.
  - **Tunable consistency** — allow client to request QUORUM reads/writes (C) or ANY (A).
  - **CRDTs for conflict-free availability** with eventual convergence.

---

## Q3: Explain CRDTs. Give two concrete CRDT types and use-cases.

### Answer:
- **CRDT (Conflict-free Replicated Data Type)**: data structure designed to converge automatically under concurrent updates without coordination. Two classes:
  - **State-based (Convergent, CvRDT)**: replicas exchange full state; merge via associative, commutative, idempotent operation (e.g., element-wise max).
  - **Operation-based (Commutative, CmRDT)**: replicas send operations that must be delivered reliably and applied in causal order.

- **Examples**
  - **G-Counter** (grow-only counter): each node increments its own slot; global value = sum. Use-case: view counters, likes where only increments occur.
  - **OR-Set (Observed-Remove Set)**: supports add/remove; tags each add with unique id; removal removes specific tags. Use-case: collaborative editing lists, membership sets.

- **When to use**: when you need high availability, partition tolerance, and automatic convergence (distributed caches, offline-first apps).

---

## Q4: Describe Raft and how it handles leader failure, network partition, and log replication.

### Answer:
- **Raft basics**
  - Leader-based consensus protocol with three roles: leader, follower, candidate.
  - Uses terms and majority quorums.
  - Leader handles client writes, appends entries to its log, replicates to followers, commits when replicated to quorum.

- **Leader failure**
  - Followers time out without heartbeats → become candidates → start election (increment term, request votes).
  - A new leader is elected by majority votes.

- **Network partition**
  - Minority partition cannot elect a leader (no majority), so system becomes unavailable for writes (CP behavior).
  - Majority partition continues to accept writes.

- **Log replication & safety**
  - Leader includes `prevLogIndex` and `prevLogTerm` to ensure follower consistency.
  - Only entries from a leader that is committed in a term can be returned to clients (safety guarantee).
  - Followers that are behind are brought up-to-date via AppendEntries; leader retries until follower caught up.

---

## Q5: Paxos vs Raft — when to pick which?

### Answer:
- **Paxos**
  - Proven, minimal core, more academic; harder to implement correctly and reason about; flexible (Multi-Paxos for steady leadership).
- **Raft**
  - Designed for understandability and engineering correctness; includes leader election, log replication, and membership change procedure.
- **Pick Raft when**: engineering teams want an easier-to-understand consensus implementation (metadata stores, config, small-cluster coordination).
- **Pick Paxos**: if you need a minimal theoretical primitive or interoperability with systems built on Paxos variants; but in practice, Raft is common.

---

## Q6: Explain strong consistency models—linearizability vs serializability vs sequential consistency.

### Answer:
- **Linearizability**
  - Operations appear to occur atomically at a single point in time between invocation and response; respects real-time order.
  - Strongest: useful for single-object correctness (e.g., distributed lock, register).

- **Serializability**
  - Transactional model: concurrent transactions appear to run in some serial order; does not require real-time order.
  - Important in databases for correctness across multiple keys.

- **Sequential Consistency**
  - All operations appear in some sequential order that respects program order per client (not necessarily real-time).
  - Weaker than linearizability (allows reordering across clients as long as program order preserved).

- **When to use**
  - **Linearizability** for coordination primitives (locks, leader election).
  - **Serializability** for ACID transactional databases.
  - **Sequential** for certain replicated systems where real-time ordering isn’t required.

---

## Q7: Describe snapshot isolation, write skew, and how to avoid anomalies.

### Answer:
- **Snapshot Isolation (SI)**
  - Each transaction reads from a consistent snapshot (typically at start time). Writes are applied if no write-write conflicts (concurrent writes to same row).
  - SI prevents many anomalies but allows **write skew**.

- **Write Skew**
  - Two concurrent transactions read overlapping data and write disjoint sets, leading to invariant violation (e.g., doctor on-call example).

- **Avoiding anomalies**
  - Use **serializable isolation** (e.g., SSI) that detects dangerous patterns and aborts conflicting txns.
  - Use explicit locking on read sets or application-level checks.
  - Prefer serializable or employ compensating checks for invariants.

---

## Q8: Two-Phase Commit (2PC) vs Sagas — pros & cons.

### Answer:
- **2PC**
  - Coordinator asks participants to PREPARE; if all yes → COMMIT; otherwise ABORT.
  - Provides atomic multi-resource commit (ACID across services).
  - **Cons**: blocking if coordinator fails; requires participants to hold locks/resources until commit/abort.

- **Sagas**
  - Long-running transactions broken into sequence of local transactions with compensating actions on failure.
  - **Pros**: non-blocking, better for long-running business processes (booking/reservation flows).
  - **Cons**: requires compensating actions (not always possible or simple); eventual consistency.

- **When**
  - Use **2PC** for short, critical, strongly consistent financial transfers (if locking is acceptable).
  - Use **Sagas** for user flows (order->payment->shipment) where eventual consistency and compensation are acceptable.

---

## Q9: FLP impossibility and practical workarounds.

### Answer:
- **FLP theorem**: In an asynchronous network with even one faulty process, deterministic consensus cannot be guaranteed to always terminate.
- **Workarounds used in practice**
  - **Randomized algorithms**: add randomness to ensure progress (Paxos/Raft use timeouts and randomized election delays).
  - **Partial synchrony model**: assume network is eventually synchronous (practical systems assume bounded delays most of the time).
  - **Failure detectors / timeouts**: use heartbeat and leader election heuristics to make progress in typical conditions.

---

## Q10: Byzantine Fault Tolerance (BFT) basics and trade-offs.

### Answer:
- **BFT systems** tolerate arbitrary faults (including malicious behavior).
- **Classic results**: to tolerate `f` Byzantine faults you need `3f + 1` replicas for safety.
- **Trade-offs**
  - Higher replication overhead and message complexity (often O(n²) messaging).
  - Higher latency; complex implementations (e.g., PBFT, Tendermint).
- **When to use**
  - When nodes are not trusted (multi-party financial systems, blockchain consensus for certain threat models).
- **Alternative**
  - Use crash-fault tolerant consensus (Raft/Paxos) when nodes are trusted; employ additional cryptographic checks for integrity.

---

# Closing Tips for Interviews (Theory + Practical)

1. Always state **consistency & availability** goals up front.  
2. Pick the **weakest** consistency model that satisfies correctness, to get better availability/latency.  
3. Use **practical examples** (e.g., leader election, shopping cart) to justify choices.  
4. When proposing consensus, explain **failure modes** (leader crash, network partition) and recovery.  
5. For distributed time, prefer **logical clocks** or **version vectors**; use TrueTime-like models only when available and justified.  
6. For production systems, add observability (metrics, tracing), operational playbooks, and safety nets (canaries, feature flags).
