# System Design Interview Q&A – Set 12 (Adversarial / “If X Fails AND Y Must Still Work”)

---

## Q1: Design a Global Key-Value Store — THEN interviewer says:
> Assume **network partitions are frequent** and you STILL must guarantee **read-your-writes** for clients. How?

### Answer:

### Invariants:
- Each client must see its own writes
- Global strong consistency NOT required
- Network partitions are normal

### Strategy:

1. **Session Affinity:**
   - Each client is pinned to a **home region** (or replica set).
   - All its reads/writes go to the same region as long as possible.

2. **Client-Side Metadata:**
   - Every write returns a **version vector** or `(region_id, logical_ts)`.
   - Client stores its **last-seen version**.

3. **Read-Your-Writes Protocol:**
   - On read:
     - Send last-seen version to server.
     - Server guarantees **returned version >= client version**.
     - If not locally available, server blocks or forwards internally.

4. **Partition Handling:**
   - If client can’t reach home region:
     - **Either**:
       - Say: “degraded mode, read-only from stale replica”.
       - OR queue writes locally until region available.
   - Explicitly tell interviewer: *“I choose availability OR strict read-your-writes; both across hard partition are impossible (CAP)”*.

---

## Q2: Design a Distributed Cache — THEN interviewer says:
> You must survive **complete cache flush** without bringing down DB. What do you do?

### Answer:

### Problem:
- Cache stampede: sudden surge of DB reads after cache loss.

### Strategy:

1. **Staggered Warm-Up:**
   - Rate-limit cache miss → DB lookup.
   - Use **token bucket** per key or per service.

2. **Request Coalescing:**
   - Only **single request** per key hits DB.
   - Others **wait** on same in-flight promise.

3. **Background Warming:**
   - Periodic jobs warm most popular keys.
   - On full flush, **do not** instantly re-warm all; use dynamic priority.

4. **Fail-Soft Strategy:**
   - For less-critical endpoints:
     - Serve **stale cache** (if persisted locally/disk).
     - Or serve **fallback defaults**.

5. **DB Protection:**
   - Circuit-breaker:
     - If DB is overwhelmed – throttle traffic.
     - Prefer to fail some requests than kill DB.

---

## Q3: Design an Order Processing System — THEN interviewer says:
> At-least-once delivery is guaranteed, so orders can be processed twice. Money must **never** be charged twice.

### Answer:

### Invariants:
- Payment must be **idempotent**.
- Duplicate messages are normal.

### Strategy:

1. **Idempotency Key:**
   - Each payment request has unique `idempotency_key` (e.g., `order_id`).
   - Store `idempotency_key → payment_status, response`.

2. **Payment Flow:**
   - On request:
     - Check idempotency store.
     - If exists:
       - Return stored response (don’t re-charge).
     - Else:
       - Charge customer.
       - Record result along with idempotency key.

3. **Storage:**
   - Idempotency store must be:
     - **Strongly consistent** (or at least per key).
     - Durable (e.g., SQL, strongly consistent KV).

4. **Ordering vs Payment:**
   - Order state machine:
     - `CREATED → PAYMENT_PENDING → PAID → FULFILLED`.
   - All state transitions are **idempotent** as well (compare-and-set).

---

## Q4: Design a Task Scheduler — THEN interviewer says:
> Tasks are scheduled on multiple nodes; **clocks are skewed** and you still want “run not earlier than X”.

### Answer:

### Problem:
- System clocks can’t be trusted.

### Strategy:

1. **Monotonic Time Per Node:**
   - Use **monotonic clock** for relative delays, not absolute wall time.
   - When task created, store **delay interval**, not absolute timestamp.

2. **Central Time Authority (Optional):**
   - Time Coordination Service:
     - Periodically synchronizes offset per node (NTP-like).
     - Exposes *“cluster time”* to schedulers.

3. **Scheduling Logic:**
   - Store task metadata:
     - `created_at_cluster_time`
     - `min_delay_ms`
   - Node gets cluster time + local offset:
     - Executes when `cluster_now >= created_at + delay`.

4. **Guardrails:**
   - If time is uncertain (no sync):
     - Use conservative delay (sleep extra).
     - Better late than early if that’s the invariant.

---

## Q5: Design a Notification System — THEN interviewer says:
> Sometimes external providers (email/SMS/push) **silently drop messages**. Users complain “I never got it”. What now?

### Answer:

### Requirements:
- End-to-end observability
- At-least “delivery attempt” visibility

### Strategy:

1. **Delivery Status Tracking:**
   - Notification record:
     - `requested`, `sent_to_provider`, `provider_ack`, `opened` (if possible).
   - Correlate each message with `notification_id`.

2. **Provider Webhooks:**
   - Use callbacks for:
     - Delivered
     - Bounced
     - Rejected
   - If provider does not support, periodically pull status.

3. **Redundancy:**
   - Multi-channel fallback:
     - If email fails → SMS or push (according to user preferences).
   - Multi-provider setup:
     - On high failure rate with provider A → switch to B.

4. **User-Facing Audit Trail:**
   - “Notification Center” in app:
     - Show that message was generated.
     - Even if provider dropped, user still sees it in app inbox.

---

## Q6: Design a Document Collaboration System — THEN interviewer says:
> Clients can go offline for hours, then come back with conflicting edits. No central “edit lock” is allowed.

### Answer:

### Requirements:
- No locks
- Must merge changes
- Offline-first

### Strategy: **CRDT / OT**

1. **Use CRDTs:**
   - Each edit is an operation with:
     - `op_id`, `actor_id`, `logical_ts`.
   - Merging operations:
     - Associative, commutative, idempotent.

2. **Client Behavior:**
   - While offline, client appends operations to local log.
   - On reconnection:
     - Syncs local log with server.
     - Server merges ops into doc CRDT.

3. **Conflict Resolution:**
   - Position changes (e.g., text):
     - CRDT sequence algorithms (e.g., RGA, LSEQ).
   - Last-writer-wins for attributes (colors, formatting).

4. **Eventual Consistency:**
   - All replicas see same set of operations.
   - CRDT guarantees they converge.

---

## Q7: Design a Feature Flag System — THEN interviewer says:
> You must roll out features based on **real-time error rate**, per flag. Turn off if too many errors.

### Answer:

### Requirements:
- Feature flags + dynamic kill-switch
- Feedback loop based on metrics

### Strategy:

1. **Metrics Integration:**
   - For each feature flag:
     - Track:
       - Error rate
       - Latency
       - Traffic volume
   - Tag metrics with `flag_name` & `variant`.

2. **Policy Engine:**
   - Rules like:
     - `if error_rate > 5% AND traffic > 1000 rpm FOR 3 minutes → disable flag`.
   - Runs periodically (e.g., every 30s).

3. **Control Plane Actions:**
   - On rule trigger:
     - Flip feature off.
     - Record event in audit log.
     - Notify on-call/Slack.

4. **Client Behavior:**
   - SDK polls or streams config updates.
   - Fast propagation (<10s).

---

## Q8: Design a Log Ingestion System — THEN interviewer says:
> Some services generate **10x more logs** than expected due to bugs. You must protect the system.

### Answer:

### Problem:
- Noisy neighbor — one service can drown infra.

### Strategy:

1. **Backpressure & Throttling:**
   - Per-tenant / per-service quotas:
     - e.g., max X MB/min.
   - When exceeded:
     - Drop extra logs **or**
     - Down-sample (keep 1 of N).

2. **Priority Levels:**
   - Critical logs (errors, security) vs debug.
   - Apply throttling to low-priority first.

3. **Local Buffering:**
   - Agents buffer on disk with size cap.
   - Oldest logs evicted when full.

4. **Tenant Visibility:**
   - Expose:
     - “You exceeded log quota; some logs may be dropped.”
   - Let them tune verbosity.

---

## Q9: Design a Search System — THEN interviewer says:
> Index updates must be visible within **5 seconds**, even during rolling re-index.

### Answer:

### Requirements:
- Near real-time indexing
- Also support large re-index jobs

### Strategy:

1. **Dual-Index Strategy:**
   - Active index (serving).
   - Shadow index (rebuild).

2. **Write Path:**
   - New documents:
     - Write to **both**:
       - Active index (fast path)
       - Shadow index (batch/async).
   - Keep a **change log** for real-time updates.

3. **Cutover:**
   - Once shadow index catches up:
     - Swap alias from active → shadow.
   - Fully atomic switch at alias level.

4. **5-Second SLA:**
   - Use:
     - Near real-time segments.
     - Commit small segments frequently (every few seconds).

---

## Q10: Design a Leader Election System — THEN interviewer says:
> Clocks can’t be trusted, and **split-brain is catastrophic**. How do you guarantee a single leader?

### Answer:

### Requirements:
- Single leader
- No split-brain
- Unreliable clocks

### Strategy:

1. **Consensus-Based Leader Election:**
   - Use Raft / Paxos:
     - Leader elected via majority vote.
     - Term numbers, not clocks.

2. **Leases with Quorum:**
   - Leader holds lease only if:
     - Heartbeats with majority succeed.
   - If leader loses contact with majority:
     - Steps down voluntarily.

3. **No Time Assumptions:**
   - Don’t rely on wall-clock time to decide leadership.
   - Only rely on:
     - Message round-trips.
     - Term progression.

4. **Client Behavior:**
   - Writes only accepted by current leader.
   - If leader suspected down:
     - Retry logic with backoff.
     - Detect new leader via cluster metadata.

---

# How to Handle “AND THEN X BREAKS” Changes in System Design

When interviewer adds new constraints:

1. State current invariants (what must remain true).
2. Explicitly name the trade-off you’re now making (availability vs consistency vs latency).
3. Introduce:
   - idempotency
   - leases/TTL
   - async replication
   - CRDTs / OT
   - backpressure & throttling
   - dual-write / dual-index patterns
4. Add **observability**:
   - metrics
   - audits
   - alerts
5. End with a **degradation story**:
   - what still works
   - what becomes best-effort
   - how you recover.
