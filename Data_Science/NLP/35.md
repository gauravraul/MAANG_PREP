# Set 24 – Advanced System Design for Speech + Multimodal AI Pipelines

---

## Category 1: Speech System Design Basics

**Q1. How would you design a large-scale ASR system for millions of users simultaneously?**  
**A1.**  
- Use **streaming ASR models** (RNN-T, Conformer) with low-latency decoding.  
- Deploy on **GPU clusters with load balancing**.  
- Store frequent queries in a **speech cache** to reduce reprocessing.  
- Use **sharding by geography** for latency reduction.  

---

**Q2. What trade-offs exist between on-device vs. cloud speech recognition?**  
**A2.**  
- **On-device**: Low latency, privacy-preserving, limited compute.  
- **Cloud**: Powerful models, scalable, but higher latency and privacy concerns.  
- Hybrid (edge-cloud split) is often best for scaling.  

---

## Category 2: Multimodal AI Pipelines

**Q3. How would you design a pipeline for real-time video + speech captioning?**  
**A3.**  
- Extract audio → ASR for transcripts.  
- Extract video frames → vision encoder (ViT/CLIP).  
- Fuse embeddings with **cross-attention** for context-aware captions.  
- Optimize with batching, GPU sharing, and memory-efficient inference.  

---

**Q4. What are the bottlenecks in scaling multimodal search (e.g., image + voice queries)?**  
**A4.**  
- High-dimensional embeddings → costly storage/search.  
- Vector DB latency for billions of embeddings.  
- Solution: **Approximate Nearest Neighbor (ANN) search**, product quantization, and multimodal late fusion to reduce cost.  

---

## Category 3: Latency & Optimization

**Q5. How do you handle latency in real-time speech translation?**  
**A5.**  
- Use **streaming ASR + incremental MT + low-latency TTS**.  
- Apply **chunking** (predict before full input).  
- Cache frequent translation phrases.  
- Use **speculative decoding** to reduce inference time.  

---

**Q6. How can you optimize inference for speech+vision+text agents on low-resource devices?**  
**A6.**  
- Quantization (int8/4-bit) for speech + vision models.  
- Distillation into smaller student models.  
- Modular pipeline (run vision only when needed).  
- On-device caching for frequent tasks.  

---

## Category 4: Storage & Cost

**Q7. How do you store and index billions of multimodal embeddings efficiently?**  
**A7.**  
- Use **vector databases** (FAISS, Milvus, Pinecone).  
- Apply **product quantization, HNSW graphs** for compression and fast retrieval.  
- Maintain **hierarchical storage** (hot SSD, cold HDD).  

---

**Q8. How do you balance training cost vs. inference cost in speech+multimodal systems?**  
**A8.**  
- Use **self-supervised pretraining** (wav2vec, CLIP) to reduce labeled data needs.  
- Offload training to high-performance clusters, inference to lightweight edge devices.  
- Use **knowledge distillation** for deployment efficiency.  

---

## Category 5: Reliability & Fault Tolerance

**Q9. How would you design a speech agent system that must be 99.99% available?**  
**A9.**  
- Multi-region deployment with automatic failover.  
- Caching recent ASR/TTS results.  
- Graceful degradation (fallback to text-only if speech fails).  
- Monitoring pipelines with anomaly detection for audio quality.  

---

**Q10. How do you detect and mitigate failure cases in speech-driven agents?**  
**A10.**  
- Log misrecognitions and use **active learning** for retraining.  
- Apply **confidence scoring** in ASR → ask for clarification if low.  
- Use **anomaly detection** for accents/noise conditions.  
- Integrate fallback heuristics like keyword spotting.
