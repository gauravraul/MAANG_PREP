# Research-Level NLP Interview Questions – Set 6 (Evaluation & Safety in Multimodal Agentic AI)

## 1. Why is evaluation harder in multimodal systems compared to text-only LLMs?
**Answer:**  
- Multimodal systems combine vision, text, audio, etc., so correctness depends on cross-modal grounding.  
- Traditional NLP metrics (BLEU, ROUGE) don’t capture visual or action accuracy.  
- Subjectivity: “Describe the image” → multiple valid answers.  
- Need task-specific evaluation: VQA accuracy, retrieval precision, grounding overlap.  

---

## 2. How can bias propagate in multimodal agentic AI systems?
**Answer:**  
- **Text bias**: stereotypes in language corpora.  
- **Vision bias**: skewed datasets (e.g., underrepresented demographics).  
- **Cross-modal amplification**: text bias + vision bias → stronger reinforcement.  
- Example: “doctor” associated with men more in both text and images.  
- Mitigation: balanced datasets, fairness-aware loss functions, debiasing embeddings.

---

## 3. What safety risks arise when multimodal agents interact with the real world?
**Answer:**  
- **Physical risks**: robots misinterpreting instructions → accidents.  
- **Privacy risks**: misuse of facial recognition.  
- **Security risks**: prompt injection in multimodal context (e.g., adversarial images).  
- **Misinformation**: multimodal hallucinations spreading fake content.  
- Requires strict human-in-the-loop and sandboxed testing.

---

## 4. How do you test robustness of multimodal models under adversarial conditions?
**Answer:**  
- **Perturb input**: noisy text, blurry images, distorted audio.  
- **Adversarial attacks**: imperceptible pixel changes, poisoned captions.  
- **Stress tests**: OOD (out-of-distribution) datasets.  
- Evaluation metrics: accuracy drop, calibration error, failure rate under adversarial scenarios.  

---

## 5. What role does explainability play in evaluating multimodal agents?
**Answer:**  
- Helps humans **trust** system outputs.  
- Techniques:  
  - Attention heatmaps (visual
