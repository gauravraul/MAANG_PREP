# Research-Level NLP Interview Questions – Set 4 (Cutting-Edge)

## 1. What are Agentic LLMs, and how do they differ from standard LLMs?
**Answer:**  
- Standard LLMs: Passive next-token predictors with no memory or planning.  
- Agentic LLMs:  
  - **Autonomous loop**: observe → reason → act → reflect.  
  - Can call tools, retrieve knowledge, store memories, and refine outputs.  
- They differ by **persistence (long-term memory)**, **decision-making**, and **self-reflection**.

---

## 2. How do you design a memory architecture for an Agentic LLM?
**Answer:**  
- **Short-term memory**: Context window + KV cache.  
- **Episodic memory**: Stores interaction history.  
- **Semantic memory**: Vector DB of embeddings for facts.  
- **Procedural memory**: Policies, rules, workflows.  
Challenges: forgetting irrelevant info, retrieval efficiency, and avoiding memory poisoning.

---

## 3. What are the limitations of vanilla RAG when used in reasoning-heavy tasks?
**Answer:**  
- RAG retrieves documents but struggles with:  
  - **Multi-step reasoning** (chaining facts).  
  - **Query drift** due to poor embedding quality.  
  - **Hallucinations** if documents contradict.  
- Advanced approaches: **GraphRAG**, **Tree-of-Retrievals**, **Agentic RAG** with iterative query refinement.

---

## 4. How can you evaluate the reasoning capabilities of an Agentic RAG system?
**Answer:**  
- Traditional metrics (BLEU, ROUGE) are insufficient.  
- Use:  
  - **Faithfulness**: Consistency with retrieved docs.  
  - **Completeness**: Coverage of relevant facts.  
  - **Reasoning path evaluation**: Checking the correctness of intermediate steps.  
- Tools: Chain-of-thought evaluation, self-consistency checks, human-in-the-loop audits.

---

## 5. What role do external tools (APIs, databases, simulators) play in Agentic LLMs?
**Answer:**  
- Extend LLM’s capabilities beyond static text.  
- Examples:  
  - SQL API → real-time structured queries.  
  - Math API → precise computation.  
  - Simulator → test hypotheses.  
- Challenges:  
  - Secure tool use (prevent prompt injection).  
  - Orchestration latency.  
  - Tool grounding to natural language queries.

---

## 6. Explain how multi-hop retrieval improves over standard RAG.
**Answer:**  
- Standard RAG: single retrieval → answer.  
- Multi-hop retrieval: decomposes queries into sub-questions, retrieves separately, then aggregates.  
- Benefits: handles **reasoning chains** like “Which company acquired the startup founded by the brother of Elon Musk?”  
- Risks: query explosion, noisy intermediate retrievals.

---

## 7. How can you reduce hallucinations in multi-agent LLM systems?
**Answer:**  
- Use **role-based agents**: planner, retriever, verifier.  
- Introduce **fact-checking loops** with retrieval grounding.  
- Apply **majority voting** or **debate mechanisms**.  
- Implement **confidence calibration** to discard low-certainty outputs.

---

## 8. What are Retrieval Compression Techniques in RAG pipelines?
**Answer:**  
- **Problem**: Too many retrieved docs → bloated context.  
- Techniques:  
  - **Summarization-based compression** (extractive/abstractive).  
  - **Dense re-ranking** with cross-encoders.  
  - **Clustering and centroid selection**.  
  - **Saliency-based token pruning**.  
- Trade-off: compress too much → lose signal; too little → increase latency/cost.

---

## 9. How can graph-based methods enhance RAG?
**Answer:**  
- Documents → entities + relations → Knowledge Graph.  
- Retrieval works on **semantic paths** instead of flat embeddings.  
- Advantages:  
  - Better reasoning (multi-hop over graph).  
  - Explicit relation grounding.  
- Example: GraphRAG in biomedical domain (genes–diseases–drugs).

---

## 10. What are the scaling challenges in deploying Agentic LLMs in production?
**Answer:**  
- **Cost explosion**: multiple agents + tool calls.  
- **Latency**: long reasoning chains.  
- **Memory management**: storing/retrieving past interactions.  
- **Monitoring**: ensuring correctness, safety, and avoiding malicious actions.  
- Mitigation: caching, dynamic agent orchestration, adaptive retrieval, continuous evaluation pipelines.
