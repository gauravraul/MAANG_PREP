# Research-Level NLP Q&A – Set 31

## 1. What is “Representation Collapse” in self-supervised or multimodal learning?
**Answer:**  
Representation collapse occurs when model embeddings for diverse inputs converge to a narrow cluster or a constant vector, losing discriminative power. It often happens in contrastive or masked prediction setups without sufficient negative samples or normalization. Techniques like **variance-invariance-covariance regularization (VICReg)**, **stop-gradient tricks (BYOL)**, or **decorrelation losses** help prevent collapse.

---

## 2. How do scaling laws differ across modalities (text, vision, speech)?
**Answer:**  
Scaling exponents vary by modality due to data redundancy and complexity:
- **Text:** exhibits smoother power-law scaling due to discrete, compositional structure.
- **Vision:** saturates earlier; more data is needed for similar performance gains.
- **Speech:** lies between text and vision — redundancy from acoustics but contextual richness.  
Understanding cross-modal scaling informs multimodal fusion strategies.

---

## 3. What is the “Lottery Ticket Hypothesis” and its implications for NLP?
**Answer:**  
The hypothesis posits that within large networks exist smaller subnetworks (“winning tickets”) that, when trained alone, reach comparable performance. For NLP, this means:
- Pre-trained models may contain **sparse interpretable subnetworks**.
- Fine-tuning efficiency can improve via **structured pruning**.
- It supports model compression without heavy loss in quality.

---

## 4. How do “Adapter Layers” enable efficient fine-tuning in LLMs?
**Answer:**  
Adapters are lightweight modules inserted between Transformer layers. During fine-tuning, only these adapters are trained while the base model is frozen. This allows:
- **Parameter-efficient tuning** (~1–3% of full parameters).
- **Task modularity** (one base model, many adapters).  
Extensions include **Prefix-tuning**, **LoRA (Low-Rank Adaptation)**, and **Compacter**.

---

## 5. What is “Concept Erasure” in interpretability and alignment research?
**Answer:**  
Concept erasure involves removing the influence of specific latent concepts (e.g., gender, sentiment) from a model’s representation while maintaining task performance. Methods include:
- **Orthogonal projection** in embedding space.
- **Adversarial representation learning**.  
It’s critical for **fairness**, **bias mitigation**, and **controlled generation** in LLMs.

---

## 6. Explain the trade-off between alignment and creativity in LLMs.
**Answer:**  
Strong alignment (via RLHF or filtering) constrains harmful outputs but may suppress novel or divergent responses — reducing creativity. This trade-off arises because alignment narrows the response distribution. Emerging solutions:
- **Conditional alignment** (adaptive safety per context).
- **Mixture-of-decoders** allowing balanced generation diversity.

---

## 7. What are “Sparse Autoencoders” and their role in interpretability?
**Answer:**  
Sparse autoencoders constrain hidden activations, promoting disentangled feature discovery. Applied to LLMs, they reveal **human-interpretable latent dimensions** corresponding to syntactic or semantic attributes.  
Recent research (e.g., Anthropic’s work) uses sparse autoencoders to **map neurons to conceptual features**, advancing mechanistic transparency.

---

## 8. What is the role of “Temperature” in model scaling and decoding?
**Answer:**  
Temperature controls randomness in sampling from model logits:
\[
P_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]
- **Low T (<1)** → deterministic, less creative.  
- **High T (>1)** → diverse but risk of incoherence.  
Scaling larger models typically allows **lower temperatures** since they exhibit inherent diversity even with sharp distributions.

---

## 9. Explain “Context Length Scaling” and its challenges.
**Answer:**  
Increasing context length (e.g., 128K+ tokens) challenges:
- **Quadratic attention cost** (O(n²)).
- **Positional encoding limits** (e.g., rotary embeddings saturate).  
Solutions:  
- **Linear attention** (Performer, Hyena, RWKV).  
- **Recurrent memory** and **chunked context caching**.  
Goal: achieve infinite-context transformers with stable recall and coherence.

---

## 10. What are “Latent Space Editing” techniques in language models?
**Answer:**  
These methods manipulate internal representations to steer outputs.  
Examples:
- **Activation editing:** adding/removing direction vectors (e.g., gender bias).
- **Representation steering:** projecting toward desired semantic subspaces.
- **Causal mediation analysis:** identifying causal latent variables.  
Used for **controlled text generation**, **bias correction**, and **model steering** without retraining.

---
