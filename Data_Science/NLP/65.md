# Advanced NLP Research-Level Questions — Set 28

## 1. What are the major challenges in training multilingual language models like XLM-R or mT5?
**Answer:**  
Training multilingual models involves vocabulary overlap, handling script diversity, and balancing performance across low-resource and high-resource languages. Tokenizer inefficiency and data imbalance often lead to “language drift,” where high-resource languages dominate the learned representations.

---

## 2. How do retrieval-augmented generation (RAG) systems improve factual consistency in LLMs?
**Answer:**  
RAG models augment generation with external retrieval mechanisms, fetching relevant documents during inference. This provides factual grounding and reduces hallucination by conditioning responses on retrieved knowledge rather than memorized facts.

---

## 3. What is the role of Mixture-of-Experts (MoE) in scaling NLP models efficiently?
**Answer:**  
MoE architectures activate only a subset of parameters (experts) per input token, reducing compute cost while maintaining model capacity. It allows scaling to trillions of parameters without linear increases in computational requirements.

---

## 4. How can we evaluate interpretability in large language models?
**Answer:**  
Interpretability can be assessed via feature attribution methods (like Integrated Gradients), probing classifiers to test what linguistic properties are encoded, and attention visualization. However, interpretability remains subjective and lacks standard benchmarks.

---

## 5. What are “instruction-tuned” models and how do they differ from fine-tuned models?
**Answer:**  
Instruction-tuned models are trained on datasets that pair natural language instructions with desired outputs (e.g., FLAN-T5). Unlike task-specific fine-tuning, instruction tuning generalizes across unseen tasks by learning how to follow instructions.

---

## 6. Explain the concept of “chain-of-thought” prompting and its impact on reasoning.
**Answer:**  
Chain-of-thought prompting encourages models to produce intermediate reasoning steps before the final answer. It improves multi-step reasoning and interpretability, though it increases latency and can expose biases in model reasoning.

---

## 7. What are the potential risks of model alignment using reinforcement learning from human feedback (RLHF)?
**Answer:**  
RLHF may introduce human biases, over-optimize for superficial preferences, or reduce creativity. It also makes models more sensitive to reward mis-specification and can degrade factual accuracy in favor of “polite” responses.

---

## 8. How do scaling laws help in predicting model performance?
**Answer:**  
Scaling laws describe predictable improvements in loss or accuracy as a function of model size, data, and compute. They guide resource allocation and enable extrapolation of performance without full-scale training runs.

---
