# Research-Level NLP — Set 13: Interpretability and Explainability in LLMs

---

### **Q1.** What is the difference between *mechanistic interpretability* and *functional interpretability*?

**A1.**  
- **Mechanistic interpretability** seeks to understand how model internals (neurons, attention heads, circuits) compute specific behaviors. Example: tracing a neuron responsible for negation.  
- **Functional interpretability** focuses on input-output behavior and how model responses change under different contexts. Example: probing attention maps or activation vectors.  
Mechanistic gives *why* at the neuron level, while functional gives *what* and *how* at the behavioral level.

---

### **Q2.** How can *probing classifiers* help in interpreting representations in NLP models?

**A2.**  
Probing classifiers are small supervised models trained to predict linguistic properties (like POS tags, tense, or syntax) from hidden states of LLMs.  
If a probe performs well, it suggests that the representation encodes that linguistic information — helping quantify what the model “knows.”

---

### **Q3.** What are *attribution methods* and how are they used in LLM interpretability?

**A3.**  
Attribution methods (e.g., Integrated Gradients, SHAP, LIME) trace which input tokens contribute most to an output prediction.  
They visualize how changes in token embeddings affect logits — making models more transparent for debugging or bias detection.

---

### **Q4.** Explain *attention-based interpretability* and its limitations.

**A4.**  
Attention maps are often visualized to show which tokens a model “focuses” on.  
However, attention does not always correlate with actual causal importance — models can rely on features not reflected in attention weights. Hence, attention is *indicative*, not *explanatory*.

---

### **Q5.** How can *representation similarity analysis (RSA)* be used to compare model layers?

**A5.**  
RSA computes pairwise similarity (e.g., cosine similarity or CKA) between layer representations to analyze hierarchical structure.  
Example: Early layers capture local syntax, mid layers capture semantics, later layers encode task-specific abstractions.  
It helps in diagnosing overfitting or representational collapse.

---

### **Q6.** What is *concept activation vector (CAV)* and how does it help interpret models?

**A6.**  
A **CAV** represents the direction in the embedding space that corresponds to a human-understandable concept (e.g., “gender bias”, “toxicity”).  
By measuring the model’s sensitivity to movement along that vector, researchers can quantify and control concept influence in predictions.

---

### **Q7.** What are *activation patching* and *circuit discovery* in interpretability?

**A7.**  
- **Activation patching:** Replaces activations in a target model layer with activations from another input to test causal roles of components.  
- **Circuit discovery:** Identifies neuron-level pathways responsible for specific computations (e.g., token dependency tracking).  
These methods are part of mechanistic interpretability pipelines.

---

### **Q8.** What is *feature superposition* and why is it a problem?

**A8.**  
Feature superposition occurs when multiple abstract features share the same neurons or directions in activation space due to limited model capacity.  
This makes interpretability harder, as neurons no longer correspond to distinct, human-understandable features.

---

### **Q9.** How can *linear probes* be used for bias detection in LLMs?

**A9.**  
Linear probes analyze hidden representations for correlations with sensitive attributes (like gender or race).  
High separability indicates encoded bias.  
Researchers can then use counterfactual data augmentation or orthogonal projection to mitigate this.

---

### **Q10.** Describe a realistic challenge in scaling interpretability research for trillion-parameter models.

**A10.**  
Challenges include:
- **Computational limits:** Full activation tracing is infeasible.  
- **Dimensional complexity:** Interpretations become combinatorially large.  
- **Dynamic context effects:** Representations shift based on prior dialogue turns.  
Thus, scalable interpretability may require automated symbolic abstraction or meta-learning
