# Hard Level NLP Questions and Answers (Set 14)

### 1. What is the difference between encoder-only, decoder-only, and encoder-decoder Transformer architectures?
**Answer:**  
- **Encoder-only (BERT):** Best for understanding tasks (classification, NER).  
- **Decoder-only (GPT):** Best for generative tasks (text completion, dialogue).  
- **Encoder-decoder (T5, BART):** Best for sequence-to-sequence tasks (translation, summarization).  

---

### 2. How does masked language modeling differ from causal language modeling?
**Answer:**  
- **Masked LM (BERT):** Learns bidirectional context by predicting masked tokens.  
- **Causal LM (GPT):** Learns autoregressively, predicting the next token given previous ones.  

---

### 3. What is catastrophic forgetting in NLP fine-tuning, and how can it be mitigated?
**Answer:**  
Catastrophic forgetting occurs when fine-tuning on new tasks erases knowledge of previously learned tasks. Mitigation methods include multi-task learning, replay methods, adapters, and regularization (e.g., EWC).  

---

### 4. How does the attention mechanism handle long sequences, and what are its limitations?
**Answer:**  
Attention compares every token with every other token (O(nÂ²) complexity). It captures global dependencies but struggles with very long sequences. Variants like Longformer, Performer, and BigBird reduce complexity.  

---

### 5. Explain contrastive learning in NLP with an example.
**Answer:**  
Contrastive learning trains models to bring similar text pairs closer in embedding space while pushing dissimilar pairs apart. Example: SimCSE uses dropout-based augmentation to generate positive pairs.  

---

### 6. What is retrieval-augmented generation (RAG)?
**Answer:**  
RAG combines retrieval (fetching documents from a knowledge base) with generation (LLMs). It improves factual correctness by grounding answers in external knowledge.  

---

### 7. Explain the role of positional encodings in Transformers.
**Answer:**  
Transformers lack recurrence/convolution, so positional encodings inject sequence order information. They can be sinusoidal (original Transformer) or learned embeddings (BERT).  

---

### 8. What are prompt-based learning and prefix-tuning?
**Answer:**  
- **Prompt-based learning:** Reformulates tasks into masked LM or generative prompts.  
- **Prefix-tuning:** Learns task-specific continuous vectors prepended to inputs, freezing the main model.  

---

### 9. How does reinforcement learning from human feedback (RLHF) improve language models?
**Answer:**  
RLHF aligns models with human preferences by:  
1. Pretraining on large text.  
2. Fine-tuning with supervised examples.  
3. Optimizing via reinforcement learning with a reward model trained on human feedback.  

---

### 10. Compare evaluation metrics BLEU, ROUGE, and METEOR.
**Answer:**  
- **BLEU:** Measures n-gram precision (used in MT).  
- **ROUGE:** Measures recall overlap of n-grams, useful in summarization.  
- **METEOR:** Considers synonyms and stemming, making it more semantically aware.
