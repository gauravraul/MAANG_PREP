# Hard Level NLP Questions and Answers (Set 18)

### 1. Why is tokenization critical in NLP, and what are its trade-offs?
**Answer:**  
Tokenization determines how text is split (words, subwords, characters).  
- **Subword (BPE, WordPiece):** Handles rare words, compact vocab.  
- **Character-level:** Robust to noise, but long sequences.  
- **Trade-off:** Balance between expressiveness and computational cost.

---

### 2. How do contextual embeddings differ from static embeddings?
**Answer:**  
- **Static (Word2Vec, GloVe):** One vector per word, no context.  
- **Contextual (ELMo, BERT):** Word representation changes with sentence context.  
- This resolves polysemy (e.g., "bank" in *river bank* vs *finance bank*).  

---

### 3. Explain vanishing gradient issues in RNNs and how LSTMs/GRUs solve them.
**Answer:**  
RNN gradients shrink over long sequences, preventing learning dependencies.  
- **LSTMs/GRUs** introduce gating mechanisms to preserve memory and regulate updates.  
- This allows models to capture **long-term dependencies** effectively.  

---

### 4. What is negative sampling in Word2Vec, and why is it efficient?
**Answer:**  
Instead of softmax over large vocabularies, **negative sampling** updates only a subset of word embeddings.  
This reduces computational complexity and improves scalability for training large corpora.  

---

### 5. How do positional encodings enable Transformers to handle sequential data?
**Answer:**  
Transformers lack recurrence, so **sinusoidal encodings** inject positional information.  
Alternative methods: **learned embeddings, rotary encodings (RoPE), ALiBi biases** improve long-sequence generalization.  

---

### 6. Why is exposure bias a challenge in sequence generation?
**Answer:**  
Models are trained with ground-truth prefixes (teacher forcing) but tested on self-generated sequences.  
This mismatch accumulates errors.  
Mitigation: **scheduled sampling, reinforcement learning (e.g., policy gradient)**.  

---

### 7. How do dependency parsers differ from constituency parsers in NLP?
**Answer:**  
- **Dependency parsing:** Focuses on head-modifier relationships.  
- **Constituency parsing:** Breaks text into nested sub-phrases (NP, VP).  
Dependency parsing is often preferred in relation extraction and information retrieval.  

---

### 8. What is the role of transfer learning in NLP?
**Answer:**  
Pretrained models (BERT, GPT) on massive corpora transfer general language knowledge.  
Fine-tuning adapts them to domain-specific tasks (e.g., legal, medical NLP).  
This reduces data requirements and improves generalization.  

---

### 9. How does label smoothing improve NLP model generalization?
**Answer:**  
Instead of hard 0/1 targets, **soft labels** (e.g., 0.9/0.1) prevent overconfidence, regularize the model, and improve calibration. Widely used in sequence-to-sequence and classification tasks.  

---

### 10. Why is zero-shot cross-lingual transfer possible in multilingual models?
**Answer:**  
Shared embeddings align semantically similar words across languages.  
Training on multiple languages encourages transfer.  
Example: XLM-R trained on English can answer questions in Hindi without explicit supervision.
