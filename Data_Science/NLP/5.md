
# NLP Interview Questions (Advanced Level)

## 1. What is the Attention Mechanism in NLP?
**Answer:**  
The attention mechanism allows models to focus on specific parts of the input sequence when generating an output.  
Instead of processing all words equally, attention assigns higher weights to more relevant words.  
Example: In machine translation, while translating "I love Paris", the word "Paris" gets higher weight when generating "Paris" in French.

---

## 2. Explain the Transformer architecture.
**Answer:**  
Transformers rely on **self-attention** and feed-forward layers, removing recurrence (RNNs) and convolution (CNNs).  
Main components:  
- Encoder: Processes input sequence into hidden representations.  
- Decoder: Generates output sequence using encoder output + attention.  
Transformers enable parallelization and handle long dependencies better.

---

## 3. What is BERT and why is it important?
**Answer:**  
BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained NLP model that learns deep bidirectional context by training on masked language modeling and next sentence prediction.  
It improved benchmarks on many NLP tasks like **question answering, sentiment analysis, NER**.

---

## 4. What is GPT, and how is it different from BERT?
**Answer:**  
- **BERT**: Encoder-based, bidirectional, trained with Masked Language Modeling (MLM).  
- **GPT**: Decoder-only transformer, unidirectional (left-to-right), trained with causal language modeling.  
- **Key Difference**: BERT is better for understanding tasks, GPT is better for generative tasks.

---

## 5. What is the role of Positional Encoding in Transformers?
**Answer:**  
Since transformers don’t use recurrence, positional encoding injects information about word order into embeddings.  
It uses sinusoidal functions to represent position so that the model knows the relative order of words.

---

## 6. Explain the concept of Transfer Learning in NLP.
**Answer:**  
Transfer learning in NLP involves pre-training models (like BERT, GPT) on large corpora and fine-tuning them on specific downstream tasks (sentiment analysis, QA, summarization).  
This drastically reduces data and training requirements for specialized tasks.

---

## 7. What is Seq2Seq with Attention, and where is it used?
**Answer:**  
Seq2Seq (sequence-to-sequence) models encode an input sequence and decode it into an output sequence.  
Attention allows the decoder to selectively focus on relevant encoder outputs during decoding.  
Applications: Machine Translation, Summarization, Chatbots.

---

## 8. What is the difference between Extractive and Abstractive Summarization?
**Answer:**  
- **Extractive**: Selects important sentences/phrases directly from text.  
- **Abstractive**: Generates new sentences capturing the essence of the original text (like a human-written summary).  
Abstractive is harder but more natural.

---

## 9. What is Beam Search in NLP decoding?
**Answer:**  
Beam Search is a heuristic search algorithm used in text generation (translation, summarization).  
Instead of choosing the most likely word at each step (greedy search), it keeps track of the top `k` sequences (beam width) and expands them.  
This improves fluency and reduces errors.

---

## 10. Explain Fine-tuning vs. Prompt-tuning in LLMs.
**Answer:**  
- **Fine-tuning**: Training a pre-trained model further on task-specific data by updating model weights.  
- **Prompt-tuning**: Keeping model weights frozen and crafting inputs/prompts to steer the model’s responses (sometimes with small trainable parameters like prefix-tuning).  
Prompt-tuning is cheaper and faster than full fine-tuning.
