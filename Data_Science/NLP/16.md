## Q41: What is the role of syntactic parsing in NLP, and how does it differ from shallow parsing (chunking)?

**Answer:**
- **Syntactic Parsing (Full Parsing):**
  - Produces a complete parse tree that represents the full syntactic structure of a sentence according to a formal grammar (e.g., CFG).
  - Example: For the sentence *"The cat sat on the mat"*, it identifies noun phrases (NP), verb phrases (VP), prepositions (PP), etc., and builds a hierarchical tree.

- **Shallow Parsing (Chunking):**
  - Focuses on identifying only the top-level phrases like noun phrases and verb phrases without building a full parse tree.
  - Faster and less computationally expensive.
  - Example: [NP The cat] [VP sat] [PP on [NP the mat]].

- **Difference:**
  - Full parsing → complete hierarchical structure.
  - Shallow parsing → flat, partial structures.

---

## Q42: Explain the concept of Embedding Alignment in multilingual NLP.

**Answer:**
- **Definition:** Embedding alignment is the process of mapping word embeddings from different languages into a shared vector space such that semantically similar words align closely regardless of language.
- **Techniques:**
  1. **Supervised Alignment:** Using bilingual dictionaries to learn a transformation matrix.
  2. **Unsupervised Alignment:** Using adversarial training or distributional similarity to align embeddings without parallel data.
  3. **Multilingual Pretrained Models:** Models like mBERT and XLM-R implicitly align embeddings across languages.
- **Applications:**
  - Cross-lingual information retrieval.
  - Multilingual machine translation.
  - Zero-shot transfer learning.

---

## Q43: What is a Pointer-Generator Network in NLP, and why is it used?

**Answer:**
- **Problem:** Traditional seq2seq models often struggle with:
  1. Copying rare or OOV (out-of-vocabulary) words from the source.
  2. Balancing between generating new words and reusing words from input.

- **Pointer-Generator Network:**
  - Hybrid model combining **seq2seq generation** with a **pointer mechanism**.
  - At each decoding step, the model decides:
    - Generate from the vocabulary.
    - Or copy a word directly from the source text.
  - Uses a probability distribution `p_gen` to mix both.

- **Applications:**
  - Text summarization (especially abstractive + extractive mix).
  - Dialogue systems where copying names or entities is essential.

---

## Q44: What is the difference between contextualized embeddings (e.g., BERT) and static embeddings (e.g., Word2Vec)?

**Answer:**
- **Static Embeddings (Word2Vec, GloVe, FastText):**
  - Each word has a single vector, regardless of context.
  - Example: "bank" has one vector whether referring to *river bank* or *financial bank*.
  - Limitation: Cannot handle polysemy well.

- **Contextualized Embeddings (BERT, ELMo, GPT):**
  - Word representations depend on surrounding context.
  - Example: "bank" will have different vectors in *"river bank"* vs *"financial bank"*.
  - Achieved using deep language models with attention mechanisms.
  
- **Summary:**
  - Static = one vector per word.
  - Contextualized = dynamic vectors per context.

---

## Q45: What are Contrastive Learning techniques in NLP, and how are they applied?

**Answer:**
- **Definition:** Contrastive learning aims to bring semantically similar representations closer and push dissimilar ones apart in embedding space.
- **Core Idea:**
  - Positive pairs: Two semantically similar texts (e.g., sentence and its paraphrase).
  - Negative pairs: Two unrelated texts.
- **Applications:**
  1. **Sentence Embeddings:** Models like SimCSE use contrastive learning to produce high-quality embeddings.
  2. **Cross-lingual Models:** Aligning representations of parallel sentences across languages.
  3. **Information Retrieval:** Improving ranking quality by distinguishing relevant from irrelevant documents.
- **Loss Function:** InfoNCE loss, triplet loss, or margin ranking loss.

---
