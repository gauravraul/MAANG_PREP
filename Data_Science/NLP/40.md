# Research-Level NLP / Multimodal AI / Speech Questions (Set 28)

## Q1. What are the unique challenges in real-time video-language understanding compared to static image-text tasks?  
**A1.**  
- Temporal alignment: models must track objects/events across frames.  
- Higher compute cost due to long video sequences.  
- Ambiguity in scene transitions (e.g., scene cuts).  
- Need for memory-efficient architectures to handle streaming input.  

---

## Q2. How do multimodal LLMs handle grounding failures in dynamic environments?  
**A2.**  
- Failures occur when the model generates text not consistent with evolving visual/audio inputs.  
- Example: describing “a man playing piano” after the scene changes to “a woman speaking.”  
- Solutions: temporal grounding modules, recurrent attention, event segmentation.  

---

## Q3. Why is evaluation of video reasoning especially difficult in AI research?  
**A3.**  
- Multiple valid answers: “person enters room” vs. “man walks into office.”  
- Long temporal dependencies make unit-based metrics (BLEU, ROUGE) insufficient.  
- Requires **semantic event understanding** rather than surface token overlap.  

---

## Q4. What are the bottlenecks in 3D multimodal embeddings (vision + text + spatial data)?  
**A4.**  
- High dimensionality of 3D point clouds/meshes.  
- Difficulty aligning symbolic text tokens with continuous 3D structures.  
- Lack of large-scale labeled 3D-text datasets.  
- Emerging solutions: voxelization, implicit neural representations, cross-modal contrastive learning.  

---

## Q5. How does speech-to-speech translation with large models differ from text-to-text translation?  
**A5.**  
- Direct speech-to-speech requires preserving prosody, tone, and speaker identity.  
- Cascaded approaches (speech → text → translation → speech) lose nuances.  
- Current challenge: end-to-end models require massive multilingual datasets.  

---

## Q6. What trade-offs exist between streaming vs. batch inference in multimodal AI systems?  
**A6.**  
- **Streaming**: low latency, supports real-time applications, but limited context window.  
- **Batch**: richer context, better accuracy, but higher latency.  
- Trade-off depends on use-case (e.g., live transcription vs. offline video summarization).  

---

## Q7. Why do multimodal models often fail in “zero-shot compositional generalization”?  
**A7.**  
- Models trained on co-occurrence (e.g., “red ball” + “blue cube”) may fail on unseen combos (“blue ball on red cube”).  
- Lack of explicit compositional representations.  
- Research direction: disentangled embeddings, compositional pretraining objectives.  

---

## Q8. How can memory-augmented neural networks help multimodal reasoning?  
**A8.**  
- Provide explicit storage and retrieval of past events in video/audio streams.  
- Useful for **long-context reasoning**, like summarizing a 2-hour meeting video.  
- Helps avoid catastrophic forgetting across modalities.  

---

## Q9. Why is adversarial robustness harder in multimodal AI than in unimodal systems?  
**A9.**  
- Attacks can exploit multiple channels (e.g., imperceptible noise in audio + misleading text prompt).  
- Cross-modal dependencies make detection harder.  
- Defenses must operate across modalities simultaneously, not independently.  

---

## Q10. What role will embodied AI (robots with multimodal models) play in future AI alignment research?  
**A10.**  
- Embodied AI forces models to ground predictions in **real-world actions**.  
- Example: robot must actually “pick up red cup” instead of just describing it.  
- Makes alignment more measurable → success/failure tied to physical outcomes.  
- Key challenge: safe exploration in real-world environments.
