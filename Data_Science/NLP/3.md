# NLP Interview Questions (Set 3 - Intermediate/Advanced)

---

### 1. What is the difference between stemming and lemmatization?  
**Answer:**  
- **Stemming**: Rule-based process that chops off word endings to get the root form (e.g., "running" → "run", "flies" → "fli"). It may produce non-dictionary words.  
- **Lemmatization**: Uses vocabulary and morphological analysis to return the base form of a word (lemma). Always results in valid dictionary words (e.g., "running" → "run", "better" → "good").  
- Lemmatization is more accurate but computationally more expensive.

---

### 2. What is the difference between Bag-of-Words (BoW) and TF-IDF?  
**Answer:**  
- **BoW**: Represents text as a vector of word counts. Ignores order and context.  
- **TF-IDF**: Adjusts BoW by reducing the weight of common words and increasing rare but important words.  
  Formula:  
  \[
  \text{TF-IDF}(t,d) = \text{TF}(t,d) \times \log \frac{N}{DF(t)}
  \]  
  where TF = Term Frequency, DF = Document Frequency, N = total documents.  

---

### 3. What are word embeddings, and why are they important?  
**Answer:**  
- Word embeddings (e.g., Word2Vec, GloVe, FastText) map words into dense vector space where similar words are closer.  
- They capture semantic meaning (e.g., vector("king") - vector("man") + vector("woman") ≈ vector("queen")).  
- Better than one-hot encoding because they reduce sparsity and capture relationships.  

---

### 4. What is the role of Attention in NLP?  
**Answer:**  
- Attention helps models focus on relevant parts of the input when generating output.  
- Example: In machine translation, while generating a word, the model attends more to the corresponding words in the source sentence.  
- This solves the limitation of fixed-size context vectors in seq2seq models.  

---

### 5. What is BERT and why is it powerful?  
**Answer:**  
- **BERT (Bidirectional Encoder Representations from Transformers)** is a transformer-based model trained on masked language modeling and next sentence prediction.  
- Unlike traditional LSTMs or unidirectional models, BERT considers context from both left and right simultaneously.  
- Pretrained on large corpora and fine-tuned for specific tasks (e.g., sentiment analysis, QA).  

---

### 6. What are the limitations of RNNs for NLP tasks?  
**Answer:**  
- Struggle with long-range dependencies due to vanishing/exploding gradients.  
- Sequential processing → slow training.  
- Poor parallelization.  
- LSTMs/GRUs mitigate some issues but Transformers have largely replaced RNNs.  

---

### 7. What is the difference between seq2seq with attention and transformers?  
**Answer:**  
- **Seq2seq with Attention**: Uses RNN/LSTM encoder-decoder with an additional attention mechanism.  
- **Transformers**: Entirely rely on self-attention (no recurrence), allowing parallelization and better handling of long dependencies.  
- Transformers scale better and form the foundation for BERT, GPT, etc.  

---

### 8. Explain the concept of Transfer Learning in NLP.  
**Answer:**  
- Pretrain a large model on massive text corpus (unsupervised tasks like masked language modeling).  
- Fine-tune on specific downstream tasks (e.g., sentiment classification, named entity recognition).  
- Benefits: Saves computation, improves performance with limited labeled data.  
- Examples: BERT, GPT, RoBERTa.  

---

### 9. What is the difference between extractive and abstractive text summarization?  
**Answer:**  
- **Extractive**: Selects important sentences/phrases directly from the text.  
- **Abstractive**: Generates new sentences capturing the meaning (like how humans summarize).  
- Extractive is simpler but less human-like, abstractive requires seq2seq or transformer models.  

---

### 10. How do you evaluate NLP models?  
**Answer:**  
- **Classification tasks**: Accuracy, Precision, Recall, F1-score.  
- **Language models**: Perplexity (lower is better).  
- **Machine translation**: BLEU, ROUGE scores.  
- **Summarization**: ROUGE, METEOR.  
- **Embedding similarity**: Cosine similarity.  

---
