# NLP Interview Questions (Intermediate Level)

## 1. What is the difference between stemming and lemmatization?
**Answer:**  
- **Stemming**: Cuts words to their root form by rule-based methods, may not always be valid words.  
  Example: "studies" → "studi".  
- **Lemmatization**: Uses vocabulary and morphological analysis to return a valid word.  
  Example: "studies" → "study".  

---

## 2. What is TF-IDF, and why is it useful?
**Answer:**  
TF-IDF (Term Frequency-Inverse Document Frequency) evaluates how important a word is to a document in a collection.  
- **TF**: How often a word appears in a document.  
- **IDF**: How rare a word is across all documents.  
It reduces the weight of common words and highlights unique terms.

---

## 3. What are Word Embeddings?
**Answer:**  
Word embeddings are vector representations of words in continuous space where semantic similarity is preserved.  
Examples: Word2Vec, GloVe, FastText.  
They allow algorithms to understand relationships like:  
**vector("king") - vector("man") + vector("woman") ≈ vector("queen")**.

---

## 4. What are Stop Words and why are they removed?
**Answer:**  
Stop words are common words like "the," "is," "in," that usually don’t add much meaning in text analysis.  
They are removed to reduce noise and dimensionality. However, in some tasks (e.g., sentiment analysis), they may still be useful.

---

## 5. Explain Bag-of-Words (BoW) model.
**Answer:**  
BoW represents text as a set of word frequencies, ignoring order and context.  
Example:  
- Document 1: "I love NLP" → {I:1, love:1, NLP:1}  
- Document 2: "NLP is fun" → {NLP:1, is:1, fun:1}  

---

## 6. What are n-grams?
**Answer:**  
An n-gram is a sequence of `n` words used together in text.  
- Unigram: "NLP"  
- Bigram: "Natural Language"  
- Trigram: "I love NLP"  
They capture context better than single words.

---

## 7. What is Named Entity Recognition (NER)?
**Answer:**  
NER identifies and classifies entities like **names, organizations, locations, dates** in text.  
Example: "Apple Inc. is based in California" → {Apple Inc.: Organization, California: Location}.

---

## 8. Explain Cosine Similarity in NLP.
**Answer:**  
Cosine similarity measures the cosine of the angle between two vectors (documents).  
It is used in information retrieval, recommendation systems, and semantic similarity.  
Formula:  
cos(θ) = (A · B) / (||A|| * ||B||).

---

## 9. What is Word Sense Disambiguation (WSD)?
**Answer:**  
WSD determines the correct meaning of a word in context.  
Example: "bank" in  
- "He sat on the river bank" → river side  
- "He deposited money in the bank" → financial institution  

---

## 10. What is Latent Semantic Analysis (LSA)?
**Answer:**  
LSA uses Singular Value Decomposition (SVD) on the term-document matrix to identify hidden relationships between words and documents.  
It reduces dimensionality and captures semantic meaning beyond surface-level word frequency.
