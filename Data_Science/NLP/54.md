
# Research-Level NLP Q&A – Set 35

## 1. What is “Test-Time Training (TTT)” and how can it improve NLP model robustness?
**Answer:**  
Test-Time Training allows a model to adapt during inference by leveraging **self-supervised signals** from the test input itself.  
In NLP, TTT can correct domain shifts or noise by:
- Minimizing a **self-prediction loss** (e.g., masked language modeling) during inference.
- Dynamically updating layer norms or adapter weights.  
TTT improves robustness in tasks like **domain adaptation**, **low-resource translation**, and **robust question answering**, though it adds latency.

---

## 2. How do “Neural Scaling Laws” change under noisy or low-quality data regimes?
**Answer:**  
Under ideal scaling, loss ∝ compute⁻ᵃ.  
However, with noisy or low-quality data:
- Scaling efficiency drops (diminishing returns appear earlier).  
- The model tends to **memorize noise**, flattening the scaling curve.  
Recent works (Hoffmann et al., Chinchilla scaling) highlight that **data quality and token diversity** can be more critical than raw dataset size.

---

## 3. How can “Functional embeddings” extend the capabilities of static word embeddings?
**Answer:**  
Unlike static embeddings (e.g., Word2Vec), functional embeddings treat words as **parameterized functions** over context.  
They allow dynamic meaning shifts depending on sentence semantics.  
Applications include:
- Context-aware polysemy resolution.  
- Fine-grained sentiment detection.  
This moves NLP closer to **continuous meaning representations**, reducing the reliance on discrete tokens.

---

## 4. How do “Gradient Inversion Attacks” threaten LLM fine-tuning privacy?
**Answer:**  
Attackers can reconstruct original training samples by analyzing **shared gradients** (common in federated or distributed training).  
They exploit:
- The correlation between gradients and input features.
- The lack of differential privacy.  
Mitigation involves **gradient clipping**, **DP-SGD**, or **secure aggregation**, especially when training on private or user data.

---

## 5. What is “Contrastive Decoding” and how does it reduce hallucinations in text generation?
**Answer:**  
Contrastive decoding runs two models — a **large (accurate)** and a **small (fluent but hallucination-prone)** one.  
The decoder prefers tokens where the large model assigns higher likelihood **relative** to the small one:
\[
s(y_t) = p_{\text{large}}(y_t|x) - \alpha p_{\text{small}}(y_t|x)
\]
This method preserves fluency while improving factuality, commonly used in **factual summarization** and **knowledge-grounded chatbots**.

---

## 6. Explain the importance of “Synthetic data scaling” in LLM evolution.
**Answer:**  
As real-world data saturates, LLMs increasingly depend on **synthetically generated training data** from existing models.  
Challenges:
- **Distributional collapse** (models learning their own biases).  
- **Quality control** and **diversity decay**.  
Solutions include **data filtering with teacher-student models**, **feedback loops**, and **semantic deduplication**, allowing scaling beyond web-crawled corpora.

---

## 7. How do “Transformer circuits” contribute to mechanistic interpretability?
**Answer:**  
Transformer circuits describe structured neuron groups that perform modular reasoning tasks (e.g., subject-verb agreement, coreference resolution).  
By tracing **activation flow** through attention heads, researchers can:
- Identify causal pathways for reasoning.
- Modify or ablate components to study behavior.  
Projects like Anthropic’s *Transformer Circuits* reveal interpretable subgraphs behind model predictions.

---

## 8. What are the main limitations of “Cross-lingual transfer” in multilingual NLP models?
**Answer:**  
Cross-lingual models (e.g., mBERT, XLM-R) struggle with:
- **Script mismatches** and tokenization inefficiencies.  
- **Underrepresented languages** lacking fine-tuning data.  
- **Semantic drift** when mapping dissimilar linguistic structures.  
Recent approaches use **adapter fusion**, **language-specific subspaces**, and **contrastive alignment losses** to mitigate performance disparities.

---

## 9. How can we detect and prevent “Representation Collapse” in contrastive learning for NLP?
**Answer:**  
Representation collapse occurs when all embeddings converge to a similar point.  
Prevention techniques:
- Use **negative samples** effectively.  
- Add **temperature scaling** in similarity loss.  
- Employ **stop-gradient mechanisms** (e.g., in BYOL).  
Maintaining embedding diversity is critical for **semantic retrieval**, **sentence embeddings**, and **contrastive pretraining**.

---

## 10. What is “Compositional Generalization via Causal Abstractions” in NLP?
**Answer:**  
This approach represents linguistic tasks as **causal graphs** linking concepts.  
By modeling how compositional structures (like verbs, objects) interact causally, models can **generalize beyond training distributions**.  
This unites symbolic reasoning and neural models — key to achieving **true language understanding** rather than pattern mimicry.

---
