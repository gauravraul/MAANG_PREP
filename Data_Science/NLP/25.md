# Hard Level NLP Questions and Answers (Set 13)

### 1. What is the vanishing gradient problem in RNNs and how is it mitigated?
**Answer:**  
RNNs suffer from vanishing gradients during backpropagation through time (BPTT), making it difficult to learn long-term dependencies. LSTMs and GRUs mitigate this issue by using gating mechanisms that control the flow of gradients.

---

### 2. Explain the concept of "Attention is All You Need" from the Transformer paper.
**Answer:**  
The paper proposed that sequence transduction tasks can be solved without recurrence or convolution by relying solely on self-attention and feed-forward layers. This significantly improved parallelization and long-range dependency modeling.

---

### 3. How does Byte-Pair Encoding (BPE) help in tokenization?
**Answer:**  
BPE reduces the vocabulary size by merging the most frequent character pairs iteratively. It balances between word-level and character-level tokenization, handling rare words and subwords efficiently.

---

### 4. What is the role of the [CLS] token in BERT?
**Answer:**  
The [CLS] token is a special token prepended to every input sequence in BERT. Its final hidden state is used as an aggregate representation of the sequence for classification tasks like sentiment analysis.

---

### 5. Compare greedy decoding, beam search, and sampling in sequence generation.
**Answer:**  
- **Greedy decoding:** Picks the highest probability token at each step, fast but suboptimal.  
- **Beam search:** Explores multiple candidate sequences, balances quality and speed.  
- **Sampling:** Picks tokens probabilistically, increases diversity but can reduce coherence.

---

### 6. What is label smoothing in NLP?
**Answer:**  
Label smoothing prevents overconfidence by replacing one-hot encoded targets with smoothed probability distributions. This improves generalization and calibration of models like Transformers.

---

### 7. Explain the difference between extractive and abstractive summarization.
**Answer:**  
- **Extractive summarization:** Selects important sentences/phrases from the original text.  
- **Abstractive summarization:** Generates new sentences that may not exist in the text, requiring deeper semantic understanding.

---

### 8. What are contextual embeddings, and how do they differ from static embeddings?
**Answer:**  
- **Static embeddings (Word2Vec, GloVe):** Each word has a single vector, ignoring context.  
- **Contextual embeddings (BERT, ELMo):** Word representations vary depending on sentence context, capturing polysemy and semantics.

---

### 9. Why is fine-tuning large pre-trained models challenging in low-resource settings?
**Answer:**  
Fine-tuning requires large memory and data. In low-resource scenarios, models may overfit. Techniques like parameter-efficient fine-tuning (LoRA, adapters) or few-shot learning help address this.

---

### 10. Explain how perplexity is used as an evaluation metric in language modeling.
**Answer:**  
Perplexity measures how well a model predicts a sequence, defined as the exponential of the average negative log-likelihood. Lower perplexity indicates better predictive performance.
