## Q46: What problem do Transformer variants like Longformer, Reformer, and BigBird solve in NLP?

**Answer:**
- **Problem:** Standard Transformers have **quadratic time and memory complexity** (`O(n²)`) due to the self-attention mechanism, making them inefficient for very long sequences (e.g., long documents).
- **Solutions:**
  1. **Longformer:** Uses *local attention* with optional *global tokens*, reducing complexity to near-linear.
  2. **Reformer:** Replaces standard attention with **locality-sensitive hashing (LSH)** attention and reversible residual layers.
  3. **BigBird:** Combines **sparse attention** (local, random, and global) to approximate full attention with theoretical guarantees.
- **Use Cases:** Document classification, legal/medical text, summarization of long inputs.

---

## Q47: What is the difference between autoregressive and autoencoding Transformer models?

**Answer:**
- **Autoregressive Models (e.g., GPT):**
  - Predict the next token given previous tokens.
  - Trained using **causal (left-to-right) masking**.
  - Good for **text generation** tasks.
  
- **Autoencoding Models (e.g., BERT):**
  - Predict masked tokens within a sentence.
  - Use **bidirectional context**.
  - Good for **understanding tasks** like classification, NER, QA.

- **Hybrid Models (e.g., T5, BART):**
  - Combine both: use encoder-decoder frameworks to handle both **generation** and **understanding**.

---

## Q48: How does T5 (Text-to-Text Transfer Transformer) unify NLP tasks?

**Answer:**
- **Concept:** T5 frames *all NLP tasks* as a **text-to-text problem**.
  - Input: task description + text.
  - Output: target text.
- **Examples:**
  - Translation: `"translate English to German: How are you?" → "Wie geht es dir?"`
  - Summarization: `"summarize: The cat sat on the mat..." → "Cat on mat"`
  - Sentiment: `"classify sentiment: I loved the movie" → "positive"`
- **Advantages:**
  - One model for multiple tasks.
  - Simple and flexible formulation.
  - Transfer learning across tasks.

---

## Q49: What are evaluation challenges for large language models in NLP?

**Answer:**
- **Key Challenges:**
  1. **Hallucination:** Models may generate fluent but factually incorrect text.
  2. **Bias & Fairness:** Models may reproduce or amplify societal biases.
  3. **Evaluation Metrics:**
     - BLEU, ROUGE → measure surface similarity, not meaning.
     - BERTScore, BLEURT → measure semantic similarity.
     - Human evaluation still necessary for quality & truthfulness.
  4. **Task Generalization:** Difficult to evaluate on unseen tasks (zero-shot).
  5. **Robustness:** Performance can drop with adversarial inputs or domain shift.

---

## Q50: What is Retrieval-Augmented Generation (RAG), and why is it important?

**Answer:**
- **Problem:** Standard LLMs have limited knowledge fixed at training time and may hallucinate.
- **Solution: RAG**
  - Combines **retrieval** with **generation**:
    1. Retrieve relevant documents from an external knowledge base.
    2. Feed retrieved documents + query into the generator model.
  - Reduces hallucination and improves factual correctness.
- **Examples:**
  - Open-domain QA.
  - Chatbots with knowledge grounding.
  - Enterprise search with LLMs.
- **Frameworks:** Facebook’s RAG, LangChain, LlamaIndex.

---
