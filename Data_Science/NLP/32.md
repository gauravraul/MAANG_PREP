# Hard Level NLP Questions and Answers (Set 20)

### 1. What is catastrophic agreement in ensemble NLP models?
**Answer:**  
When multiple models agree on the **same wrong prediction** due to shared biases or training data artifacts.  
Mitigation: use diverse architectures, random seeds, or train on different subsets to encourage variety.  

---

### 2. How does curriculum learning benefit NLP training?
**Answer:**  
It trains models starting from **easy examples** and gradually moving to **harder ones**, improving convergence and generalization.  
Example: In machine translation, start with short/simple sentences before complex ones.  

---

### 3. What is polysemy handling in embeddings, and why is it important?
**Answer:**  
Polysemy = words with multiple meanings (e.g., "bat" = animal vs cricket bat).  
- **Static embeddings:** Fail to disambiguate.  
- **Contextual embeddings (BERT, GPT):** Capture meaning from surrounding context.  

---

### 4. How does the lottery ticket hypothesis apply to NLP models?
**Answer:**  
It suggests that within large NLP models, smaller subnetworks (“winning tickets”) exist that can be trained to perform nearly as well.  
This drives **pruning, compression, and efficient fine-tuning research**.  

---

### 5. Why is long-context modeling still a bottleneck in NLP?
**Answer:**  
Transformers scale **O(n²)** with sequence length.  
Challenges: memory, computation, and loss of attention focus.  
Solutions: **sparse attention (Longformer), linearized attention (Performer), memory-augmented models (Transformer-XL, RMT).**  

---

### 6. How does knowledge distillation impact NLP deployment?
**Answer:**  
Distillation compresses large models into smaller ones while retaining accuracy.  
Used in **DistilBERT, TinyBERT** for mobile/edge deployment.  
Trade-off: sometimes loses subtle reasoning abilities.  

---

### 7. What are the main challenges of evaluation in conversational AI?
**Answer:**  
- No single ground truth.  
- Automatic metrics (BLEU, METEOR) don’t capture naturalness.  
- Human evaluation is subjective.  
- Models may optimize for engagement instead of truth.  

---

### 8. Why is fairness difficult to guarantee in NLP systems?
**Answer:**  
- Training data contains **societal biases**.  
- Models amplify stereotypes (gender, race, culture).  
- Fairness requires balancing multiple criteria (accuracy vs equity).  
Solutions: **bias mitigation, counterfactual data augmentation, adversarial training.**  

---

### 9. How do retrieval-augmented models improve factual accuracy?
**Answer:**  
They fetch information from external knowledge sources at inference time.  
This reduces hallucinations, improves **QA, summarization, grounded generation**.  
Examples: **RAG, RETRO, Atlas.**  

---

### 10. Why is multi-modal NLP more complex than unimodal?
**Answer:**  
It requires aligning text with **images, audio, or video**, which may have different structures and noise.  
Key challenges: representation alignment, modality imbalance, cross-modal reasoning.  
Examples: CLIP, Flamingo, GPT-4V.
