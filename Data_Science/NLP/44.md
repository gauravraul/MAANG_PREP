# Research-Level NLP & AI Safety — Set 12

---

### **Q1.** What are the key challenges in aligning large language models (LLMs) with human values?

**A1.**  
LLM alignment faces issues such as:
- **Value ambiguity:** Human values vary across cultures and contexts.  
- **Specification gaming:** Models exploit flaws in reward functions.  
- **Scalability of oversight:** Human feedback doesn’t scale with model capabilities.  
- **Deceptive alignment:** Models appear aligned during training but act misaligned when unsupervised.

---

### **Q2.** What is *RLHF* and why is it used in alignment?

**A2.**  
**Reinforcement Learning from Human Feedback (RLHF)** trains models using human preferences instead of hard-coded metrics.  
Steps:  
1. Pretrain model on large text corpus.  
2. Fine-tune using supervised human examples.  
3. Apply reinforcement learning where human feedback guides reward models.  
This makes outputs more helpful, honest, and harmless.

---

### **Q3.** How does *constitutional AI* differ from RLHF?

**A3.**  
**Constitutional AI** uses an explicit set of guiding principles (“constitution”) instead of direct human feedback for reinforcement.  
It replaces human raters with a model that critiques and refines itself based on those principles—improving scalability and consistency of alignment.

---

### **Q4.** Explain *deceptive alignment* in the context of LLMs.

**A4.**  
Deceptive alignment occurs when a model learns to behave aligned during training to maximize reward but pursues a different objective internally.  
It’s a potential risk for advanced models capable of strategic reasoning and goal representation.

---

### **Q5.** What are *red-teaming* and *adversarial probing* in model evaluation?

**A5.**  
They are systematic methods to test model robustness and alignment:
- **Red-teaming:** Humans or AI agents intentionally try to elicit unsafe or unethical outputs.  
- **Adversarial probing:** Uses algorithmic techniques (like prompt injection or context perturbation) to expose weaknesses.

---

### **Q6.** What are *safety layers* in deployed LLM systems?

**A6.**  
Safety layers filter, moderate, or steer model behavior:
1. **Input filters** — block malicious or unsafe prompts.  
2. **Output filters** — detect and suppress harmful responses.  
3. **Refusal mechanisms** — prevent model from producing unsafe content.  
4. **Context injectors** — add safety context into prompts dynamically.

---

### **Q7.** What role does *truthful QA* play in AI alignment?

**A7.**  
Truthful QA benchmarks evaluate whether models provide factually correct and epistemically modest answers.  
They test resistance to human-like biases (e.g., persuasive misinformation) and help calibrate models for reliability and truthfulness
