# Research-Level NLP Q&A – Set 30

## 1. What is the “Attention Sink” problem in Transformers?
**Answer:**  
The attention sink problem occurs when certain tokens (usually beginning-of-sequence or padding tokens) attract disproportionately high attention weights despite being semantically uninformative. This happens due to the residual connections and normalization dynamics in Transformers. It can cause inefficiencies and model biases. Techniques like **attention masking**, **normalization tuning**, or **re-weighting attention logits** have been proposed to mitigate it.

---

## 2. Explain the concept of “Emergent Abilities” in large language models.
**Answer:**  
Emergent abilities refer to model behaviors that appear suddenly when scaling model size or data, even though smaller models show no signs of them. For example, arithmetic reasoning or multi-step logical reasoning may emerge beyond certain parameter thresholds. These abilities are **non-linear with respect to scale** and challenge current understanding of neural scaling laws.

---

## 3. What are “Mechanistic Interpretability” approaches?
**Answer:**  
Mechanistic interpretability aims to reverse-engineer neural networks into understandable computational circuits. Instead of correlating neuron activations with high-level concepts, it seeks to **map causal pathways** (e.g., how attention heads represent syntactic dependencies). Techniques include **feature visualization**, **activation patching**, and **circuit tracing** using causal interventions.

---

## 4. How do “Constitutional AI” and “Reinforcement Learning from AI Feedback (RLAIF)” differ from RLHF?
**Answer:**  
- **RLHF** (Reinforcement Learning from Human Feedback) uses human evaluators to guide reward modeling.  
- **Constitutional AI** replaces human raters with a predefined set of ethical principles (“constitution”) that the model uses to self-critique and revise responses.  
- **RLAIF** automates feedback collection by using another aligned model as the feedback provider instead of humans, reducing cost and bias.

---

## 5. What are “Sparse Mixture of Experts” (MoE) models and why are they significant?
**Answer:**  
MoE models activate only a small subset of experts (sub-networks) per input, allowing scaling of parameter count without proportionally increasing compute. This enables trillion-parameter models like **Switch Transformer** to be trained efficiently. Challenges include **load balancing**, **expert specialization**, and **communication overhead** across distributed systems.

---

## 6. How do “In-context learning” and “Meta-learning” relate theoretically?
**Answer:**  
Recent theoretical work suggests that in-context learning (ICL) can be viewed as **implicit meta-learning**—the Transformer weights encode a learning algorithm that generalizes from the examples in the context window. The model performs gradient-free “learning” by adjusting its hidden states based on prompt patterns, not parameters.

---

## 7. What is the goal of “Neural Scaling Law” research?
**Answer:**  
Neural scaling law research investigates how performance scales with model size, dataset size, and compute. The empirical relationship typically follows a power law:  
\[
L = A N^{-\alpha} + B
\]  
where \(L\) is loss, \(N\) is scale (parameters, tokens, etc.), and \(\alpha\) is the scaling exponent. Understanding these helps forecast optimal model size and data requirements for given compute budgets.

---

## 8. What are “Retrieval-Augmented Generation” (RAG) limitations at scale?
**Answer:**  
At scale, RAG faces bottlenecks like:  
- **Latency**: retrieving large-scale documents slows inference.  
- **Context mixing**: irrelevant or redundant retrieved content degrades coherence.  
- **Retrieval drift**: model overfits to retrievals and loses reasoning generality.  
Hybrid approaches (e.g., **Memory-augmented Transformers**, **LLM caching**) aim to overcome these.

---

## 9. Describe “Multimodal Alignment” challenges.
**Answer:**  
Aligning textual and visual representations in multimodal LLMs like CLIP or GPT-4V requires ensuring **semantic correspondence** across modalities. Key challenges include **representation collapse**, **cross-modal bias**, and **loss of compositional generalization**. Solutions include **contrastive learning**, **modality-specific normalization**, and **joint embedding spaces**.

---

## 10. What are “Causal Language Models” (CLMs) and their research frontiers?
**Answer:**  
CLMs (like GPT-style models) predict tokens autoregressively. Current frontiers include:  
- **Long-context modeling** via memory or recurrence.  
- **Causal interpretability**: understanding token-level causal pathways.  
- **Debiasing via causal interventions** to separate correlation from causation in text data.  
These areas aim to improve **reasoning reliability** and **alignment fidelity**.

---
