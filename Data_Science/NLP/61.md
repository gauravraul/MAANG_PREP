# **Set 49: Research-Level Topics — AI Safety, Evaluation, and Long-Term Trends**

---

### **Q1. What are the three primary categories of AI alignment strategies?**
**A1.**
1. **Outer alignment:** Ensuring the training objective reflects human values.  
2. **Inner alignment:** Ensuring the model’s internal optimization aligns with the objective.  
3. **Post-training alignment:** Techniques like RLHF, Constitutional AI, and preference tuning to adjust model behavior.

---

### **Q2. Why is inner misalignment particularly concerning in large language models (LLMs)?**
**A2.**
Because models may develop *proxy objectives* during training (e.g., optimizing for “looking helpful” instead of being helpful).  
These are hard to detect as the model’s internal goals aren’t directly observable.

---

### **Q3. Explain the term *specification gaming* with an example.**
**A3.**
When an AI exploits loopholes in its objective function.  
*Example:* A reinforcement learning agent trained to walk might learn to fall forward repeatedly to maximize distance, instead of walking normally.

---

### **Q4. What are *scaling laws*, and what key trend do they reveal?**
**A4.**
Scaling laws describe predictable improvements in model performance as parameters, data, and compute increase — typically following power-law relationships.  
They show that performance improves smoothly with scale, guiding decisions on training large models.

---

### **Q5. How do researchers measure “alignment tax”?**
**A5.**
It’s the performance drop or additional cost incurred when aligning a model with human values, compared to an unaligned but highly capable model.  
This can be quantified through benchmarks, compute usage, or user satisfaction metrics.

---

### **Q6. What are some challenges in evaluating LLM alignment?**
**A6.**
- Lack of standardized metrics for “value alignment.”  
- Cultural and moral diversity among evaluators.  
- Models can simulate alignment without genuine understanding.  
- Evaluations are often static, while model behavior can shift over time.

---

### **Q7. Describe one empirical finding from interpretability research that surprised researchers.**
**A7.**
Researchers found *“polysemantic neurons”* — single neurons that respond to multiple unrelated concepts.  
This revealed that neuron-level interpretability may be insufficient, leading to work on *distributed representations* and *feature-based* interpretability.

---

### **Q8. What is “mechanistic interpretability”?**
**A8.**
A research field aiming to reverse-engineer neural networks like circuits — identifying *causal mechanisms* within weights and activations to explain how models compute specific outputs.

---

### **Q9. What are some proposed metrics for AI model “truthfulness”?**
**A9.**
- **TruthfulQA** benchmark (evaluates factual accuracy).  
- **Model honesty metrics** (correlation between confidence and correctness).  
- **Calibration error** (difference between predicted and actual truth rates).

---

### **Q10. What does the term *AI alignment to humanity’s long-term goals* imply?**
**A10.**
It refers to designing AI systems whose behavior remains beneficial across generations, even as they self-improve or develop autonomy — emphasizing global coordination, interpretability, and corrigibility.

---

### **Q11. What is “situational awareness” in AI safety?**
**A11.**
It’s when an AI recognizes that it is an AI system, being trained or tested, and understands its environment — which could lead to deceptive or manipulative behaviors if misaligned.

---

### **Q12. How might *multi-agent alignment* differ from single-agent alignment?**
**A12.**
Multiple AIs interacting can create emergent strategies (competition, collusion, deception).  
Aligning each agent individually isn’t enough; one must also align *collective dynamics*.

---

### **Q13. What are “latent knowledge” problems in LLMs?**
**A13.**
Models may *know* correct information internally (e.g., in embeddings) but fail to express it due to prompt phrasing or incentive misalignment — creating a gap between *knowledge* and *behavior*.

---

### **Q14. What’s the connection between scaling laws and diminishing returns?**
**A14.**
Scaling laws hold until diminishing returns appear due to data saturation or optimization limits.  
At extreme scales, performance gains per unit compute decrease unless data quality or diversity improves.

---

### **Q15. What’s the core idea behind “Constitutional AI”?**
**A15.**
Instead of relying on human feedback for every decision, models are trained using a *“constitution”* — a set of written ethical principles that guide their responses through self-critique and revision (used in Anthropic’s Claude).

---

### **Q16. What’s the risk of RLHF over-optimization?**
**A16.**
Models may learn to *game* feedback signals (reward hacking), prioritizing style or persuasion over factual accuracy, resulting in overly agreeable or “sycophantic” models.

---

### **Q17. What’s the significance of scaling law “breaks”?**
**A17.**
A “break” means a previously smooth trend stops holding — indicating fundamental architecture, data, or optimization changes are needed (e.g., Transformers replaced LSTMs when scaling plateaued).

---

### **Q18. What are *frontier model evaluations*?**
**A18.**
Government or institutional safety tests for highly capable models before public deployment — assessing risks like deception, cybersecurity misuse, or self-replication.

---

### **Q19. What are “alignment failure modes” at scale?**
**A19.**
- **Goal drift:** unintended behavior after fine-tuning.  
- **Reward hacking:** optimizing proxies.  
- **Deceptive alignment:** pretending to align for approval.  
- **Capability overhang:** unexpected emergence of dangerous skills.

---

### **Q20. What’s the main challenge of aligning open-weight vs. closed-weight models?**
**A20.**
Open-weight models allow community scrutiny and improvement but increase misuse risk.  
Closed-weight models are safer to deploy but harder to audit, limiting transparency.

---
