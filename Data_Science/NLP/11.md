# Research-Level NLP Interview Questions – Set 7 (Optimization & Efficiency in Agentic Multimodal AI)

## 1. Why is training multimodal LLMs more expensive than text-only LLMs?
**Answer:**  
- Multimodal models require **heterogeneous data pipelines** (images, video, text, audio).  
- Larger input embeddings (e.g., 224×224 images vs. 1D text tokens).  
- Vision models (ViTs, CNNs) + text models must be co-trained/aligned.  
- Video = temporal dimension → quadratic scaling.  
- Hardware: GPUs with higher memory & bandwidth.  

---

## 2. What techniques are used to reduce inference costs in multimodal systems?
**Answer:**  
- **Quantization**: lower precision (FP32 → INT8/INT4).  
- **Pruning**: remove redundant neurons/attention heads.  
- **Distillation**: train smaller student models from larger teachers.  
- **Caching**: reuse embeddings for repeated image/text queries.  
- **Adaptive computation**: early-exit when confidence is high.  

---

## 3. How can retrieval augmentation be optimized for multimodal agents?
**Answer:**  
- Store **compressed multimodal embeddings** (e.g., product quantization, FAISS).  
- Use **modality-specific encoders** + joint fusion late in pipeline.  
- Dynamic retrieval budget: only fetch extra images/docs when confidence is low.  
- Batch and cache retrieval calls for efficiency.  

---

## 4. Why is video understanding more computationally intensive than image or text tasks?
**Answer:**  
- Video has **spatial + temporal dimensions** → massive token count.  
- Example: 30s video @ 30 fps = 900 frames (each like an image).  
- Need temporal reasoning (3D CNNs, transformers).  
- Solutions:  
  - Frame sampling (keyframe extraction).  
  - Temporal pooling.  
  - Efficient video transformers (TimeSformer, MViT).  

---

## 5. What role does parameter-efficient fine-tuning (PEFT) play in multimodal AI?
**Answer:**  
- Enables adaptation of large multimodal models without retraining full weights.  
- Techniques:  
  - **LoRA** (Low-Rank Adaptation) → inject small trainable matrices.  
  - **Adapters**: modality-specific layers added between frozen blocks.  
  - **Prompt tuning**: learnable embeddings instead of updating core model.  
- Cuts training cost by 90%+, allows domain-specific tuning.  

---

## 6. How do memory bottlenecks arise in multimodal training, and how are they solved?
**Answer:**  
- High-resolution images/video produce huge activation maps.  
- Attention layers scale quadratically with tokens.  
- Solutions:  
  - Gradient checkpointing (trade compute for memory).  
  - FlashAttention (IO-efficient attention).  
  - Mixed-precision training (FP16/BF16).  
  - Sharded training (ZeRO, FSDP).  

---

## 7. What is model distillation in multimodal systems, and how does it differ from unimodal distillation?
**Answer:**  
- **Unimodal distillation**: teacher → student for text/vision only.  
- **Multimodal distillation**: must preserve **cross-modal alignment**.  
- Example:  
  - Teacher: CLIP-large.  
  - Student: CLIP-small but aligned on joint vision–text space.  
- Challenges: avoid collapsing modality-specific representations.  

---

## 8. How do you optimize inference for real-time multimodal agents (e.g., AR/VR assistants)?
**Answer:**  
- Low-latency requirements (<100ms).  
- Solutions:  
  - Edge inference (run on-device for preprocessing).  
  - Model partitioning: lightweight model locally + heavy model in cloud.  
  - Streaming inference (process partial input before full completion).  
  - Neural caching (reusing embeddings for repeated scenes).  

---

## 9. Why is multimodal scaling different from pure text scaling?
**Answer:**  
- Text scaling laws (power-law scaling in data & parameters) don’t transfer cleanly.  
- Adding vision/audio introduces **different noise levels, entropy, and modality gaps**.  
- More data ≠ better → multimodal alignment may plateau.  
- Scaling requires **balanced multimodal datasets**, not just larger size.  

---

## 10. What are future directions for efficient multimodal LLM optimization?
**Answer:**  
- **Sparse mixture-of-experts** (activate only subset of parameters).  
- **Neural compression**: compress vision/audio embeddings while retaining semantics.  
- **Adaptive modality routing**: only process relevant modalities.  
- **Hardware co-design**: GPUs/TPUs optimized for multimodal workloads.  
- **Continual learning**: avoid retraining from scratch when new modalities are added.
