# Hard Level NLP Questions and Answers (Set 19)

### 1. What is the difference between extractive and abstractive summarization?
**Answer:**  
- **Extractive:** Selects key sentences/phrases from the text (e.g., TextRank).  
- **Abstractive:** Generates new sentences capturing the meaning (e.g., seq2seq with attention, Transformers).  
Abstractive is closer to human-like summarization but harder due to semantic understanding.  

---

### 2. Why is coreference resolution challenging in NLP?
**Answer:**  
Identifying that "he" refers to "John" requires world knowledge, discourse understanding, and long-context modeling.  
Ambiguity, pronouns, and nested references make it hard.  
Modern approaches use span-based Transformers (e.g., SpanBERT).  

---

### 3. How do masked language models (MLMs) differ from autoregressive models?
**Answer:**  
- **MLMs (BERT):** Predict missing tokens using both left and right context.  
- **Autoregressive (GPT):** Predict next token given left context only.  
MLMs are bidirectional (good for understanding), while autoregressive models are better for generation.  

---

### 4. What are pretrained embeddings’ limitations in downstream NLP tasks?
**Answer:**  
- Domain mismatch (e.g., medical vs general text).  
- Bias amplification from training corpora.  
- High computational cost for fine-tuning.  
Solutions: domain adaptation, prompt tuning, lightweight adapters.  

---

### 5. Why is semantic role labeling (SRL) important in NLP?
**Answer:**  
SRL identifies **who did what to whom, when, and where**.  
It helps in QA, IE, and dialogue systems.  
Example: *"Mary gave John a book"* → (Agent=Mary, Recipient=John, Object=book).  

---

### 6. What is catastrophic forgetting in continual NLP training?
**Answer:**  
When models fine-tuned on new tasks forget old ones.  
Mitigation: **Elastic Weight Consolidation (EWC), replay buffers, parameter isolation techniques**.  

---

### 7. How do knowledge graphs enhance NLP applications?
**Answer:**  
They provide structured world knowledge that improves reasoning and disambiguation.  
Example: Linking "Apple" to {company, fruit} depending on context.  
Used in RAG, QA, and dialogue systems.  

---

### 8. How does contrastive learning apply in NLP?
**Answer:**  
Contrastive learning pushes similar samples closer and dissimilar ones apart.  
Examples: SimCSE for sentence embeddings, CLIP for text-image alignment.  
It improves semantic representation without heavy supervision.  

---

### 9. Why do Transformer models struggle with rare words?
**Answer:**  
Rare tokens may be split into many subwords, leading to weak semantic representation.  
Mitigation: **dynamic vocabularies, character-level embeddings, or retrieval-augmented methods**.  

---

### 10. What are evaluation challenges in open-ended NLP tasks?
**Answer:**  
- **Summarization, dialogue, generation** lack single correct answers.  
- Metrics (BLEU, ROUGE) measure overlap, not meaning.  
- Human evaluation is costly.  
Newer metrics: **BERTScore, BLEURT, GPT-based evaluators**.
