## Q71: How do we evaluate hallucinations in NLP models?

**Answer:**
- **Hallucination:** When a model generates fluent but factually incorrect outputs.
- **Evaluation Methods:**
  - **String Matching:** Compare outputs with gold references (e.g., BLEU, ROUGE).
  - **Fact-Checking Models:** Use external verifiers or NLI models.
  - **Retrieval Grounding:** Compare generated facts against trusted knowledge bases.
- **Challenge:** A model may generate novel but correct facts not in the dataset, making evaluation tricky.

---

## Q72: What are the challenges of multilingual NLP?

**Answer:**
- **Challenges:**
  - Resource imbalance: High-resource (English, Chinese) vs low-resource (Amharic, Quechua).
  - Script and morphology diversity.
  - Code-switching (mix of languages in one sentence).
- **Approaches:**
  - Multilingual embeddings (mBERT, XLM-R).
  - Translation-based transfer.
  - Cross-lingual pretraining.
- **Key Problem:** Models often perform poorly on languages underrepresented in training data.

---

## Q73: How do LLMs transfer knowledge to low-resource languages?

**Answer:**
- **Mechanisms:**
  - Shared subword vocabularies (Byte-Pair Encoding, SentencePiece).
  - Cross-lingual representation learning.
  - Zero-shot transfer from high-resource to low-resource.
- **Example:** Training on English and French helps generalize to Spanish.
- **Limitation:** Idiomatic expressions and cultural context often get lost.

---

## Q74: What ethical concerns arise in NLP for large models?

**Answer:**
- **Bias & Fairness:** Models may amplify stereotypes.
- **Privacy:** LLMs may memorize sensitive training data.
- **Misinformation:** Models may generate convincing fake news.
- **Mitigation:** Differential privacy, bias audits, red-teaming, content filters.
- **Key Issue:** Balancing performance with responsible use.

---

## Q75: How can evaluation benchmarks become outdated in NLP?

**Answer:**
- **Causes:**
  - Models trained on benchmark data (data leakage).
  - Benchmarks not reflecting real-world complexity.
  - Static benchmarks fail to capture evolving knowledge.
- **Solutions:**
  - Dynamic benchmarks (e.g., updated yearly).
  - Adversarial datasets.
  - Task-specific and domain-specific evaluation.
- **Example:** GLUE became saturated â†’ led to SuperGLUE.
