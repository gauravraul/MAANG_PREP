## Q51: What is prompt engineering, and why is it important in NLP?

**Answer:**
- **Prompt Engineering** is the process of crafting input prompts to guide LLMs toward producing the desired output.
- **Importance:**
  1. LLMs are sensitive to input phrasing.
  2. Well-structured prompts can drastically improve accuracy without retraining.
  3. Enables zero-shot and few-shot learning.
- **Examples:**
  - Poor prompt: `"Translate this: Hello"`
  - Better prompt: `"Translate 'Hello' from English to Spanish." → "Hola"`
- **Applications:** Chatbots, QA systems, summarization, data extraction.

---

## Q52: What are adapter layers in NLP, and how do they differ from fine-tuning?

**Answer:**
- **Adapter Layers:** Small bottleneck layers added to each Transformer block.
- **Key Points:**
  - Pretrained LLM weights are frozen.
  - Only adapters are trained for a specific task.
  - Much smaller number of trainable parameters compared to full fine-tuning.
- **Advantages:**
  - Efficient multi-task adaptation.
  - Memory and compute savings.
  - Supports parameter-efficient fine-tuning (PEFT).
- **Example Frameworks:** Hugging Face’s `AdapterHub`.

---

## Q53: What is Low-Rank Adaptation (LoRA), and why is it effective?

**Answer:**
- **LoRA (Low-Rank Adaptation):**
  - Decomposes large weight update matrices into two smaller low-rank matrices.
  - Only these matrices are trained, while base LLM weights remain frozen.
- **Advantages:**
  - Significant reduction in trainable parameters.
  - Faster training, lower memory usage.
  - Easily mergeable with the base model.
- **Use Case:** Fine-tuning 65B+ models on consumer GPUs.

---

## Q54: What role does Reinforcement Learning from Human Feedback (RLHF) play in NLP?

**Answer:**
- **Purpose:** Aligns LLM behavior with human preferences.
- **Steps:**
  1. **Supervised Fine-Tuning (SFT):** Train on curated human-labeled data.
  2. **Reward Model Training:** Human feedback is used to score outputs.
  3. **RLHF Optimization:** Use reinforcement learning (PPO, DPO) to optimize LLM against the reward model.
- **Impact:**
  - Reduces harmful or biased outputs.
  - Improves helpfulness and alignment.
  - Used in models like ChatGPT, Claude, Gemini.

---

## Q55: What are some limitations of RLHF in LLMs?

**Answer:**
- **Challenges:**
  1. **Scalability:** Human feedback collection is expensive and time-consuming.
  2. **Reward Hacking:** Models may exploit reward signals instead of truly aligning.
  3. **Bias Propagation:** Human annotators’ biases influence the model.
  4. **Inconsistency:** Feedback can be subjective across cultures and domains.
  5. **Over-Optimization Risk:** May lead to bland, generic, or over-cautious responses.
- **Mitigation:** Combining RLHF with **constitutional AI**, **synthetic feedback**, and **direct preference optimization (DPO)**.

---
