# Advanced System Design & Optimization in GenAI (Set 9)

## 1. Why is pipeline parallelism critical for large LLM training?
**Answer:**  
- Models like GPT-4 exceed single GPU memory.  
- Pipeline parallelism splits layers across devices.  
- Challenges:  
  - Bubble overhead (idle GPUs during warmup).  
  - Load imbalance across stages.  
- Solutions: virtual pipeline stages, interleaved 1F1B scheduling.

---

## 2. How does tensor parallelism differ from data parallelism?
**Answer:**  
- **Data parallelism** → each GPU processes different batches, synchronizes gradients.  
- **Tensor parallelism** → splits tensors within a layer (e.g., matrix multiplication across GPUs).  
- Used in Megatron-LM.  
- Requires **fast intra-node communication (NVLink/InfiniBand)**.

---

## 3. What makes optimizer state sharding (ZeRO) powerful?
**Answer:**  
- Adam stores **3x parameters** (weights, gradients, moments).  
- ZeRO shards optimizer states across devices.  
- Reduces memory footprint drastically → enables 10–100B scale training.  
- Used in **DeepSpeed**.

---

## 4. Why is activation checkpointing useful in training?
**Answer:**  
- Normal training stores all activations for backprop.  
- Memory bottleneck for large models.  
- Checkpointing = recompute some activations on backward pass.  
- Tradeoff: memory ↓, compute ↑.  
- Essential for scaling beyond single GPU memory.

---

## 5. What are the key difficulties in mixed precision training?
**Answer:**  
- FP16 or BF16 reduce memory and increase throughput.  
- Issues:  
  - Gradient underflow.  
  - Numerical instability in layer norm, softmax.  
- Solutions: loss scaling, selective FP32 accumulation.  
- Critical for LLM efficiency.

---

## 6. How does speculative decoding improve inference speed?
**Answer:**  
- Large model predicts multiple tokens ahead.  
- Small draft model proposes tokens.  
- Large model verifies them in batches.  
- If valid → accept; else fall back.  
- Cuts inference latency **30–60%**.  
- Used in GPT-4 Turbo.

---

## 7. What makes MoE (Mixture of Experts) efficient but complex?
**Answer:**  
- Only **subset of experts activated per token** → compute efficiency.  
- Scaling law: trillions of parameters but lower FLOPs.  
- Issues:  
  - Load balancing across experts.  
  - Training instability.  
- GShard, Switch Transformer explored solutions.

---

## 8. Why is KV cache compression critical for LLM inference?
**Answer:**  
- Attention requires storing **key/value states** for all previous tokens.  
- KV cache grows linearly with sequence length.  
- Compression strategies:  
  - Quantization (INT8/INT4 KV).  
  - Token eviction (drop unimportant tokens).  
  - Low-rank approximation.  
- Tradeoff: memory vs accuracy.

---

## 9. What are challenges in distributed inference serving?
**Answer:**  
- Serving billions of requests requires:  
  - Sharded models.  
  - Dynamic batching.  
  - Request scheduling under latency SLAs.  
- Issues:  
  - Stragglers in multi-GPU decoding.  
  - Efficient prefill + decode separation.  
- Solutions: **vLLM, FasterTransformer**.

---

## 10. What’s the next frontier in efficient LLM training?
**Answer:**  
- Beyond parallelism + quantization:  
  - **Training with limited supervision** (self-play, distillation).  
  - **Sparse activation** beyond MoE.  
  - **Memory-augmented architectures** (long context without quadratic scaling).  
  - **Neuromorphic hardware co-design**.  
- Goal: push trillion-parameter models into real-time interactive deployment.
