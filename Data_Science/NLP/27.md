# Hard Level NLP Questions and Answers (Set 15)

### 1. What is the difference between extractive and abstractive summarization?
**Answer:**  
- **Extractive:** Selects key sentences/phrases directly from text (e.g., TextRank).  
- **Abstractive:** Generates new sentences that capture the essence (e.g., BART, T5).  

---

### 2. How do multilingual models like mBERT handle cross-lingual transfer?
**Answer:**  
They learn shared subword embeddings across languages using joint tokenization (WordPiece, SentencePiece). This allows transfer between languages even without parallel corpora.  

---

### 3. Explain the vanishing gradient problem in RNNs and how LSTMs/GRUs solve it.
**Answer:**  
RNN gradients shrink across long sequences, limiting memory. LSTMs/GRUs use gating mechanisms (forget, input, output gates) to preserve information over longer spans.  

---

### 4. What are adversarial examples in NLP, and why are they problematic?
**Answer:**  
Adversarial examples are small perturbations (e.g., typos, paraphrasing) that mislead models. They expose robustness issues in NLP systems, especially in security-sensitive tasks like spam detection.  

---

### 5. How do knowledge graphs enhance NLP applications?
**Answer:**  
Knowledge graphs provide structured semantic relationships between entities. They enhance QA, dialogue, and recommendation by grounding model outputs in real-world facts.  

---

### 6. Explain the concept of embedding alignment across modalities.
**Answer:**  
Embedding alignment ensures representations from different modalities (e.g., text, images) map to a shared space. Example: CLIP aligns image embeddings with textual descriptions.  

---

### 7. What are the challenges of training extremely large language models?
**Answer:**  
- Computational cost (TPU/GPU clusters).  
- Memory bottlenecks.  
- Data quality/curation.  
- Alignment with human values.  
- Environmental impact (energy usage).  

---

### 8. How does the Transformer’s self-attention compare with convolutional and recurrent operations?
**Answer:**  
- **Self-attention:** Captures global dependencies directly (O(n²) complexity).  
- **CNNs:** Capture local patterns with fixed receptive fields.  
- **RNNs:** Capture sequential dependencies but struggle with long contexts.  

---

### 9. What is zero-shot learning in NLP?
**Answer:**  
Zero-shot learning enables models to solve unseen tasks using natural language instructions, without task-specific training (e.g., GPT-3 performing sentiment classification via prompts).  

---

### 10. How does gradient checkpointing help in NLP training?
**Answer:**  
Gradient checkpointing reduces memory usage by storing fewer intermediate activations and recomputing them during backpropagation, enabling training of larger models with limited GPU memory.
