## Q81: What is neuro-symbolic NLP, and why is it important?

**Answer:**
- **Definition:** Combines neural networks (statistical learning) with symbolic reasoning (rules, logic).
- **Importance:**
  - Enhances interpretability and reasoning.
  - Improves factual accuracy in knowledge-heavy tasks.
- **Example:** Neural model extracts entities → symbolic system verifies relations via knowledge graph.
- **Challenge:** Bridging continuous embeddings with discrete logical operations.

---

## Q82: What is compositional generalization in NLP?

**Answer:**
- **Definition:** Ability to generalize to new combinations of known concepts (like humans do).
- **Example:** If model knows “red ball” and “blue cube,” it should infer “blue ball” without training.
- **Problem:** LLMs often fail to extrapolate beyond training distribution.
- **Solutions:**
  - Structured architectures (Compositional Attention Networks).
  - Data augmentation with novel compositions.
- **Applications:** Natural language understanding, robotic commands.

---

## Q83: What are advanced prompt engineering strategies in NLP?

**Answer:**
- **Strategies:**
  - **Chain-of-Thought (CoT):** Step-by-step reasoning.
  - **Self-Consistency:** Sample multiple reasoning paths, then majority vote.
  - **Few-Shot with Examples:** Provide demonstrations in context.
  - **Role Prompting:** Assign roles (e.g., "You are a math teacher").
  - **Tool-Augmented Prompts:** Include explicit API or calculator calls.
- **Challenge:** Sensitivity to phrasing; brittle to distribution shifts.

---

## Q84: What role does Reinforcement Learning from Human Feedback (RLHF) play in NLP?

**Answer:**
- **Purpose:** Align model outputs with human preferences.
- **Process:**
  1. Pretrain base LLM.
  2. Collect human preference data (A > B).
  3. Train reward model.
  4. Fine-tune with RL (PPO).
- **Advantages:** Reduces toxic or unhelpful outputs.
- **Limitations:** Expensive, subjective labeling, reward hacking risks.

---

## Q85: What are self-improving NLP systems?

**Answer:**
- **Definition:** Systems that refine themselves continuously using feedback, external tools, or new data.
- **Examples:**
  - LLMs correcting own mistakes via reflection.
  - Self-play in dialogue generation.
  - Online learning pipelines with RAG + feedback loops.
- **Challenges:**
  - Avoiding error reinforcement.
  - Guaranteeing safety with autonomous updates.
- **Future:** Move from static pretrained models → dynamic lifelong learners.
