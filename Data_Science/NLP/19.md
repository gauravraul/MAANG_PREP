## Q56: How do multilingual language models handle cross-lingual transfer?

**Answer:**
- **Cross-lingual transfer:** Ability of a model trained on one language to perform tasks in another.
- **Mechanism:**
  - Shared subword vocabularies (e.g., SentencePiece, BPE).
  - Shared embeddings across languages with overlapping alphabets.
  - Alignment of representations in multilingual pretraining.
- **Advantages:**
  - Enables zero-shot translation or QA.
  - Resource-poor languages benefit from resource-rich ones.
- **Challenge:** Performance imbalance â€” high-resource languages dominate low-resource ones.

---

## Q57: What are retrieval-augmented language models, and how do they improve NLP tasks?

**Answer:**
- **Retrieval-Augmented Models:** Combine LLMs with external knowledge bases.
- **Architecture:**
  - Query encoder retrieves relevant documents.
  - Retrieved context is fed into the LLM for response generation.
- **Benefits:**
  - Reduces hallucination by grounding in facts.
  - Improves domain-specific QA.
  - Supports dynamic knowledge updates without retraining.
- **Examples:** RAG, REALM, RETRO, Atlas.

---

## Q58: How do you evaluate large-scale NLP systems beyond accuracy and F1?

**Answer:**
- **Advanced Evaluation Metrics:**
  1. **Calibration:** How well predicted probabilities reflect true likelihood.
  2. **Fairness & Bias:** Gender, race, culture sensitivity.
  3. **Explainability:** Interpretability of attention weights, gradients, or SHAP.
  4. **Latency & Throughput:** Efficiency for deployment.
  5. **Robustness:** Adversarial input resistance.
- **Real-world Evaluation:** User satisfaction, A/B testing, task completion.

---

## Q59: What are catastrophic forgetting issues in NLP fine-tuning?

**Answer:**
- **Catastrophic Forgetting:** A fine-tuned LLM loses general pretraining knowledge when adapting to a specific task.
- **Causes:**
  - Overwriting parameters during gradient updates.
  - Narrow domain adaptation.
- **Mitigation:**
  - Elastic Weight Consolidation (EWC).
  - Adapter layers, LoRA, or prefix-tuning.
  - Multi-task or continual learning strategies.
- **Example:** Fine-tuning GPT-3 on medical texts may reduce performance on general QA tasks.

---

## Q60: What are the main challenges in scaling NLP models beyond 100B parameters?

**Answer:**
- **Challenges:**
  1. **Compute Cost:** Training requires thousands of GPUs for months.
  2. **Memory Bottlenecks:** Model and optimizer states exceed GPU RAM.
  3. **Data Limitations:** Quality, diversity, and availability of training data.
  4. **Communication Overhead:** Synchronizing parameters across distributed clusters.
  5. **Inference Latency:** Deployment at scale is expensive and slow.
- **Solutions:**
  - Efficient architectures (Mixture-of-Experts).
  - Quantization & pruning.
  - Parameter-efficient tuning.
  - Specialized hardware (TPUs, AI accelerators).
