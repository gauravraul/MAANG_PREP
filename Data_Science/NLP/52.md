# Research-Level NLP Q&A – Set 33

## 1. What is "Sparse Mixture of Experts" (MoE) and why is it used in LLMs?
**Answer:**  
Sparse Mixture of Experts divides a large model into multiple subnetworks (experts), activating only a few per input token.  
This allows scaling model capacity without proportional compute cost.  
For example, GLaM and Switch Transformer route tokens using a **gating network**, reducing FLOPs by ~10× while maintaining performance.  
Challenges include **load balancing**, **expert collapse**, and **routing instability** during training.

---

## 2. How do retrieval-augmented models differ from memory-augmented models?
**Answer:**  
- **Retrieval-Augmented Generation (RAG):** Queries an **external static corpus** at inference to inject factual grounding.  
- **Memory-Augmented Models:** Maintain a **learned dynamic memory** that updates as the model interacts with new data.  
While RAG is grounded and explicit, memory models (like RETRO, MemGPT) provide **continual context** and **personalization**, but are harder to align and scale.

---

## 3. Explain the "Lottery Ticket Hypothesis" and its relevance to NLP.
**Answer:**  
The Lottery Ticket Hypothesis states that within a large overparameterized network, there exists a smaller **subnetwork (winning ticket)** that can be trained to reach comparable performance.  
In NLP, this inspires:
- **Sparse fine-tuning** (LoRA, adapter pruning).
- **Efficient transfer learning**.
- **Interpretable subnetworks** that preserve linguistic capabilities with fewer weights.

---

## 4. What are "Scaling Laws" and why are they vital for LLM design?
**Answer:**  
Scaling laws describe predictable relationships between **loss, model size, dataset size, and compute**.  
Empirical studies (Kaplan et al., 2020) show power-law relations:
\[
\text{Loss} \propto N^{-\alpha} + D^{-\beta} + C^{-\gamma}
\]
These help forecast diminishing returns, guide budget allocation, and decide when to **increase data vs. model capacity**.  
Recent work explores **data quality-aware scaling** and **multimodal scaling laws**.

---

## 5. How does "Neural Tangent Kernel (NTK)" theory explain transformer generalization?
**Answer:**  
NTK approximates neural network behavior as a kernel regression problem in infinite-width limits.  
In transformers, NTK helps understand:
- **Why large models generalize better.**
- **How training dynamics linearize.**
It provides a theoretical lens for **gradient flow stability** and **implicit regularization**, especially under overparameterization.

---

## 6. What is "Constitutional AI" and how does it improve alignment?
**Answer:**  
Constitutional AI replaces human feedback with **AI-driven feedback**, guided by a predefined set of ethical or policy principles (the "constitution").  
Steps:
1. Model generates responses.
2. A critique model evaluates them via principles.
3. The model self-revises.  
Used in **Anthropic’s Claude**, this approach improves **transparency**, **consistency**, and **alignment scalability**.

---

## 7. What are “Sparse Autoencoders” used for in interpretability research?
**Answer:**  
Sparse Autoencoders decompose model activations into **interpretable latent features** by enforcing sparsity.  
They help identify human-understandable concepts (e.g., syntax, sentiment) encoded in neurons.  
These features assist in **mechanistic interpretability**, **bias tracing**, and **representation debugging**.

---

## 8. How do "LoRA" and "QLoRA" differ in efficient fine-tuning?
**Answer:**  
- **LoRA (Low-Rank Adaptation):** Adds trainable low-rank matrices to attention weights. Only a few parameters are updated.  
- **QLoRA:** Extends LoRA by combining **quantization (4-bit)** with low-rank adapters, enabling **fine-tuning 65B models on a single GPU**.  
QLoRA achieves near full-precision performance while saving >90% memory.

---

## 9. What is “Speculative Decoding” and how does it accelerate inference?
**Answer:**  
Speculative decoding uses a **small draft model** to predict multiple tokens ahead.  
The large model then **verifies** or **rejects** those tokens in parallel.  
This drastically reduces latency without changing output quality.  
It’s a key technique for **real-time chat** and **streaming applications**.

---

## 10. How does "Direct Preference Optimization (DPO)" improve over RLHF?
**Answer:**  
DPO aligns models using preference data without reinforcement learning.  
It directly optimizes:
\[
\log P_\theta(y_1|x) - \log P_\theta(y_2|x)
\]
to match human-labeled preferences (preferred vs. dispreferred responses).  
Advantages:
- No reward model needed.
- Stable training.
- Faster convergence.  
DPO has become a foundation for **alignment-efficient fine-tuning** in LLMs.

---
