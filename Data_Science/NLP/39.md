# Research-Level NLP / Multimodal AI / Speech Questions (Set 27)

## Q1. How does reinforcement learning (RLHF/RLAIF) differ when applied to multimodal models compared to text-only LLMs?  
**A1.**  
- Text-only models: reward based on coherence, helpfulness, safety.  
- Multimodal models: reward also depends on grounding (e.g., does the description match the image?).  
- Challenge: defining consistent cross-modal reward signals.  
- Example: a model may produce a correct caption linguistically but mismatch the visual evidence.

---

## Q2. What are the bottlenecks in scaling long-context multimodal transformers?  
**A2.**  
- Quadratic attention cost with both long sequences and high-dimensional image embeddings.  
- Memory explosion due to storing multimodal tokens.  
- Current mitigations: sparse attention, hierarchical encodings, retrieval-based context extension.

---

## Q3. Why is evaluation of multimodal reasoning more difficult than unimodal reasoning?  
**A3.**  
- Text-only: BLEU, ROUGE, perplexity.  
- Multimodal: correctness may require subjective human judgment.  
- Example: Captioning an image of a dog in the snow → "dog in winter" vs. "dog playing outside" (both plausible).  
- Lack of standardized benchmarks across tasks and modalities.

---

## Q4. How do scaling laws break down in speech-language models?  
**A4.**  
- Speech models rely on acoustic variability; scaling dataset size doesn’t always improve phoneme recognition.  
- Performance saturates faster in low-resource languages.  
- Pretraining on synthetic speech sometimes violates scaling laws due to lack of acoustic diversity.

---

## Q5. What are the ethical concerns unique to multimodal foundation models?  
**A5.**  
- Privacy leaks from images/videos (faces, locations).  
- Misinterpretation in medical imaging.  
- Multimodal deepfakes (voice + video).  
- Higher risk of misuse since evidence appears more “trustworthy” than text-only hallucinations.

---

## Q6. Why is retrieval harder in multimodal RAG systems than text-only RAG?  
**A6.**  
- Need to align heterogeneous embeddings (image, text, speech).  
- Cross-modal retrieval often suffers from **modality dominance** (text bias).  
- Lack of multimodal knowledge graphs for structured grounding.  

---

## Q7. How does temporal reasoning complicate speech-language models?  
**A7.**  
- Speech is inherently temporal and continuous, unlike text.  
- Models must align phonemes with textual tokens.  
- Errors accumulate over time → cascading misalignments in transcription and reasoning.

---

## Q8. What are the technical limits of current multimodal attention mechanisms?  
**A8.**  
- Standard attention assumes token homogeneity.  
- Multimodal models mix dense and sparse features (pixels vs. words).  
- Leads to attention collapse (model ignores weaker modalities).  
- Solutions: modality-specific attention heads, gated fusion.

---

## Q9. Why do multimodal models hallucinate “phantom objects” in visual question answering?  
**A9.**  
- Bias from training data (frequent co-occurrences).  
- Model relies too much on language priors rather than visual grounding.  
- Example: asked about "zebra in image," model may answer "yes" even if no zebra is present.

---

## Q10. What role could causal inference play in multimodal AI?  
**A10.**  
- Helps disentangle correlation vs. causation in cross-modal features.  
- Example: distinguishing “umbrella causes shadow” vs. “rain causes umbrella.”  
- Reduces spurious correlations → improves robustness and generalization.
