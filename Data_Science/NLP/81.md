# GenAI Interview — Sample Questions & Answers (For 10 Years Experienced Candidate)

---

## **1. What is the difference between a traditional ML pipeline and a GenAI pipeline?**

**Answer:**  
A traditional ML pipeline = structured data → feature extraction → model training → evaluation → deployment.

A GenAI pipeline includes:  
- Data collection  
- Tokenization & representation learning  
- Pretraining  
- Instruction tuning (SFT)  
- Preference optimization (DPO/PPO)  
- Evaluation using LLM-judge or generative metrics  
- RAG integration  
- Guardrails  
- Deployment optimization (quantization, LoRA, distillation)

**Key differences:**  
- Pretraining (self-supervised)  
- Generative tasks  
- Post-training with human feedback  
- Heavy compute requirement

---

## **2. How do you decide between fine-tuning, RAG, and prompting?**

**Answer:**  

| Requirement | Best Approach |
|------------|---------------|
| No new domain knowledge | Prompting |
| Knowledge changes frequently | RAG |
| Model must learn deep domain patterns | Fine-tuning |
| Need both knowledge + behavior | RAG + Fine-tuning |

**Rule of thumb:**  
- RAG → knowledge  
- Fine-tuning → behavior  
- Prompting → logic/formatting

---

## **3. Explain RAG architecture end-to-end.**

**Answer:**  
1. **Indexing** → chunk, embed, store in vector DB  
2. **Retrieval** → ANN search  
3. **Augmentation** → retrieved chunks added to prompt  
4. **Generation** → grounded LLM answer  
5. **Evaluation** → hallucination check, reranking  

**Improvements:**  
- Hybrid search  
- Cross-encoder rerankers  
- Semantic chunking

---

## **4. What techniques help reduce hallucinations in LLMs?**

**Answer:**  
- RAG  
- Reranking  
- Guardrails (Llama Guard, NeMo)  
- Constrained decoding  
- Logit biasing  
- Self-verification loops  
- Evidence-based generation checks

---

## **5. What are the training stages of a modern GenAI/LLM model?**

**Answer:**  
1. Pretraining  
2. Supervised Fine-Tuning (SFT)  
3. Preference Optimization (DPO, PPO)  
4. Domain LoRA  
5. Evaluation & red teaming  
6. Deployment optimization (quant, distillation)

---

## **6. How do you evaluate generative models beyond BLEU/ROUGE?**

**Answer:**  

### **Human-Based**
- A/B testing  
- Rubric-based scoring  

### **Model-Based**
- LLM-as-a-judge  
- Reward modeling  

### **Domain-Specific**
- Fact-checking  
- Code execution accuracy  
- Retrieval metrics (nDCG, mAP)  
- Hallucination rate tracking

---

## **7. How would you design a scalable enterprise RAG pipeline?**

**Answer:**  
- Document ingestion service  
- Semantic + overlap chunking  
- Embeddings pipeline  
- Hybrid search (BM25 + vector)  
- Cross-encoder reranker  
- Orchestration using LangGraph / DSPy  
- Semantic caching  
- Security: PII scrub, encryption, audits  
- Monitoring: latency, token usage, hit-rate

---

## **8. Difference between LoRA and QLoRA.**

**Answer:**  

| Feature | LoRA | QLoRA |
|--------|------|-------|
| Weight precision | FP16/BF16 | 4-bit quantized |
| Memory usage | Medium | Very low |
| Training cost | Higher | Lowest |
| Use case | Enterprise infra | Single GPU fine-tuning |

---

## **9. How do you productionize an LLM-based agent?**

**Answer:**  
- Finite-state agent design  
- Tool calling enabled (DB, APIs, RAG)  
- Memory (episodic, long-term)  
- Hallucination reduction via tool-first policies  
- Telemetry for cost/latency  
- Versioning agent graphs  
- Sandbox tools  
- Fallback deterministic responses

---

## **10. How do you approach an ambiguous GenAI requirement?**

**Answer:**  
1. Identify task type  
2. Review available data  
3. Determine constraints  
4. Define success metrics  
5. Prototype with prompting  
6. If unstable → move to RAG  
7. If consistency required → fine-tuning  
8. For reasoning → agent + tools  

**Goal:** deliver a fast MVP to clarify hidden requirements.

---
