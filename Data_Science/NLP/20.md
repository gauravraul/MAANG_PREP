## Q61: How does multimodal NLP differ from traditional NLP?

**Answer:**
- **Traditional NLP:** Deals only with text (sequence-to-sequence, classification, etc.).
- **Multimodal NLP:** Integrates text with other modalities such as vision, audio, or video.
- **Examples:**
  - Image captioning (vision + text).
  - Video QA (vision + text + audio).
  - Speech translation (audio + text).
- **Challenges:**
  - Aligning heterogeneous representations (e.g., pixels vs tokens).
  - Fusion strategies (early, late, or hybrid fusion).
- **Models:** CLIP, Flamingo, GPT-4V, Kosmos.

---

## Q62: What is grounding in NLP, and why is it important?

**Answer:**
- **Grounding:** Linking abstract language representations to real-world context (images, environment, knowledge bases).
- **Importance:**
  - Improves factual accuracy.
  - Enables embodied agents (robots, AR assistants).
  - Supports reasoning beyond text.
- **Example:** When asked *"What’s in front of me?"*, a grounded model uses vision data instead of hallucinating from text training.

---

## Q63: How do NLP models integrate structured knowledge (e.g., Knowledge Graphs)?

**Answer:**
- **Approaches:**
  1. **Entity Linking:** Map text to graph entities.
  2. **Graph Embeddings:** Incorporate structural knowledge into vector spaces.
  3. **Neuro-Symbolic Models:** Combine symbolic reasoning with neural networks.
- **Benefits:**
  - Better factual consistency.
  - Reasoning over relations (e.g., drug–disease interactions).
- **Challenge:** Scaling graph-based reasoning with large LLMs.

---

## Q64: What is evaluation leakage in NLP, and how do you prevent it?

**Answer:**
- **Evaluation Leakage:** When test set performance is inflated because data overlaps with pretraining or fine-tuning data.
- **Causes:**
  - Web-scraped corpora overlapping with benchmarks (e.g., SQuAD, GLUE).
  - Unintended memorization by LLMs.
- **Mitigation:**
  - Curate test sets from unseen distributions.
  - Use time-split datasets (post-pretraining).
  - Evaluate on adversarial or synthetic tasks.

---

## Q65: How do you design adversarial evaluation for NLP models?

**Answer:**
- **Adversarial Evaluation:** Tests robustness by exposing models to perturbed or challenging inputs.
- **Methods:**
  - **Text Perturbations:** Synonym swaps, typos, paraphrasing.
  - **Logic Tests:** Contradictions, negations, counterfactuals.
  - **Prompt Injection (for LLMs):** Misleading instructions to bypass safeguards.
- **Example:** Rephrasing “The man is not unhappy” → should be classified as *positive sentiment*.
- **Outcome:** Helps measure generalization and resilience.
