# Set 43: Advanced NLP — Multilingual, Cross-lingual & Global AI

---

### **Q1.** What are the main challenges in multilingual NLP compared to monolingual NLP?
**A1.** Multilingual NLP faces challenges like:
- **Data imbalance** (some languages have low resources)
- **Script diversity** (non-Latin alphabets)
- **Cultural nuances** in semantics
- **Transfer inefficiency** (models fine-tuned on English may not generalize well)
- **Evaluation bias** (benchmarks are English-dominant)

---

### **Q2.** Explain zero-shot cross-lingual transfer.
**A2.** It’s when a model trained on one language (e.g., English) performs tasks in another (e.g., Hindi) **without additional training data**.  
Models like **XLM-R** achieve this by using shared subword tokenization and multilingual pretraining on large text corpora.

---

### **Q3.** What is the difference between code-switching and code-mixing in NLP?
**A3.**
- **Code-switching:** Alternating between languages at sentence or clause boundaries.  
  _Example:_ “I am going to bazaar kal.”  
- **Code-mixing:** Mixing words within a single clause.  
  _Example:_ “Kal office jaake report submit karni hai.”

---

### **Q4.** How do multilingual embeddings (like LASER, LaBSE) achieve alignment?
**A4.** They align sentence representations across languages using **parallel corpora** and **contrastive learning**, ensuring that sentences with the same meaning in different languages are close in the embedding space.

---

### **Q5.** What is massively multilingual pretraining, and which models exemplify it?
**A5.** It’s pretraining over 50–100+ languages simultaneously to enable **universal representation learning**.  
Examples include **mBERT**, **XLM-R**, and **mT5**, which use shared vocabularies and transformer architectures.

---

### **Q6.** What are typological features, and how do they affect multilingual NLP?
**A6.** Typological features (e.g., morphology, syntax order) describe linguistic structures.  
Models that incorporate them (like **Typologically-informed Transformers**) handle structurally diverse languages better, improving generalization.

---

### **Q7.** Why do multilingual models exhibit “interference” between languages?
**A7.** Because shared parameters across languages can cause **negative transfer**, where learning one language disrupts performance on another, especially when resource sizes differ drastically.

---

### **Q8.** What is cross-lingual alignment drift and how can it be mitigated?
**A8.** It’s when alignment between languages degrades after fine-tuning for a specific task.  
It can be mitigated using:
- **Alignment regularization losses**
- **Adversarial training**
- **Continual multilingual fine-tuning**

---

### **Q9.** How does translation-based pretraining differ from masked language modeling in multilingual setups?
**A9.**
- **Translation-based:** Uses translation pairs to teach semantic equivalence (e.g., XLM).  
- **MLM:** Predicts masked tokens from context without needing parallel data (e.g., mBERT).  
Translation-based is more semantically aligned, but less scalable.

---

### **Q10.** What are the future directions for multilingual NLP research?
**A10.**
- **Multimodal multilingual models** (text + speech + image)
- **Truly universal tokenizers** without Latin bias
- **Low-resource language synthesis** using generative LMs
- **Evaluation beyond accuracy** — cultural fairness, contextual sensitivity
- **Neural architecture adaptation** for morphologically rich languages

---
