# Hard Level NLP Questions and Answers (Set 17)

### 1. How does gradient checkpointing help train large NLP models?
**Answer:**  
Gradient checkpointing trades computation for memory. Instead of storing all activations for backpropagation, it selectively recomputes them during the backward pass. This reduces GPU memory usage significantly, enabling training of larger models.

---

### 2. Why is beam search used in text generation, and what are its limitations?
**Answer:**  
- **Beam search:** Keeps top-k sequences at each decoding step, balancing exploration and exploitation.  
- **Limitations:** Can lead to repetitive or generic outputs, biased towards short sequences, and computationally expensive. Alternatives: **nucleus sampling, contrastive decoding**.  

---

### 3. How does multilingual BERT (mBERT) achieve cross-lingual transfer without parallel data?
**Answer:**  
It relies on a **shared subword vocabulary** and parameter sharing across languages. Semantic similarity across embeddings allows zero-shot transfer (e.g., training in English, inference in Hindi).  

---

### 4. What are attention heads in Transformers, and why are multiple heads used?
**Answer:**  
Each head learns different relationships (syntax, semantics, positional cues). Multiple heads allow the model to capture diverse dependencies. Ablation studies show some heads are redundant, but collectively they improve robustness.  

---

### 5. Explain the concept of lottery ticket hypothesis in NLP models.
**Answer:**  
It posits that within overparameterized networks, small subnetworks (“winning tickets”) exist that, when trained alone, achieve comparable performance. In NLP, this inspires **pruning, distillation, and efficient model deployment**.  

---

### 6. How does adversarial training improve NLP models?
**Answer:**  
By exposing models to adversarial perturbations (e.g., synonym swaps, character noise), robustness improves. However, it’s computationally expensive and risks overfitting to specific adversarial patterns.  

---

### 7. Why is long-context modeling challenging for Transformers?
**Answer:**  
Self-attention scales **O(n²)** with sequence length, making memory and compute prohibitive for long documents. Solutions: **sparse attention (Longformer), linear attention (Performer), recurrence (Transformer-XL), retrieval-augmented methods**.  

---

### 8. How do probing tasks reveal linguistic structure in embeddings?
**Answer:**  
By training classifiers on frozen embeddings for syntactic (POS, parsing) or semantic (coreference, entailment) tasks. Success indicates embeddings capture these properties. However, probing may reflect surface heuristics, not true understanding.  

---

### 9. What is catastrophic agreement in machine translation ensembles?
**Answer:**  
When multiple models agree on incorrect translations due to shared biases. Ensemble diversity (different seeds, architectures, data subsets) is needed to mitigate this.  

---

### 10. How do knowledge distillation and teacher-student frameworks apply in NLP?
**Answer:**  
A large **teacher model** transfers knowledge to a smaller **student model** via soft labels and intermediate representations. Used in **DistilBERT, TinyBERT** to reduce inference cost while retaining performance.
