## Q76: How does retrieval-augmented NLP help in multilingual settings?

**Answer:**
- **Problem:** Multilingual LLMs may not have strong knowledge in low-resource languages.
- **Solution:** Retrieval provides external documents in the target language, improving factual grounding.
- **Example:** A Swahili query retrieves relevant Swahili Wikipedia passages before generating the answer.
- **Challenge:** Cross-lingual retrieval quality (query in one language, documents in another).
- **Approach:** Use multilingual dense retrievers like mDPR or LaBSE.

---

## Q77: What are the challenges of handling long-context inputs in NLP?

**Answer:**
- **Issues:**
  - Standard transformers have quadratic attention cost â†’ inefficient beyond ~4k tokens.
  - Memory and compute overheads.
- **Solutions:**
  - Sparse attention (Longformer, BigBird).
  - Segment recurrence (Transformer-XL).
  - Hierarchical models (chunk-based).
- **Applications:** Legal documents, books, genomic data.
- **Trade-off:** Longer context may dilute focus if not handled properly.

---

## Q78: How do LLMs implement memory for conversations?

**Answer:**
- **Types of Memory:**
  - **Short-term (Context Window):** Stores conversation within token limit.
  - **External Memory:** Vector databases store embeddings for recall.
  - **Persistent Memory:** Fine-tuning or adapters encode long-term knowledge.
- **Challenge:** Forgetting irrelevant info while retaining key context.
- **Example:** Memory in chatbots to recall user preferences across sessions.

---

## Q79: How can NLP models be used for program synthesis?

**Answer:**
- **Definition:** Generating executable code from natural language.
- **Approaches:**
  - Seq2Seq with code pretraining (Codex, StarCoder).
  - Syntax-aware decoding to maintain grammar.
  - Execution-based refinement (run code, fix errors).
- **Challenges:** 
  - Ambiguity in natural language.
  - Security risks (malicious code generation).
- **Applications:** Auto-complete, test generation, data pipeline creation.

---

## Q80: What is continual learning in NLP, and why is it hard?

**Answer:**
- **Continual Learning:** Updating models with new data without forgetting old knowledge.
- **Challenges:**
  - Catastrophic forgetting: Model overwrites past knowledge.
  - Balancing stability (retain old) vs plasticity (learn new).
- **Techniques:**
  - Regularization (EWC, LwF).
  - Replay (store old samples).
  - Modular architectures (adapters).
- **Use Case:** Keeping an NLP assistant updated with latest world events.
