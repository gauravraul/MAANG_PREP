# Set 21 – Research-Level AI (Scaling, Interpretability, Alignment, Multimodal & Speech)

---

## Category 1: Scaling Laws & Efficiency

**Q1. Explain the concept of scaling laws in LLMs (Kaplan et al.). How do compute, data, and parameters interact in loss reduction?**  
**A1.** Scaling laws show that model performance (loss) improves predictably as parameters, dataset size, and compute increase, following power-law curves. Larger models trained on more data with more compute achieve lower loss, until diminishing returns occur.

**Q2. Why does “Chinchilla scaling” (DeepMind) suggest data over-parameter scaling? What are its implications for small vs. large labs?**  
**A2.** Chinchilla (2022) found that models are often under-trained on data relative to their size. It showed that for fixed compute, using more training tokens (data) and fewer parameters yields better performance. Implication: smaller labs can train competitive models by focusing on more data-efficient training.

**Q3. How do mixture-of-experts (MoE) architectures enable efficient scaling? What are their tradeoffs (latency, routing)?**  
**A3.** MoE activates only a subset of parameters (experts) per input, reducing compute cost while keeping model capacity high. Tradeoffs: routing complexity, load balancing, and higher inference latency due to conditional computation.

**Q4. If you only have 10x less compute than OpenAI/DeepMind, how would you still train a competitive LLM?**  
**A4.** Options:  
- Use **data-efficient training** (Chinchilla-style).  
- Apply **parameter-efficient architectures** (MoE, LoRA).  
- Focus on **domain-specific data** (specialization).  
- Leverage **distillation & pruning**.  
- Train smaller but highly **aligned and fine-tuned** models.

---

## Category 2: Interpretability & Safety

**Q5. What is mechanistic interpretability? Give an example of circuits/feature visualization in transformers.**  
**A5.** Mechanistic interpretability seeks to understand how specific neurons or attention heads implement functions. Example: “induction heads” in transformers that copy tokens from earlier in a sequence.

**Q6. Why are attention weights not always reliable indicators of interpretability?**  
**A6.** Attention weights show where the model attends but not necessarily which features drive predictions. Models can perform correctly even if attention distributions look uniform or misleading.

**Q7. Define emergent abilities in LLMs. Why do they occur and why are they controversial?**  
**A7.** Emergent abilities are sudden jumps in performance on tasks (e.g., arithmetic, reasoning) as models scale. They occur due to thresholds in representation learning. Controversy: some argue they’re artifacts of evaluation metrics, not true emergence.

**Q8. How can adversarial prompting or jailbreaking bypass alignment? Suggest defense methods.**  
**A8.** Jailbreaking exploits weaknesses in prompt filters to elicit harmful outputs. Defenses: adversarial training, red-teaming, layered moderation, and more robust alignment objectives (e.g., constitutional AI).

---

## Category 3: Alignment & Ethics

**Q9. What’s the difference between RLHF and constitutional AI?**  
**A9.**  
- **RLHF**: Uses human feedback to fine-tune reward models and align outputs.  
- **Constitutional AI**: Uses a set of principles (a “constitution”) to generate synthetic feedback, reducing dependence on human annotators.

**Q10. Why is mode collapse or excessive politeness in RLHF-aligned models a problem?**  
**A10.** Over-alignment can make models too cautious, refusing harmless queries or producing repetitive, bland outputs, reducing usability.

**Q11. Discuss the alignment tax: how safety tradeoffs may reduce performance.**  
**A11.** Alignment tax refers to performance or efficiency costs introduced by safety/alignment mechanisms (e.g., lower diversity, higher latency). But it is often necessary for responsible deployment.

**Q12. How would you design an experiment to measure hallucination rates in a domain-specific LLM?**  
**A12.**  
- Collect gold-standard factual datasets in the target domain.  
- Generate model outputs.  
- Compare against ground truth using factuality metrics (e.g., faithfulness scores).  
- Track hallucination frequency across tasks (QA, summarization).

---

## Category 4: Multimodal AI

**Q13. Explain the difference between early fusion, late fusion, and joint fusion in multimodal models.**  
**A13.**  
- **Early fusion**: Combine modalities at the input level.  
- **Late fusion**: Process separately, then combine outputs.  
- **Joint fusion**: Learn shared latent space representations.

**Q14. How do Vision Transformers (ViTs) and text transformers combine in CLIP?**  
**A14.** CLIP trains a ViT for images and a Transformer for text with contrastive loss, aligning their embeddings in a joint space.

**Q15. Why does contrastive learning dominate in vision-language models? Compare with generative approaches.**  
**A15.** Contrastive learning (e.g., CLIP) is efficient, scalable, and aligns modalities without requiring paired generation. Generative approaches (e.g., Flamingo) capture richer interactions but are more compute-heavy.

**Q16. How would you extend your fine-tuned NLP LLM into a multimodal agent?**  
**A16.**  
- Add image/audio encoders.  
- Project embeddings into shared latent space.  
- Use instruction tuning for multimodal tasks.  
- Employ cross-attention for deeper fusion.

---

## Category 5: Speech & Audio Models

**Q17. What are wav2vec2 and HuBERT? How do they handle raw audio?**  
**A17.** Both are self-supervised speech models:  
- **wav2vec2**: Predicts masked latent speech representations.  
- **HuBERT**: Predicts hidden cluster assignments for masked segments.  
Both learn directly from raw waveforms.

**Q18. Explain how sequence-to-sequence models like Whisper perform speech-to-text robustly.**  
**A18.** Whisper encodes raw audio into embeddings, then decodes text with a Transformer seq2seq architecture. It is trained on diverse multilingual + noisy data, making it robust.

**Q19. What are the challenges in low-resource ASR (automatic speech recognition)?**  
**A19.** Scarcity of labeled data, accent and dialect variation, background noise, and lack of domain-specific vocabularies.

**Q20. How would you adapt a speech model for code-switching (Hindi-English, for example)?**  
**A20.** Train with code-switched corpora, apply multilingual embeddings, and adapt lexicons. Fine-tune with domain-specific conversational datasets.

---

## Category 6: Open-Ended Research Thinking

**Q21. If you were to design the next-generation foundation model, would you:**  
- Push for bigger scaling?  
- Explore modular/multimodal architectures?  
- Prioritize alignment + interpretability over raw power?  
Give a reasoned argument with reference to current research gaps.  

**A21.** A strong direction is **modular multimodal architectures** with built-in interpretability and alignment. Pure scaling faces diminishing returns and high cost. Modular designs allow specialization, composability, and efficiency. Prioritizing alignment ensures safety and trustworthiness, making models more deployable in real-world high-stakes domains.
