# Research-Level NLP / Multimodal AI / Speech Questions (Set 25)

## Q1. What are neurosymbolic approaches in NLP and why are they important?
**A1.** Neurosymbolic methods combine neural networks (for pattern recognition and representation learning) with symbolic reasoning (for logical inference and interpretability). They aim to provide:
- Better compositional generalization.  
- Stronger reasoning abilities.  
- More interpretable decision-making.  

---

## Q2. How do large multimodal models handle temporal dynamics (e.g., in video or speech)?
**A2.** Temporal modeling uses:
- Transformers with positional encodings for sequential data.  
- 3D convolutions or temporal attention for video.  
- Recurrent or conformer layers for speech.  
Challenges: balancing local detail with long-range temporal dependencies.  

---

## Q3. What are the limits of scaling in multimodal models?
**A3.** Scaling improves performance but faces diminishing returns due to:
- Data bottlenecks (high-quality multimodal datasets are scarce).  
- Compute constraints.  
- Misalignment across modalities (hallucinations).  
This motivates research into efficiency and smarter scaling.  

---

## Q4. How does causality intersect with multimodal learning?
**A4.** Causal reasoning helps models:  
- Distinguish correlation from causation across modalities.  
- Improve robustness under distribution shifts.  
- Enable counterfactual reasoning (e.g., “What if this object was removed from the image?”).  

---

## Q5. What are “multimodal hallucinations” and how are they detected?
**A5.** Multimodal hallucinations occur when the model generates content unsupported by any modality.  
Detection strategies:  
- Cross-modal consistency checks.  
- Contrastive training with negative samples.  
- Human-in-the-loop evaluations.  

---

## Q6. Why is interpretability more complex in speech-language models?
**A6.** Because:  
- Features are hierarchical (raw waveform → phonemes → words → meaning).  
- Black-box acoustic embeddings make attribution difficult.  
- Alignment errors propagate across multiple stages (ASR → NLU → generation).  

---

## Q7. What are the ethical risks of multimodal AI in surveillance?
**A7.**  
- Biometric profiling and privacy invasion.  
- Demographic bias in face or voice recognition.  
- Misuse in authoritarian settings for mass surveillance.  
Mitigation: differential privacy, fairness audits, and strict governance.  

---

## Q8. How do energy constraints affect large-scale NLP and multimodal research?
**A8.** Training large foundation models consumes massive energy. Research directions include:  
- Sparse and mixture-of-expert architectures.  
- Low-rank adaptation (LoRA) for fine-tuning.  
- Hardware co-design (GPUs/TPUs with specialized AI accelerators).  

---

## Q9. What is the role of attention bottlenecks in multimodal interpretability?
**A9.** Attention bottlenecks restrict how much cross-modal interaction occurs.  
- Useful for interpretability (clearer focus points).  
- But risk under-utilization of rich features if bottlenecks are too strong.  

---

## Q10. What future breakthroughs are expected in multimodal AI?
**A10.**  
- Fully integrated models handling text, vision, audio, video, and robotics signals.  
- Models capable of **counterfactual multimodal reasoning**.  
- Advances in **alignment and interpretability** to reduce bias and hallucination.  
- Democratization of large models via **efficient scaling methods**.
