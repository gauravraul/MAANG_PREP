# Hard Computer Vision Interview Questions & Answers (Set 2)

---

### 1. How does **Transformers (ViT)** differ from **CNNs** in computer vision?
**Answer:**
- **CNNs** use local receptive fields and hierarchical feature extraction.
- **Vision Transformers (ViT)** split an image into patches, embed them, and apply self-attention across patches.
- **Key difference**: CNNs capture locality, ViTs excel at global dependencies without convolutional bias.
- ViTs need **large datasets** (e.g., ImageNet-21k) or **pretraining** to outperform CNNs.

---

### 2. Explain **Self-Supervised Learning (SSL)** in computer vision.
**Answer:**
- SSL learns useful representations **without labeled data**.
- Common methods:
  - **Contrastive learning (SimCLR, MoCo)** – pulling similar representations closer.
  - **Masked image modeling (MAE, BEiT)** – predicting missing patches of an image.
- SSL reduces dependency on labeled datasets and improves transfer learning.

---

### 3. How would you detect **deepfake videos**?
**Answer:**
- Use a combination of:
  - **Face detection + tracking**.
  - **Temporal inconsistencies** (blinking rate, unnatural head movements).
  - **Frequency domain analysis** (artifacts).
  - **Deep learning classifiers** (CNN + LSTM or ViT-based).
- Adversarial training helps models generalize to new fake-generation techniques.

---

### 4. What is the role of **attention maps** in Explainable AI (XAI) for CV?
**Answer:**
- Attention maps highlight **which regions of an image contribute most** to a model’s decision.
- Tools: **Grad-CAM, LIME, SHAP**.
- Example: In medical imaging, attention maps can show which tumor region influenced a diagnosis.

---

### 5. How do you design a **real-time object detection system for drones**?
**Answer:**
- Challenges: low power, latency, unstable camera.
- Solutions:
  - Use **lightweight models** (YOLOv5-Nano, MobileNet-SSD).
  - **Quantization & pruning** for efficiency.
  - Deploy on **edge devices (Jetson Nano, Coral TPU)**.
  - Optimize with **TensorRT** or **ONNX Runtime**.

---

### 6. What is the **role of transformers in multi-modal CV tasks**?
**Answer:**
- Multi-modal tasks combine **vision + text/audio**.
- Transformers like **CLIP (OpenAI)** align image embeddings with text embeddings.
- Used in:
  - **Image captioning**.
  - **Visual question answering (VQA)**.
  - **Zero-shot image classification**.

---

### 7. How do you evaluate **instance segmentation models**?
**Answer:**
- Metrics:
  - **mAP (mean Average Precision)** – measures precision/recall across IoU thresholds.
  - **AP50, AP75** – stricter overlap requirements.
  - **Mask IoU** – overlap of predicted vs. ground truth mask.
- Example models: **Mask R-CNN, YOLACT, SOLO**.

---

### 8. How would you handle **domain adaptation** in computer vision?
**Answer:**
- Problem: Training dataset ≠ target domain (e.g., synthetic vs. real-world).
- Techniques:
  - **Domain-invariant feature learning** (DANN, adversarial loss).
  - **Style transfer** (CycleGAN).
  - **Fine-tuning on small labeled samples** from target domain.
- Application: Autonomous driving (simulation → real-world).

---

### 9. Explain **Neural Radiance Fields (NeRF)** in computer vision.
**Answer:**
- NeRF represents 3D scenes using a neural network that maps **3D coordinates + viewing direction → color & density**.
- Trained using multiple 2D images from different angles.
- Applications:
  - **3D reconstruction**.
  - **Virtual reality (VR/AR)**.
  - **Synthetic dataset generation**.

---

### 10. How do you design a **large-scale image search engine**?
**Answer:**
- Steps:
  1. Extract **image embeddings** using CNN/ViT.
  2. Store embeddings in a **vector database** (e.g., FAISS, Milvus).
  3. Use **Approximate Nearest Neighbor (ANN)** for fast retrieval.
  4. Implement **re-ranking** using contextual or semantic similarity.
- Applications: e-commerce (search by image), stock photo retrieval.

---
