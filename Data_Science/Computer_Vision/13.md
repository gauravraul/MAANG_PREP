# Hard-Level Computer Vision Questions and Answers (Set 4)

---

### 1. How does **Vision Transformer (ViT)** differ from CNNs in processing images?
**Answer:**  
- CNNs rely on local convolutional filters to capture spatial features.  
- ViTs treat an image as a sequence of patches (like words in NLP) and use self-attention to model global dependencies.  
- ViTs require large-scale datasets (like ImageNet-21k or JFT) for pretraining due to fewer inductive biases compared to CNNs.

---

### 2. What is **Group Normalization** and why is it preferred over Batch Normalization in CV tasks?
**Answer:**  
- Group Normalization divides channels into groups and normalizes within each group, independent of batch size.  
- Unlike Batch Normalization, it works well for small batch sizes (common in large image tasks like object detection/segmentation).  
- Used in **Mask R-CNN** and other detection frameworks.

---

### 3. Explain **Homography Estimation** and its role in Computer Vision.
**Answer:**  
- Homography is a 3×3 transformation matrix mapping points from one plane to another in images.  
- Used in **image stitching, AR, motion tracking, camera calibration**.  
- Estimated using **RANSAC** to handle outliers when matching keypoints between two views.

---

### 4. What is **Attention Mechanism in Object Detection** (e.g., in DETR)?
**Answer:**  
- DETR (DEtection TRansformer) replaces hand-designed components like anchors with transformer attention.  
- Attention allows the model to directly attend to relevant parts of the image and predict object bounding boxes in a **set-based formulation**.  
- This removes the need for NMS (Non-Maximum Suppression).

---

### 5. How do **Graph Neural Networks (GNNs)** apply to computer vision?
**Answer:**  
- Images or objects can be represented as graphs (nodes = regions/objects, edges = relationships).  
- GNNs model **scene graphs, human pose estimation, action recognition**, and **3D mesh reconstruction**.  
- Example: Modeling object interactions in visual question answering.

---

### 6. What are **Capsule Networks**, and how do they improve over CNNs?
**Answer:**  
- Capsule Networks group neurons into “capsules” that capture **pose, orientation, and spatial hierarchy** of features.  
- Unlike CNNs, they preserve spatial relationships instead of losing them via pooling.  
- Useful in viewpoint-invariant recognition but computationally heavy.

---

### 7. Explain **3D Convolutional Neural Networks (3D CNNs)** and their use cases.
**Answer:**  
- 3D CNNs extend convolution kernels to **3D (height × width × time/volume)**.  
- Capture spatiotemporal features from videos or volumetric data (e.g., MRI, CT scans).  
- Widely used in **video classification, medical imaging, action recognition**.

---

### 8. What is **Optical Flow Estimation** and its challenges?
**Answer:**  
- Optical Flow estimates the motion of pixels between consecutive frames.  
- Applications: motion tracking, video stabilization, autonomous driving.  
- Challenges: handling occlusion, large displacements, illumination changes, non-rigid motions.  
- Algorithms: Lucas-Kanade, Horn-Schunck, PWC-Net.

---

### 9. How do **Self-Supervised Learning** methods benefit computer vision?
**Answer:**  
- Self-Supervised methods learn representations without labeled data by solving **pretext tasks** (e.g., contrastive learning, image inpainting, colorization).  
- Models like **SimCLR, MoCo, BYOL, DINO** train on huge unlabeled datasets.  
- They achieve competitive or superior performance to supervised pretraining.

---

### 10. What are **Adversarial Attacks** in computer vision, and how to defend against them?
**Answer:**  
- Adversarial attacks add imperceptible perturbations to images that cause models to misclassify (e.g., stop sign → speed limit).  
- Attacks: FGSM, PGD, DeepFool.  
- Defenses:  
  - **Adversarial training** (train with perturbed examples).  
  - **Defensive distillation**.  
  - **Certified defenses** like randomized smoothing.  

---
