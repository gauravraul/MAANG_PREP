# Hard-Level Computer Vision Interview Questions and Answers

## 1. How does attention mechanism improve vision models compared to convolution?
**Answer:**  
- Convolution applies local filters and struggles with long-range dependencies.  
- Attention computes pairwise relationships between all pixels/patches, enabling global context.  
- Vision Transformers (ViT) leverage self-attention to capture dependencies without locality bias, often outperforming CNNs on large datasets.  
- Hybrid models (e.g., ConvNeXt + Attention) combine benefits of both.

---

## 2. What are the main challenges in training 3D CNNs for video classification?
**Answer:**  
- **High computation/memory cost** due to spatio-temporal kernels.  
- **Overfitting** since datasets are smaller compared to image datasets.  
- **Optimization difficulties** because of vanishing gradients in deep temporal layers.  
- Solutions: mixed-precision training, transfer learning from 2D CNNs, and temporal sampling strategies (e.g., SlowFast networks).

---

## 3. How does self-supervised learning (SSL) work in Computer Vision?
**Answer:**  
- SSL learns representations without manual labels.  
- Pretext tasks include:  
  - Predicting rotation angle.  
  - Contrastive learning (SimCLR, MoCo).  
  - Masked image modeling (MAE, BEiT).  
- Learned embeddings are fine-tuned on downstream tasks (classification, detection).  
- Key benefit: reduces dependency on large labeled datasets.

---

## 4. Explain the difference between GANs and Diffusion Models in image generation.
**Answer:**  
- **GANs**: Generator tries to fool discriminator, training is adversarial and unstable, but fast inference.  
- **Diffusion Models**: Iteratively denoise random noise to generate samples, stable training but slower inference.  
- GANs excel in sharp images but may collapse; Diffusion models achieve higher fidelity and diversity (e.g., Stable Diffusion, DALLÂ·E 2).

---

## 5. What are the challenges of deploying large vision models (e.g., ViT, SAM) in production?
**Answer:**  
- **Latency** due to high FLOPs.  
- **Memory constraints** for edge devices.  
- **Model updates** are costly due to size.  
- Solutions: model compression (quantization, pruning), distillation, caching intermediate embeddings, or using efficient variants (e.g., MobileViT, TinySAM).

---

## 6. How does multi-modal learning improve vision tasks?
**Answer:**  
- Combines visual + textual/audio signals.  
- Example: CLIP (aligns vision and text embeddings).  
- Benefits: zero-shot classification, open-vocabulary detection, cross-modal retrieval.  
- Challenges: aligning modalities, dataset bias, and scaling compute.

---

## 7. What are graph neural networks (GNNs) used for in Computer Vision?
**Answer:**  
- CV data can be modeled as graphs (superpixels, regions, object relations).  
- GNNs capture **relational reasoning**:  
  - Scene understanding.  
  - Human-object interaction.  
  - Skeleton-based action recognition.  
- Example: Graph Convolutional Networks (GCN) on human pose keypoints for activity recognition.

---

## 8. How do Neural Radiance Fields (NeRF) work for 3D reconstruction?
**Answer:**  
- NeRF represents 3D scenes as continuous volumetric fields (density + color).  
- Uses a neural network to predict pixel color by integrating along camera rays.  
- Produces photorealistic novel views from few images.  
- Challenges: slow rendering, large memory, training instability.  
- Accelerations: Instant-NGP, PlenOctrees.

---

## 9. What is domain adaptation in Computer Vision and why is it hard?
**Answer:**  
- Model trained on source domain (e.g., synthetic images) performs poorly on target domain (e.g., real-world).  
- Causes: distribution shift (illumination, style, resolution).  
- Techniques: adversarial domain adaptation, feature alignment, style transfer (CycleGAN).  
- Hard because full supervision in target domain is missing, and shifts can be multi-dimensional.

---

## 10. How do you ensure fairness and reduce bias in vision systems?
**Answer:**  
- **Bias sources:** imbalanced datasets (race, gender, lighting conditions).  
- **Mitigation:**  
  - Data augmentation & balanced sampling.  
  - Fairness-aware loss functions.  
  - Post-processing calibration.  
- Example: Face recognition systems can show disparities if not trained on diverse datasets.  
- Real challenge: aligning fairness with accuracy in production.

---
